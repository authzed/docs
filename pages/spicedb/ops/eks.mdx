import { Callout, Steps } from 'nextra/components'
import YouTube from 'react-youtube';

# Installing SpiceDB on Amazon EKS

Amazon Elastic Kubernetes Service or [Amazon EKS] is the managed Kubernetes service provided by Amazon and is the best way to run SpiceDB on AWS.

This guide walks through creating a highly-available SpiceDB deployment on an existing EKS cluster.
TLS configuration is managed by [cert-manager] and [Amazon Route53].

[Amazon  EKS]: https://docs.aws.amazon.com/eks/latest/userguide/what-is-eks.html
[cert-manager]: https://cert-manager.io
[Amazon Route53]: https://aws.amazon.com/route53/

## Steps

<Steps>

### Confirming the Prerequisites

The rest of this guide assumes [kubectl] is configured to use an operational EKS cluster along with a Route53 External Hosted Zone.

[kubectl]: https://kubernetes.io/docs/tasks/tools/#kubectl

### Creating an IAM Policy

In order for the cluster to dynamically configure DNS, the first step is grant access via the creation of an IAM Policy and attach it to the role used for the pods in EKS:

```json
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": "route53:GetChange",
			"Resource": "arn:aws:route53:::change/*"
		},
		{
			"Effect": "Allow",
			"Action": [
				"route53:ChangeResourceRecordSets",
				"route53:ListResourceRecordSets"
			],
			"Resource": "arn:aws:route53:::hostedzone/*"
		},
		{
			"Effect": "Allow",
			"Action": "route53:ListHostedZonesByName",
			"Resource": "*"
		}
	]
}
```

### Deploying cert-manager

Before modifying any cluster, we recommend double-checking that your current context is configured for the target cluster:

```sh
kubectl config current-context
```

Now you're ready to apply the manifests that install cert-manager:

```sh
kubectl apply -f https://github.com/cert-manager/cert-manager/releases/latest/download/cert-manager.yaml
```

If you'd like to use another installation method for cert-manager, you can find more in the [cert-manager documentation][cert-manager-docs].

When cert-manager is operational, all the pods should be healthy provided the following command:

```sh
kubectl -n cert-manager get pods
```

[cert-manager]: https://cert-manager.io
[cert-manager-docs]: https://cert-manager.io/docs/installation

### Creating a Namespace

Next up, we'll create a new [Namespace] for deploying SpiceDB.

This guide uses one named `spicedb`, but feel free to use replace this throughout the guide with whatever you prefer.

```sh
kubectl apply --server-side -f - <<EOF
apiVersion: v1
kind: Namespace
metadata:
  name: spicedb
EOF
```

[Namespace]: https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/

### Creating an Issuer & Certificate

In order to setup an [ACME] flow with [Let's Encrypt], we'll be configuring cert-manager to use a DNS challenge.

After changing the commented fields below, apply the following manifests:

```sh
kubectl apply --server-side -f - <<EOF
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: spicedb-tls-issuer
  namespace: spicedb
spec:
  acme:
    email: example@email.com                                # Change this
    server: https://acme-v02.api.letsencrypt.org/directory
    privateKeySecretRef:
      name: letsencrypt-production
    solvers:
    - selector:
        dnsZones:
          - "example.com"                                  # Change this
      dns01:
        route53:
          region: us-east-1                                # Change this
          hostedZoneID: ABC123ABC123                       # Change this
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: spicedb-le-certificate
  namespace: spicedb
spec:
  secretName: spicedb-le-tls
  issuerRef:
    name: spicedb-tls-issuer
    kind: ClusterIssuer
  commonName: demo.example.com                            # Change this
  dnsNames:
  - demo.example.com                                      # Change this
EOF
```

After about one minute, cert-manager should have detected the creation of these resources and created a new [Secret]:

```sh
kubectl -n spicedb get secrets
```

If the `spicedb-le-tls` secret is not present, follow this [troubleshooting guide].

[ACME]: https://cert-manager.io/docs/concepts/acme-orders-challenges
[Let's Encrypt]: https://letsencrypt.org
[Secret]: https://kubernetes.io/docs/concepts/configuration/secret
[troubleshooting guide]: https://cert-manager.io/docs/troubleshooting/acme/

### Configuring Cross-Pod TLS

SpiceDB will be more performant when cross-pod communication is enabled (see [performance]).
We'll be securing this pod-to-pod communication with its own internal-only certificate:

[gRPC]: https://grpc.io
[performance]: performance#by-enabling-cross-node-communication

```sh
kubectl apply --server-side -f - <<EOF
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: dispatch-selfsigned-issuer
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: dispatch-ca
  namespace: spicedb
spec:
  isCA: true
  commonName: dev.spicedb   # Change optional: in this example, "dev" is the name of the SpiceDBCluster object (defined at the "Configure SpiceDB settings" step below). If you don't want to use "dev", change "dev" to what you will use.
  dnsNames:
  - dev.spicedb             # Change optional: (see above)
  secretName: dispatch-root-secret
  privateKey:
    algorithm: ECDSA
    size: 256
  issuerRef:
    name: dispatch-selfsigned-issuer
    kind: ClusterIssuer
    group: cert-manager.io
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: my-ca-issuer
  namespace: cert-manager
spec:
  ca:
    secretName: dispatch-root-secret
EOF
```

- Apply the above configuration with: `kubectl apply -f internal-dispatch.yaml`

### Deploying a Datastore

Deploy a Postgres RDS datastore that is network reachable from SpiceDBâ€™s EKS cluster.
SpiceDB will connect to your Postgres DB with a connection string.

### Configuring and Deploy SpiceDB

#### Initialize the Operator

- Install a release of the operator with: `kubectl apply --server-side -f https://github.com/authzed/spicedb-operator/releases/latest/download/bundle.yaml`

#### Configure SpiceDB settings

```yaml
cat << EOF > spicedb-config.yaml
apiVersion: authzed.com/v1alpha1
kind: SpiceDBCluster
metadata:
  name: dev                                #Change optional: you can change this name, but be mindful of your dispatch TLS certificate URL and service selector
spec:
  config:
    datastoreEngine: postgres
    replicas:  2                           #Change optional: at least two replicas are required for HA
    tlsSecretName: spicedb-le-tls
    dispatchUpstreamCASecretName: dispatch-root-secret
    dispatchClusterTLSCertPath: "/etc/dispatch/tls.crt"
    dispatchClusterTLSKeyPath: "/etc/dispatch/tls.key"
  secretName: dev-spicedb-config
  patches:
  - kind: Deployment
     patch:
       spec:
         template:
           spec:
             containers:
             - name: spicedb
               volumeMounts:
               - name: custom-dispatch-tls
                 readOnly: true
                 mountPath: "/etc/dispatch"
             volumes:
             - name: custom-dispatch-tls
               secret:
                 secretName: dispatch-root-secret
---
apiVersion: v1
kind: Secret
metadata:
  name: dev-spicedb-config
stringData:
  preshared_key: "averysecretpresharedkey" #Change: this is your API token definition and should be kept secure.
  datastore_uri: "postgresql://user:password@postgres.com:5432" #Change: this is a Postgres connection string
EOF
```

- Apply the above configuration with: `kubectl apply -f spicedb-config.yaml -n spicedb`

### Deploy Cloud Load Balancer Service

This step will deploy a service of type load balancer, which will deploy an external AWS load balancer to route traffic to our SpiceDB pods.

```yaml
cat << EOF > spicedb-lb.yaml
apiVersion: v1
kind: Service
metadata:
  name: spicedb-external-lb
  namespace: spicedb
spec:
  ports:
  - name: grpc
    port: 50051
    protocol: TCP
    targetPort: 50051
  - name: gateway
    port: 8443
    protocol: TCP
    targetPort: 8443
  - name: metrics
    port: 9090
    protocol: TCP
    targetPort: 9090
  selector:
    app.kubernetes.io/instance: dev-spicedb    #Change optional: in this example, "dev" is the name of the SpiceDBCluster object. If you didn't use "dev", change "dev" to what you used.
  sessionAffinity: None
  type: LoadBalancer
EOF
```

- Apply the above configuration with: `kubectl apply -f spicedb-lb.yaml`
- Run the following command to get the External-IP of the load balancer: `kubectl get -n spicedb services spicedb-external-lb -o json | jq '.status.loadBalancer.ingress[0].hostname'`
- Take the output of the command and add it as a C-Name record in your Route 53 Hosted Zone. **Note**: Ensure that it's added to the record containing the `dnsNames` that was specified while creating an Issuer & Certificate in Step 5.

### Test

You can use the Zed CLI tool to make sure everything works as expected:

`zed context set eks-guide demo.example.com:50051 averysecretpresharedkey`

Write a schema

```yaml
zed schema write <(cat << EOF
definition user {}

definition doc {
  relation owner: user
  permission view = owner
}
EOF
)
```

Write a relationship:

```yaml
zed relationship create doc:1 owner user:emilia
```

Check a permission:

```yaml
zed permission check doc:1 view user:emilia
```

</Steps>

Here's a YouTube video that describes the above steps:

<YouTube videoId="KT1RqTBeA1c" className="youtubeContainer" opts={{playerVars:{start: 1127}}} />
