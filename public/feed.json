{
    "version": "https://jsonfeed.org/version/1",
    "title": "AuthZed Blog",
    "home_page_url": "https://authzed.com",
    "feed_url": "https://authzed.com/feed/json",
    "description": "The AuthZed blog: Articles from the AuthZed team about SpiceDB, Fine Grained Authorization, Google Zanzibar, and engineering culture.",
    "icon": "https://authzed.com/authzed-logo-multi.svg",
    "items": [
        {
            "id": "https://authzed.com/blog/timeline-mcp-breaches",
            "content_html": "<h1 id=\"user-content-mcp-article-1\">MCP Article #1:</h1>\n<h1 id=\"user-content-timeline-of-mcp-breaches\">Timeline of MCP Breaches</h1>\n<blockquote>\n<p>AI fundamentally changes the interface, but not the fundamentals of security. Read on to find out why</p>\n</blockquote>\n<p>It feels like eons ago when the <strong>Model Context Protocol (MCP)</strong> was introduced (it was <em>only</em> <a href=\"https://www.anthropic.com/news/model-context-protocol\">in November 2024</a> lol)</p>\n<p>It promised to become the <strong>USB-C of AI agents</strong> — a universal bridge for connecting LLMs to tools, APIs, documents, emails, codebases, databases and cloud infrastructure. In just months, the ecosystem exploded: dozens of tool servers, open-source integrations, host implementations, and hosted MCP registries began to appear.</p>\n<p>As the ecosystem rapidly adopted MCP, it presented the classic challenge of securing any new technology: developers connected powerful, sensitive systems without rigorously applying established security controls and fundamental principles to the new spec. By mid-2025, the vulnerabilities were exposed, confirming that the new AI-native world is governed by the same security principles as traditional software.</p>\n<p>Below is the first consolidated timeline tracing the major MCP-related breaches and security failures - what happened, what data was exposed, why it happened, and what they reveal about the new threat surface LLMs bring into organisations.</p>\n<h1 id=\"user-content-timeline\">Timeline:</h1>\n<h2 id=\"user-content-apr---jun-2025\">Apr - Jun 2025</h2>\n<h3 id=\"user-content-1-april-2025--whatsapp-mcp-exploited-chat-history-exfiltration\"><strong>1. April 2025 – WhatsApp MCP Exploited: Chat-History Exfiltration</strong></h3>\n<ul>\n<li>\n<p><strong>What happened:</strong> Invariant Labs demonstrated that a malicious MCP server could <strong>silently exfiltrate a user’s entire WhatsApp history</strong> by combining “tool poisoning” with a legitimate <code>whatsapp-mcp</code> server in the same agent. A “random fact of the day” tool morphed into a sleeper backdoor that rewrote how WhatsApp messages are sent. <a href=\"https://invariantlabs.ai/blog/whatsapp-mcp-exploited\">Invariant Labs Link</a></p>\n</li>\n<li>\n<p><strong>Data at risk &#x26; why:</strong> Once the agent read the poisoned tool description, it happily followed hidden instructions to send <strong>hundreds or thousands of past WhatsApp messages</strong> (personal chats, business deals, customer data) to an attacker-controlled phone number – all disguised as ordinary outbound messages, bypassing typical Data Loss Prevention (DLP) tooling.</p>\n</li>\n</ul>\n<h3 id=\"user-content-2-may-2025--github-mcp-prompt-injection-data-heist\"><strong>2. May 2025 – GitHub MCP “Prompt Injection Data Heist”</strong></h3>\n<ul>\n<li>\n<p><strong>What happened:</strong> Invariant Labs uncovered a <strong>prompt-injection attack</strong> against the official GitHub MCP server: a malicious public GitHub issue could hijack an AI assistant and make it pull data from private repos, then leak that data back to a public repo. <a href=\"https://invariantlabs.ai/blog/mcp-github-vulnerability\">Invariant Labs link</a></p>\n</li>\n<li>\n<p><strong>Data breached &#x26; why:</strong> With a single over-privileged Personal Access Token wired into the MCP server, the compromised agent exfiltrated <strong>private repository contents, internal project details, and even personal financial/salary information</strong> into a public pull request. The root cause was <strong>broad PAT scopes combined with untrusted content (issues) in the LLM context</strong>, letting a prompt-injected agent abuse legitimate MCP tool calls.</p>\n</li>\n</ul>\n<h3 id=\"user-content-3-june-2025-asana-mcp-server-bug\"><strong>3. June 2025: Asana MCP Server Bug</strong></h3>\n<ul>\n<li>\n<p><strong>What happened:</strong> Asana discovered a bug in its MCP-server feature that could allow data belonging to one organisation to be seen by other organisations using their system. <a href=\"https://www.upguard.com/blog/asana-discloses-data-exposure-bug-in-mcp-server\">Upguard link</a>.</p>\n</li>\n<li>\n<p><strong>Data breached &#x26; why:</strong> Projects, teams, tasks and other Asana objects belonging to one customer potentially accessible by a different customer. This was caused by a logic flaw in the access control of their MCP-enabled integration (cross-tenant access not properly isolated).</p>\n</li>\n</ul>\n<h3 id=\"user-content-4-june-2025--anthropic-mcp-inspector-rce\"><strong>4. June 2025 – Anthropic MCP Inspector RCE</strong></h3>\n<ul>\n<li>\n<p><strong>What happened:</strong> Researchers found that Anthropic’s <strong>MCP Inspector</strong> developer tool allowed <strong>unauthenticated remote code execution</strong> via its inspector–proxy architecture. An attacker could get arbitrary commands run on a dev machine just by having the victim inspect a malicious MCP server, or even by driving the inspector from a browser. <a href=\"https://nvd.nist.gov/vuln/detail/CVE-2025-49596\">CVE Link</a></p>\n</li>\n<li>\n<p><strong>Data at risk &#x26; why:</strong> Because the inspector ran with the user’s privileges and lacked authentication while listening on localhost / 0.0.0.0, a successful exploit could expose <strong>the entire filesystem, API keys, and environment secrets</strong> on the developer workstation – effectively turning a debugging tool into a remote shell. <a href=\"https://medium.com/@vssec/when-local-isnt-local-the-critical-ai-security-flaw-you-probably-missed-8d99e554000e\">VSec Medium Link</a></p>\n</li>\n</ul>\n<hr>\n<h2 id=\"user-content-jul---sept-2025\">Jul - Sept 2025</h2>\n<h3 id=\"user-content-1-july-2025--mcp-remote-os-command-injection\"><strong>1. July 2025 – mcp-remote OS Command Injection</strong></h3>\n<ul>\n<li>\n<p><strong>What happened:</strong> JFrog disclosed <strong>CVE-2025-6514</strong>, a critical OS command-injection bug in <strong><code>mcp-remote</code></strong>, a popular OAuth proxy for connecting local MCP clients to remote servers. Malicious MCP servers could send a booby-trapped <code>authorization_endpoint</code> that <code>mcp-remote</code> passed straight into the system shell, achieving <strong>remote code execution on the client machine</strong>. <a href=\"https://nvd.nist.gov/vuln/detail/CVE-2025-6514\">CVE Link</a></p>\n</li>\n<li>\n<p><strong>Data at risk &#x26; why:</strong> With over <strong>437,000 downloads</strong> and adoption in Cloudflare, Hugging Face, Auth0 and other integration guides, the vuln effectively turned any unpatched install into a <strong>supply-chain backdoor</strong>: an attacker could execute arbitrary commands, steal <strong>API keys, cloud credentials, local files, SSH keys, and Git repo contents</strong>, all triggered by pointing your LLM host at a malicious MCP endpoint. <a href=\"https://www.docker.com/blog/mcp-horror-stories-the-supply-chain-attack/\">Docker Blog</a></p>\n</li>\n</ul>\n<h3 id=\"user-content-2-august-2025-anthropic-filesystem-mcp-server-vulnerabilities\"><strong>2. August 2025: Anthropic “Filesystem MCP Server” Vulnerabilities</strong></h3>\n<ul>\n<li>\n<p><strong>What happened:</strong> Security researchers found two critical flaws in Anthropic’s Filesystem-MCP server: <strong>sandbox escape and symlink/containment bypass</strong>, enabling arbitrary file access and code execution. <a href=\"https://cymulate.com/blog/cve-2025-53109-53110-escaperoute-anthropic/\">Cymulate Link</a></p>\n</li>\n<li>\n<p><strong>Data breached &#x26; why:</strong> Host filesystem access, meaning sensitive files, credentials, logs, or other data on servers could be impacted. The root cause was poor sandbox implementation and insufficient directory-containment enforcement in the MCP server’s file-tool interface.</p>\n</li>\n</ul>\n<h3 id=\"user-content-3-september-2025-malicious-mcp-server-in-the-wild\"><strong>3. September 2025: Malicious MCP Server in the Wild</strong></h3>\n<ul>\n<li>\n<p><strong>What happened:</strong> A malicious MCP server package masquerading as a legitimate “Postmark MCP Server” was found injecting BCC copies of all email communications (including confidential docs) to an attacker’s server. <a href=\"https://www.itpro.com/security/a-malicious-mcp-server-is-silently-stealing-user-emails\">IT Pro</a></p>\n</li>\n<li>\n<p><strong>Data breached &#x26; why:</strong> Emails, internal memos, invoices — essentially all mail traffic processed by that MCP server were exposed. This was due to a <strong>supply-chain compromise / malicious package</strong> in MCP ecosystem, and the fact that MCP servers often run with high-privilege accesses which were exploited.</p>\n</li>\n</ul>\n<hr>\n<h2 id=\"user-content-oct---dec-2025\">Oct - Dec 2025</h2>\n<h3 id=\"user-content-1-october-2025--smithery-mcp-hosting-supply-chain-breach\"><strong>1. October 2025 – Smithery MCP Hosting Supply-Chain Breach</strong></h3>\n<ul>\n<li>\n<p><strong>What happened:</strong> While researching Smithery’s hosted MCP server platform, GitGuardian found a <strong>path-traversal bug</strong> in the <code>smithery.yaml</code> build config. By setting <code>dockerBuildPath: \"..\"</code>, attackers could make the registry build Docker images from the <strong>builder’s home directory</strong>, then exfiltrate its contents and credentials. <a href=\"https://blog.gitguardian.com/breaking-mcp-server-hosting/\">GitGuardian Blog</a></p>\n</li>\n<li>\n<p><strong>Data breached &#x26; why:</strong> The exploit leaked the builder’s <strong><code>~/.docker/config.json</code></strong>, including a <strong>Fly.io API token</strong> that granted control over <strong>>3,000 apps</strong>, most of them hosted MCP servers. From there, attackers could run arbitrary commands in MCP server containers and tap <strong>inbound client traffic that contained API keys and other secrets</strong> for downstream services (e.g. Brave API keys), turning the MCP hosting service itself into a high-impact supply-chain compromise.</p>\n</li>\n</ul>\n<h3 id=\"user-content-2-october-2025-figmadevelopermcpframelink-mcp-vulnerability\"><strong>2. October 2025: figma‑developer‑mcp/Framelink MCP Vulnerability</strong></h3>\n<ul>\n<li>\n<p><strong>What happened:</strong> A command-injection flaw was discovered in the Figma/Framelink MCP integration: unsanitised user input in shell commands could lead to remote code execution. <a href=\"https://thehackernews.com/2025/10/severe-figma-mcp-vulnerability-lets.html\">The Hacker News Link</a></p>\n</li>\n<li>\n<p><strong>Data breached &#x26; why:</strong> Because the integration allowed AI-agents to interact with Figma docs, the flaw could enable attackers to run arbitrary commands through the MCP tooling and <strong>access design data or infrastructure</strong>. The root cause was the unsafe use of <code>child_process.exec</code> with untrusted input in the MCP server code - essentially a lack of input sanitisation.<a href=\"https://nvd.nist.gov/vuln/detail/CVE-2025-53967\">CVE Link</a></p>\n</li>\n</ul>\n<p>..And we’re sure there are more to come. We’ll keep this blog updated with the latest in security and data breaches in the MCP world.</p>\n<hr>\n<h2 id=\"user-content-patterns-emerging-across-incidents\">Patterns Emerging Across Incidents</h2>\n<p>Across all these breaches, common themes appear:</p>\n<p><strong>1. Local AI dev tools behave like exposed remote APIs</strong></p>\n<p><code>MCP Inspector</code>, <code>mcp-remote</code>, and similar tooling turned into Remote Code Execution (RCE) surfaces simply by trusting localhost connections.</p>\n<p><strong>2. Over-privileged API tokens are catastrophic in MCP workflows</strong></p>\n<p>GitHub MCP, Smithery, and WhatsApp attacks all exploited overly broad token scopes.</p>\n<p><strong>3. “Tool poisoning” is a new, AI-native supply chain vector</strong></p>\n<p>Traditional security tools don’t monitor changes to MCP tool descriptions.</p>\n<p><strong>4. Hosted MCP registries concentrate risk</strong></p>\n<p>Smithery illustrated what happens when thousands of tenants rely on a single build pipeline.</p>\n<p><strong>5. Prompt injection becomes a full data breach</strong></p>\n<p>The GitHub MCP incident demonstrated how natural language <em>alone</em> can cause exfiltration when MCP calls are available.</p>\n<hr>\n<h2 id=\"user-content-conclusion\">Conclusion:</h2>\n<p>The Model Context Protocol (MCP) presents a cutting-edge threat surface, yet the breaches detailed here are rooted in timeless flaws: over-privilege, inadequate input validation, and insufficient isolation.</p>\n<p>AI fundamentally changes the <em>interface</em>, but not the <em>fundamentals</em> of security. To secure the AI era, we must rigorously apply old-school principles of least privilege and zero-trust to these powerful new software components.</p>\n<p>As adoption accelerates, organisations must treat MCP surfaces with the same seriousness as API gateways, CI/CD pipelines, and Cloud IAM.</p>\n<p>Because attackers already are.</p>",
            "url": "https://authzed.com/blog/timeline-mcp-breaches",
            "title": "A Timeline of Model Context Protocol (MCP) Security Breaches",
            "summary": "AI fundamentally changes the interface, but not the fundamentals of security. Here's a timeline of security breaches in MCP Servers from the recent past.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-11-25T18:18:00.000Z",
            "date_published": "2025-11-25T18:18:00.000Z",
            "author": {
                "name": "Sohan Maheshwar",
                "url": "https://www.linkedin.com/in/sohanmaheshwar/"
            }
        },
        {
            "id": "https://authzed.com/blog/building-a-multi-tenant-rag-with-fine-grain-authorization-using-motia-and-spicedb",
            "content_html": "<blockquote>\n<p>Learn how to build a complete retrieval-augmented generation pipeline with multi-tenant authorization using Motia's event-driven framework, OpenAI embeddings, Pinecone vector search, SpiceDB permissions, and natural language querying.</p>\n</blockquote>\n<p>If I was hard-pressed to pick my favourite computer game of all time, I'd go with Stardew Valley (sorry, <a href=\"https://en.wikipedia.org/wiki/Dangerous_Dave\">Dangerous Dave</a>). The stats from my Nintendo Profile is all the proof you need:</p>\n<p><img src=\"/images/upload/img_0385.jpg\" alt=\"nintendo-stats\"></p>\n<p>Stardew Valley sits atop with 430 hours played and in second place is Mario Kart (not pictured) with ~45 hours played. That's a significant difference, and should indicate how much I adore this game.</p>\n<p>We've been talking about the importance of Fine-Grained Authorization and RAG recently, so when I sat down to build a sample usecase for a production-grade RAG with Fine-Grained Permissions, my immediate thought went to Stardew Valley.</p>\n<p>For those not familiar, Stardew Valley is a farm life simulation game where players manage a farm by clearing land, growing seasonal crops, and raising animals. So I thought I could build a logbook for a large farm that one could query using natural language processing. This usecase is ideal for RAG Pipelines (a technique that uses external data to improve the accuracy, relevancy, and usefulness of a LLM model’s output).</p>\n<p>I focused on building something that was as close to production-grade as possible (and perhaps strayed from the original intent of a single farm) where an organization can own farms and data from the farms. The farms contain harvest data and users can log and query data for the farms they're part of. This provides a sticky situation for the authorization model. How does a LLM know who has access to what data?</p>\n<p>Here's where SpiceDB and ReBAC was vital. By using metadata to indicate where the relevant embedings came from, the RAG system returned harvest data to the user only based on what data they had access to. In fact, <a href=\"https://authzed.com/customers/openai\">OpenAI uses SpiceDB</a> for their fine-grained authorization in ChatGPT Connectors using similar techniques.</p>\n<p>While I know my way around SpiceDB and authorization, I needed help to build out the other components for a production-grade harvest logbook. So I reached out to my friend <a href=\"https://www.linkedin.com/in/rohit-ghumare/\">Rohit Ghumare</a> from Motia for his expertise. <a href=\"https://motia.dev/\">Motia.dev</a> is a backend framework that unifies APIs, background jobs, workflows, and AI Agents into a single core primitive with built-in observability and state management</p>\n<p>Here's a photo of Rohit and myself at Kubecon Europe in 2025</p>\n<p><img src=\"https://hackmd.io/_uploads/r1KRbl-xZe.png\" alt=\"sohan and rohit\"></p>\n<p>What follows below is a tutorial-style post on building a Retrieval Augmented Generation system with fine-grained authorization using the Motia framework and SpiceDB. We'll use <a href=\"https://www.pinecone.io/\">Pinecone</a> as our vector database, and OpenAI as our LLM.</p>\n<h2 id=\"user-content-what-youll-build\">What You'll Build</h2>\n<p>In this tutorial, you'll create a complete RAG system with authorization that:</p>\n<ul>\n<li>Stores harvest data and automatically generates embeddings for semantic search</li>\n<li>Splits text into optimized chunks with overlap for better retrieval accuracy</li>\n<li>Implements fine-grained authorization using SpiceDB's relationship-based access control</li>\n<li>Queries harvest history using natural language with AI-powered responses</li>\n<li>Returns contextually relevant answers with source citations from vector search</li>\n<li>Supports multi-tenant access where users only see data they have permission to access</li>\n<li>Logs all queries and responses for audit trails in CSV or Google Sheets</li>\n<li>Runs as an event-driven workflow orchestrated through Motia's framework</li>\n</ul>\n<p>By the end of the tutorial, you'll have a complete system that combines semantic search with multi-tenant authorization.</p>\n<h2 id=\"user-content-prerequisites\">Prerequisites</h2>\n<p>Before starting the tutorial, ensure you have:</p>\n<ul>\n<li><a href=\"https://platform.openai.com/api-keys\">OpenAI API key</a> for embeddings and chat</li>\n<li><a href=\"https://app.pinecone.io/\">Pinecone account</a> with an index created (1536 dimensions, cosine metric)</li>\n<li><a href=\"https://docs.docker.com/get-docker/\">Docker</a> installed for running SpiceDB locally</li>\n</ul>\n<h2 id=\"user-content-getting-started\">Getting Started</h2>\n<h3 id=\"user-content-1-create-your-motia-project\">1. Create Your Motia Project</h3>\n<p>Create a new Motia project using the CLI:</p>\n<pre><code class=\"hljs language-bash\">npx motia@latest create\n</code></pre>\n<p>The installer will prompt you:</p>\n<ol>\n<li><strong>Template:</strong> Select <code>Base (TypeScript)</code></li>\n<li><strong>Project name:</strong> Enter <code>harvest-logbook-rag</code></li>\n<li><strong>Proceed?</strong> Type <code>Yes</code></li>\n</ol>\n<p>Navigate into your project:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-built_in\">cd</span> harvest-logbook-rag\n</code></pre>\n<p>Your initial project structure:</p>\n<pre><code>harvest-logbook-rag/\n├── src/\n│   └── services/\n│       └── pet-store/\n├── steps/\n│   └── petstore/\n├── .env\n└── package.json\n</code></pre>\n<p>The default template includes a pet store example. We'll replace this with our harvest logbook system. For more on Motia basics, see the <a href=\"https://www.motia.dev/docs/getting-started/quick-start\">Quick Start guide</a>.</p>\n<h3 id=\"user-content-2-install-dependencies\">2. Install Dependencies</h3>\n<p>Install the SpiceDB client for authorization:</p>\n<pre><code class=\"hljs language-bash\">npm install @authzed/authzed-node\n</code></pre>\n<p>This is the only additional package needed.</p>\n<h3 id=\"user-content-3-setup-pinecone\">3. Setup Pinecone</h3>\n<p>Pinecone will store the vector embeddings for semantic search.</p>\n<h4 id=\"user-content-create-a-pinecone-account\">Create a Pinecone Account</h4>\n<ol>\n<li>Go to <a href=\"https://app.pinecone.io/\">app.pinecone.io</a> and sign up</li>\n<li>Create a new project</li>\n</ol>\n<h4 id=\"user-content-create-an-index\">Create an Index</h4>\n<ol>\n<li>\n<p>Click <strong>Create Index</strong></p>\n</li>\n<li>\n<p>Configure:</p>\n<ul>\n<li><strong>Name:</strong> <code>harvest-logbook</code> (or your preference)</li>\n<li><strong>Dimensions:</strong> <code>1536</code> (for OpenAI embeddings)</li>\n<li><strong>Metric:</strong> <code>cosine</code></li>\n</ul>\n</li>\n<li>\n<p>Click <strong>Create Index</strong></p>\n</li>\n</ol>\n<h4 id=\"user-content-get-your-credentials\">Get Your Credentials</h4>\n<ol>\n<li>Go to <strong>API Keys</strong> in the sidebar</li>\n<li>Copy your <strong>API Key</strong></li>\n<li>Go back to your index</li>\n<li>Click the <strong>Connect</strong> tab</li>\n<li>Copy the <strong>Host</strong> (looks like: <code>your-index-abc123.svc.us-east-1.pinecone.io</code>)</li>\n</ol>\n<p>Save these for the next step.</p>\n<h3 id=\"user-content-4-setup-spicedb\">4. Setup SpiceDB</h3>\n<p>SpiceDB handles authorization and access control for the system.</p>\n<h4 id=\"user-content-start-spicedb-with-docker\">Start SpiceDB with Docker</h4>\n<p>Run this command to start SpiceDB locally:</p>\n<pre><code class=\"hljs language-bash\">docker run -d \\\n  --name spicedb \\\n  -p 50051:50051 \\\n  authzed/spicedb serve \\\n  --grpc-preshared-key <span class=\"hljs-string\">\"sometoken\"</span>\n</code></pre>\n<h4 id=\"user-content-verify-spicedb-is-running\">Verify SpiceDB is Running</h4>\n<p>Check that the container is running:</p>\n<pre><code class=\"hljs language-bash\">docker ps | grep spicedb\n</code></pre>\n<p>You should see output similar to:</p>\n<pre><code>6316f6cb50b4   authzed/spicedb   \"spicedb serve --grp…\"   31 seconds ago   Up 31 seconds   0.0.0.0:50051->50051/tcp   spicedb\n</code></pre>\n<p>SpiceDB is now running on <code>localhost:50051</code> and ready to handle authorization checks.</p>\n<h3 id=\"user-content-5-configure-environment-variables\">5. Configure Environment Variables</h3>\n<p>Create a <code>.env</code> file in the project root:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># OpenAI (Required for embeddings and chat)</span>\nOPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx\n\n<span class=\"hljs-comment\"># Pinecone (Required for vector storage)</span>\nPINECONE_API_KEY=pcsk_xxxxxxxxxxxxx\nPINECONE_INDEX_HOST=your-index-abc123.svc.us-east-1.pinecone.io\n\n<span class=\"hljs-comment\"># SpiceDB (Required for authorization)</span>\nSPICEDB_ENDPOINT=localhost:50051\nSPICEDB_TOKEN=sometoken\n\n<span class=\"hljs-comment\"># LLM Configuration (OpenAI is default)</span>\nUSE_OPENAI_CHAT=<span class=\"hljs-literal\">true</span>\n\n<span class=\"hljs-comment\"># Logging Configuration (CSV is default)</span>\nUSE_CSV_LOGGER=<span class=\"hljs-literal\">true</span>\n</code></pre>\n<p>Replace the placeholder values with your actual credentials from the previous steps.</p>\n<h3 id=\"user-content-6-initialize-spicedb-schema\">6. Initialize SpiceDB Schema</h3>\n<p>SpiceDB needs a schema that defines the authorization model for organizations, farms, and users.</p>\n<h4 id=\"user-content-create-the-schema-file\">Create the Schema File</h4>\n<p>Create <code>src/services/harvest-logbook/spicedb.schema</code> with the authorization model. <a href=\"https://authzed.com/docs/spicedb/concepts/schema\">A SpiceDB schema</a> defines the types of objects found your application, how those objects can relate to one another, and the permissions that can be computed off of those relations.</p>\n<p>Here's a snippet of the schema that defines <code>user</code>, <code>organization</code> and <code>farm</code> and the relations and permissions between them.</p>\n<pre><code>definition user {}\n\ndefinition organization {\n    relation admin: user\n    relation member: user\n    \n    permission view = admin + member\n    permission edit = admin + member\n    permission query = admin + member\n    permission manage = admin\n}\n\ndefinition farm {\n    relation organization: organization\n    relation owner: user\n    relation editor: user\n    relation viewer: user\n    \n    permission view = viewer + editor + owner + organization->view\n    permission edit = editor + owner + organization->edit\n    permission query = viewer + editor + owner + organization->query\n    permission manage = owner + organization->admin\n}\n</code></pre>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/spicedb.schema\">View the complete schema on GitHub</a></p>\n<p>The schema establishes:</p>\n<ul>\n<li>Organizations with admins and members</li>\n<li>Farms with owners, editors, and viewers</li>\n<li>Harvest entries linked to farms</li>\n<li>Permission inheritance (org members can access farms in their org)</li>\n</ul>\n<h4 id=\"user-content-create-setup-scripts\">Create Setup Scripts</h4>\n<p>Create a <code>scripts/</code> folder and add three files:</p>\n<p><strong><code>scripts/setup-spicedb-schema.ts</code></strong> - Reads the schema file and writes it to SpiceDB<br>\n<a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/scripts/setup-spicedb-schema.ts\">View on GitHub</a></p>\n<p><strong><code>scripts/verify-spicedb-schema.ts</code></strong> - Verifies the schema was written correctly<br>\n<a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/scripts/verify-spicedb-schema.ts\">View on GitHub</a></p>\n<p><strong><code>scripts/create-sample-permissions.ts</code></strong> - Creates sample users and permissions for testing<br>\n<a href=\"https://github.com/Taofiqq/motia-examples/blob/main/examples/harvest-logbook-rag/scripts/create-sample-permissions.ts\">View on GitHub</a></p>\n<h4 id=\"user-content-install-script-runner\">Install Script Runner</h4>\n<pre><code class=\"hljs language-bash\">npm install -D tsx\n</code></pre>\n<h4 id=\"user-content-add-scripts-to-packagejson\">Add Scripts to package.json</h4>\n<pre><code class=\"hljs language-json\"><span class=\"hljs-attr\">\"scripts\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">{</span>\n  <span class=\"hljs-attr\">\"spicedb:setup\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"tsx scripts/setup-spicedb-schema.ts\"</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"spicedb:verify\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"tsx scripts/verify-spicedb-schema.ts\"</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"spicedb:sample\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"tsx scripts/create-sample-permissions.ts\"</span>\n<span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<h4 id=\"user-content-run-the-setup\">Run the Setup</h4>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-comment\"># Write schema to SpiceDB</span>\nnpm run spicedb:setup\n</code></pre>\n<p>You should see output confirming the schema was written successfully:\n<img src=\"https://hackmd.io/_uploads/ryPEGryy-e.png\" alt=\"image\"></p>\n<p><strong>Verify it was written correctly</strong>:</p>\n<pre><code class=\"hljs language-bash!\">npm run spicedb:verify\n</code></pre>\n<p>This displays the complete authorization schema showing all definitions and permissions:\n<img src=\"https://hackmd.io/_uploads/BkCyQH11bl.png\" alt=\"image\"></p>\n<p>The output shows:</p>\n<ul>\n<li><strong>farm</strong> definition with owner/editor/viewer roles</li>\n<li><strong>harvest_entry</strong> definition linked to farms</li>\n<li><strong>organization</strong> definition with admin/member roles</li>\n<li><strong>query_session</strong> definition for RAG queries</li>\n<li>Permission rules for each resource type</li>\n</ul>\n<p><strong>Create sample user (user_alice as owner of farm_1):</strong></p>\n<pre><code class=\"hljs language-bash!\">npm run spicedb:sample\n</code></pre>\n<p><img src=\"https://hackmd.io/_uploads/SyiImSJJZe.png\" alt=\"image\"></p>\n<p>This creates <code>user_alice</code> as owner of <code>farm_1</code>, ready for testing.</p>\n<p>Your authorization system is now ready.</p>\n<h3 id=\"user-content-7-start-development-server\">7. Start Development Server</h3>\n<p>Start the Motia development server:</p>\n<pre><code class=\"hljs language-bash\">npm run dev\n</code></pre>\n<p>The server starts at <code>http://localhost:3000</code>. Open this URL in your browser to see the Motia Workbench.</p>\n<p>You'll see the default pet store example. We'll replace this with our harvest logbook system in the next sections.</p>\n<p><img src=\"https://hackmd.io/_uploads/BJxc8JaAex.png\" alt=\"image\"></p>\n<p>Your development environment is now ready. All services are connected:</p>\n<ul>\n<li>Motia running on <code>localhost:3000</code></li>\n<li>Pinecone index created and connected</li>\n<li>SpiceDB running with schema loaded</li>\n<li>Sample permissions created (<code>user_alice</code> owns <code>farm_1</code>)</li>\n</ul>\n<h2 id=\"user-content-exploring-the-project\">Exploring the Project</h2>\n<p>Before we start building, let's understand the architecture we're creating.</p>\n<h3 id=\"user-content-system-architecture\">System Architecture</h3>\n<pre><code>┌─────────────────────────────────────────────────────────────┐\n│  POST /harvest_logbook                                      │\n│  (Store harvest data + optional query)                      │\n└─────────┬───────────────────────────────────────────────────┘\n          │\n          ├─→ Authorization Middleware (SpiceDB)\n          │   - Check user has 'edit' permission on farm\n          │\n          ├─→ ReceiveHarvestData Step (API)\n          │   - Validate input\n          │   - Emit events\n          │\n          ├─→ ProcessEmbeddings Step (Event)\n          │   - Split text into chunks (400 chars, 40 overlap)\n          │   - Generate embeddings (OpenAI)\n          │   - Store vectors (Pinecone)\n          │\n          └─→ QueryAgent Step (Event) [if query provided]\n              - Retrieve similar content (Pinecone)\n              - Generate response (OpenAI/HuggingFace)\n              - Emit logging event\n              │\n              └─→ LogToSheets Step (Event)\n                  - Log query &#x26; response (CSV/Sheets)\n</code></pre>\n<h3 id=\"user-content-the-rag-pipeline\">The RAG Pipeline</h3>\n<p>Our system processes harvest data through these stages:</p>\n<ol>\n<li><strong>API Entry</strong> - Receive harvest data via REST endpoint</li>\n<li><strong>Text Chunking</strong> - Split content into overlapping chunks (400 chars, 40 overlap)</li>\n<li><strong>Embedding Generation</strong> - Convert chunks to vectors using OpenAI</li>\n<li><strong>Vector Storage</strong> - Store embeddings in Pinecone for semantic search</li>\n<li><strong>Query Processing</strong> - Search vectors and generate AI responses</li>\n<li><strong>Audit Logging</strong> - Log all queries and responses</li>\n</ol>\n<h3 id=\"user-content-event-driven-architecture\">Event-Driven Architecture</h3>\n<p>The system uses Motia's event-driven model:</p>\n<ul>\n<li><strong>API Steps</strong> handle HTTP requests</li>\n<li><strong>Event Steps</strong> process background tasks</li>\n<li>Steps communicate by emitting and subscribing to events</li>\n<li>Each step is independent and can be tested separately</li>\n</ul>\n<h3 id=\"user-content-authorization-layer\">Authorization Layer</h3>\n<p>Every API request passes through SpiceDB authorization:</p>\n<ul>\n<li>Users have relationships with resources (owner, editor, viewer)</li>\n<li>Permissions are checked before processing requests</li>\n<li>Multi-tenant by design (users only access their farms)</li>\n</ul>\n<h3 id=\"user-content-what-well-build\">What We'll Build</h3>\n<p>We'll create five main steps:</p>\n<ol>\n<li><strong>ReceiveHarvestData</strong> - API endpoint to store harvest entries</li>\n<li><strong>ProcessEmbeddings</strong> - Event handler for generating and storing embeddings</li>\n<li><strong>QueryAgent</strong> - Event handler for AI-powered queries</li>\n<li><strong>QueryOnly</strong> - Separate API endpoint for querying without storing data</li>\n<li><strong>LogToSheets</strong> - Event handler for audit logging</li>\n</ol>\n<p>Each component is a single file in the <code>steps/</code> directory. Motia automatically discovers and connects them based on the events they emit and subscribe to.</p>\n<h2 id=\"user-content-step-1-create-the-harvest-entry-api\">Step 1: Create the Harvest Entry API</h2>\n<h3 id=\"user-content-what-were-building\">What We're Building</h3>\n<p>In this step, we'll create an API endpoint that receives harvest log data and triggers the processing pipeline. This is the entry point that starts the entire RAG workflow.</p>\n<h3 id=\"user-content-why-this-step-matters\">Why This Step Matters</h3>\n<p>Every workflow needs an entry point. In Motia, API steps serve as the gateway between external requests and your event-driven system. By using Motia's <code>api</code> step type, you get automatic HTTP routing, request validation, and event emission, all without writing boilerplate server code. When a farmer calls this endpoint with their harvest data, it validates the input, checks authorization, stores the entry, and emits events that trigger the embedding generation and optional query processing.</p>\n<h3 id=\"user-content-create-the-step-file\">Create the Step File</h3>\n<p>Create a new file at <code>steps/harvest-logbook/receive-harvest-data.step.ts</code>.</p>\n<blockquote>\n<p>The complete source code for all steps is available on GitHub. You can reference the working implementation at any time.</p>\n</blockquote>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/steps/harvest-logbook/receive-harvest-data.step.ts\">View the complete Step 1 code on GitHub →</a></p>\n<p><img src=\"https://hackmd.io/_uploads/B1e_5Dr11Ze.png\" alt=\"image\"></p>\n<p>Now let's understand the key parts you'll be implementing:</p>\n<h3 id=\"user-content-input-validation\">Input Validation</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> bodySchema = z.<span class=\"hljs-title function_\">object</span>({\n  <span class=\"hljs-attr\">content</span>: z.<span class=\"hljs-title function_\">string</span>().<span class=\"hljs-title function_\">min</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'Content cannot be empty'</span>),\n  <span class=\"hljs-attr\">farmId</span>: z.<span class=\"hljs-title function_\">string</span>().<span class=\"hljs-title function_\">min</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'Farm ID is required for authorization'</span>),\n  <span class=\"hljs-attr\">metadata</span>: z.<span class=\"hljs-title function_\">record</span>(z.<span class=\"hljs-title function_\">any</span>()).<span class=\"hljs-title function_\">optional</span>(),\n  <span class=\"hljs-attr\">query</span>: z.<span class=\"hljs-title function_\">string</span>().<span class=\"hljs-title function_\">optional</span>()\n});\n</code></pre>\n<p>Zod validates that requests include the harvest content and farm ID. The <code>query</code> field is optional - if provided, the system will also answer a natural language question about the data after storing it.</p>\n<h3 id=\"user-content-step-configuration\">Step Configuration</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">export</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-attr\">config</span>: <span class=\"hljs-title class_\">ApiRouteConfig</span> = {\n  <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">'api'</span>,\n  <span class=\"hljs-attr\">name</span>: <span class=\"hljs-string\">'ReceiveHarvestData'</span>,\n  <span class=\"hljs-attr\">path</span>: <span class=\"hljs-string\">'/harvest_logbook'</span>,\n  <span class=\"hljs-attr\">method</span>: <span class=\"hljs-string\">'POST'</span>,\n  <span class=\"hljs-attr\">middleware</span>: [errorHandlerMiddleware, harvestEntryEditMiddleware],\n  <span class=\"hljs-attr\">emits</span>: [<span class=\"hljs-string\">'process-embeddings'</span>, <span class=\"hljs-string\">'query-agent'</span>],\n  bodySchema\n};\n</code></pre>\n<ul>\n<li><code>type: 'api'</code> makes this an HTTP endpoint</li>\n<li><code>middleware</code> runs authorization checks before the handler</li>\n<li><code>emits</code> declares this step triggers embedding processing and optional query events</li>\n<li>Motia handles all the routing automatically</li>\n</ul>\n<h3 id=\"user-content-authorization-check\">Authorization Check</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-attr\">middleware</span>: [errorHandlerMiddleware, harvestEntryEditMiddleware]\n</code></pre>\n<p>The <code>harvestEntryEditMiddleware</code> checks SpiceDB to ensure the user has <code>edit</code> permission on the specified farm. If authorization fails, the request is rejected before reaching the handler. Authorization info is added to the request for use in the handler.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/middlewares/authz.middleware.ts\">View authorization middleware →</a></p>\n<h3 id=\"user-content-handler-logic\">Handler Logic</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">export</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-attr\">handler</span>: <span class=\"hljs-title class_\">Handlers</span>[<span class=\"hljs-string\">'ReceiveHarvestData'</span>] = <span class=\"hljs-title function_\">async</span> (req, { emit, logger, state }) => {\n  <span class=\"hljs-keyword\">const</span> { content, farmId, metadata, query } = bodySchema.<span class=\"hljs-title function_\">parse</span>(req.<span class=\"hljs-property\">body</span>);\n  <span class=\"hljs-keyword\">const</span> entryId = <span class=\"hljs-string\">`harvest-<span class=\"hljs-subst\">${<span class=\"hljs-built_in\">Date</span>.now()}</span>`</span>;\n  \n  <span class=\"hljs-comment\">// Store entry data in state</span>\n  <span class=\"hljs-keyword\">await</span> state.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">'harvest-entries'</span>, entryId, {\n    content, farmId, metadata, <span class=\"hljs-attr\">timestamp</span>: <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">Date</span>().<span class=\"hljs-title function_\">toISOString</span>()\n  });\n  \n  <span class=\"hljs-comment\">// Emit event to process embeddings</span>\n  <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title function_\">emit</span>({\n    <span class=\"hljs-attr\">topic</span>: <span class=\"hljs-string\">'process-embeddings'</span>,\n    <span class=\"hljs-attr\">data</span>: { entryId, content, metadata }\n  });\n};\n</code></pre>\n<p>The handler generates a unique entry ID, stores the data in Motia's state management, and emits an event to trigger embedding processing. If a query was provided, it also emits a <code>query-agent</code> event.</p>\n<h3 id=\"user-content-event-emission\">Event Emission</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">await</span> <span class=\"hljs-title function_\">emit</span>({\n  <span class=\"hljs-attr\">topic</span>: <span class=\"hljs-string\">'process-embeddings'</span>,\n  <span class=\"hljs-attr\">data</span>: { entryId, content, <span class=\"hljs-attr\">metadata</span>: { ...metadata, farmId, userId } }\n});\n\n<span class=\"hljs-keyword\">if</span> (query) {\n  <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title function_\">emit</span>({\n    <span class=\"hljs-attr\">topic</span>: <span class=\"hljs-string\">'query-agent'</span>,\n    <span class=\"hljs-attr\">data</span>: { entryId, query }\n  });\n}\n</code></pre>\n<p>Events are how Motia steps communicate. The <code>process-embeddings</code> event triggers the next step to chunk the text and generate embeddings. If a query was provided, the <code>query-agent</code> event runs in parallel to answer the question using RAG.</p>\n<p>This keeps the API response fast as it returns immediately while processing happens in the background.</p>\n<h3 id=\"user-content-test-the-step\">Test the Step</h3>\n<p>Open the Motia Workbench and test this endpoint:</p>\n<ol>\n<li>Click on the <code>harvest-logbook</code> flow</li>\n<li>Find <code>POST /harvest_logbook</code> in the sidebar</li>\n<li>Click on it to open the request panel</li>\n<li>Switch to the <strong>Headers</strong> tab and add:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"x-user-id\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"user_alice\"</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"5\">\n<li>Switch to the <strong>Body</strong> tab and add:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"content\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"Harvested 500kg of tomatoes from field A. Weather was sunny.\"</span><span class=\"hljs-punctuation\">,</span>\n     <span class=\"hljs-attr\">\"farmId\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"farm_1\"</span><span class=\"hljs-punctuation\">,</span>\n     <span class=\"hljs-attr\">\"metadata\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">{</span>\n       <span class=\"hljs-attr\">\"field\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"A\"</span><span class=\"hljs-punctuation\">,</span>\n       <span class=\"hljs-attr\">\"crop\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"tomatoes\"</span>\n     <span class=\"hljs-punctuation\">}</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"6\">\n<li>Click <strong>Send</strong> button.</li>\n</ol>\n<p>You should see a success response with the entry ID. The Workbench will show the workflow executing in real-time, with events flowing to the next steps.</p>\n<p><lite-youtube videoid=\"81Oe2v9C8t0\" playlabel=\"Motia + SpiceDB RAG\"></lite-youtube></p>\n<h2 id=\"user-content-step-2-process-embeddings\">Step 2: Process Embeddings</h2>\n<h3 id=\"user-content-what-were-building-1\">What We're Building</h3>\n<p>This event handler takes the harvest data from Step 1, splits it into chunks, generates vector embeddings, and stores them in Pinecone for semantic search.</p>\n<h3 id=\"user-content-why-this-step-matters-1\">Why This Step Matters</h3>\n<p>RAG systems need to break down large text into smaller chunks for better retrieval accuracy. By chunking text with overlap and generating embeddings for each piece, we enable semantic search that finds relevant context even when queries don't match exact keywords.</p>\n<p>This step runs in the background after the API returns, keeping the user experience fast while handling the background work of embedding generation and vector storage.</p>\n<h3 id=\"user-content-create-the-step-file-1\">Create the Step File</h3>\n<p>Create a new file at <code>steps/harvest-logbook/process-embeddings.step.ts</code>.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/steps/harvest-logbook/process-embeddings.step.ts\">View the complete Step 2 code on GitHub →</a></p>\n<p>Now let's understand the key parts you'll be implementing:</p>\n<h3 id=\"user-content-input-schema\">Input Schema</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> inputSchema = z.<span class=\"hljs-title function_\">object</span>({\n  <span class=\"hljs-attr\">entryId</span>: z.<span class=\"hljs-title function_\">string</span>(),\n  <span class=\"hljs-attr\">content</span>: z.<span class=\"hljs-title function_\">string</span>(),\n  <span class=\"hljs-attr\">metadata</span>: z.<span class=\"hljs-title function_\">record</span>(z.<span class=\"hljs-title function_\">any</span>()).<span class=\"hljs-title function_\">optional</span>()\n});\n</code></pre>\n<p>This step receives the entry ID, content, and metadata from the previous step's event emission.</p>\n<h3 id=\"user-content-step-configuration-1\">Step Configuration</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">export</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-attr\">config</span>: <span class=\"hljs-title class_\">EventConfig</span> = {\n  <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">'event'</span>,\n  <span class=\"hljs-attr\">name</span>: <span class=\"hljs-string\">'ProcessEmbeddings'</span>,\n  <span class=\"hljs-attr\">subscribes</span>: [<span class=\"hljs-string\">'process-embeddings'</span>],\n  <span class=\"hljs-attr\">emits</span>: [],\n  <span class=\"hljs-attr\">input</span>: inputSchema\n};\n</code></pre>\n<ul>\n<li><code>type: 'event'</code> makes this a background event handler</li>\n<li><code>subscribes: ['process-embeddings']</code> listens for events from Step 1</li>\n<li>No emits - this is the end of the embedding pipeline</li>\n</ul>\n<h3 id=\"user-content-text-chunking\">Text Chunking</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> vectorIds = <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title class_\">HarvestLogbookService</span>.<span class=\"hljs-title function_\">storeEntry</span>({\n  <span class=\"hljs-attr\">id</span>: entryId,\n  content,\n  metadata,\n  <span class=\"hljs-attr\">timestamp</span>: <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">Date</span>().<span class=\"hljs-title function_\">toISOString</span>()\n});\n</code></pre>\n<p>The service handles text splitting (400 character chunks with 40 character overlap), embedding generation via OpenAI, and storage in Pinecone. This chunking strategy ensures semantic continuity across chunks.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/text-splitter.ts\">View text splitter service →</a></p>\n<h3 id=\"user-content-embedding-generation\">Embedding Generation</h3>\n<p>The OpenAI service generates 1536-dimension embeddings for each text chunk using the <code>text-embedding-ada-002</code> model.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/openai-service.ts\">View OpenAI service →</a></p>\n<h3 id=\"user-content-vector-storage\">Vector Storage</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">await</span> state.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">'harvest-vectors'</span>, entryId, {\n  vectorIds,\n  <span class=\"hljs-attr\">processedAt</span>: <span class=\"hljs-keyword\">new</span> <span class=\"hljs-title class_\">Date</span>().<span class=\"hljs-title function_\">toISOString</span>(),\n  <span class=\"hljs-attr\">chunkCount</span>: vectorIds.<span class=\"hljs-property\">length</span>\n});\n</code></pre>\n<p>After storing vectors in Pinecone, the step updates Motia's state with the vector IDs for tracking. Each chunk gets a unique ID like <code>harvest-123-chunk-0</code>, <code>harvest-123-chunk-1</code>, etc.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/pinecone-service.ts\">View Pinecone service →</a></p>\n<p>The embeddings are now stored and ready for semantic search when users query the system.</p>\n<h3 id=\"user-content-test-the-step-1\">Test the Step</h3>\n<p>Step 2 runs automatically when Step 1 emits the <code>process-embeddings</code> event. To test it:</p>\n<ol>\n<li>\n<p>Send a request to the <code>POST /harvest_logbook</code> endpoint (from Step 1)</p>\n</li>\n<li>\n<p>In the Workbench, watch the workflow visualization</p>\n</li>\n<li>\n<p>You'll see the <code>ProcessEmbeddings</code> step activate automatically</p>\n</li>\n<li>\n<p>Check the <strong>Logs</strong> tab at the bottom to see:</p>\n<ul>\n<li>Text chunking progress</li>\n<li>Embedding generation</li>\n<li>Vector storage confirmation</li>\n</ul>\n</li>\n</ol>\n<p>The step completes when you see \"Successfully stored embeddings\" in the logs. The vectors are now in Pinecone and ready for semantic search.</p>\n<p><lite-youtube videoid=\"81Oe2v9C8t0\" playlabel=\"Motia + SpiceDB RAG\"></lite-youtube></p>\n<h2 id=\"user-content-step-3-query-agent\">Step 3: Query Agent</h2>\n<h3 id=\"user-content-what-were-building-2\">What We're Building</h3>\n<p>This event handler performs the RAG query, it searches Pinecone for relevant content, retrieves matching chunks, and uses an LLM to generate natural language responses based on the retrieved context.</p>\n<h3 id=\"user-content-why-this-step-matters-2\">Why This Step Matters</h3>\n<p>This is where retrieval-augmented generation happens. Instead of the LLM generating responses from its training data alone, it uses actual harvest data from Pinecone as context. This ensures accurate, source-backed answers specific to the user's farm data.</p>\n<p>The step supports both OpenAI and HuggingFace LLMs, giving you flexibility in choosing your AI provider based on cost and performance needs.</p>\n<h3 id=\"user-content-create-the-step-file-2\">Create the Step File</h3>\n<p>Create a new file at <code>steps/harvest-logbook/query-agent.step.ts</code>.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/steps/harvest-logbook/query-agent.step.ts\">View the complete Step 3 code on GitHub →</a></p>\n<p>Now let's understand the key parts you'll be implementing:</p>\n<h3 id=\"user-content-input-schema-1\">Input Schema</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> inputSchema = z.<span class=\"hljs-title function_\">object</span>({\n  <span class=\"hljs-attr\">entryId</span>: z.<span class=\"hljs-title function_\">string</span>(),\n  <span class=\"hljs-attr\">query</span>: z.<span class=\"hljs-title function_\">string</span>(),\n  <span class=\"hljs-attr\">conversationHistory</span>: z.<span class=\"hljs-title function_\">array</span>(z.<span class=\"hljs-title function_\">object</span>({\n    <span class=\"hljs-attr\">role</span>: z.<span class=\"hljs-title function_\">enum</span>([<span class=\"hljs-string\">'user'</span>, <span class=\"hljs-string\">'assistant'</span>, <span class=\"hljs-string\">'system'</span>]),\n    <span class=\"hljs-attr\">content</span>: z.<span class=\"hljs-title function_\">string</span>()\n  })).<span class=\"hljs-title function_\">optional</span>()\n});\n</code></pre>\n<p>The step receives the query text and optional conversation history for multi-turn conversations.</p>\n<h3 id=\"user-content-step-configuration-2\">Step Configuration</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">export</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-attr\">config</span>: <span class=\"hljs-title class_\">EventConfig</span> = {\n  <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">'event'</span>,\n  <span class=\"hljs-attr\">name</span>: <span class=\"hljs-string\">'QueryAgent'</span>,\n  <span class=\"hljs-attr\">subscribes</span>: [<span class=\"hljs-string\">'query-agent'</span>],\n  <span class=\"hljs-attr\">emits</span>: [<span class=\"hljs-string\">'log-to-sheets'</span>],\n  <span class=\"hljs-attr\">input</span>: inputSchema\n};\n</code></pre>\n<ul>\n<li><code>subscribes: ['query-agent']</code> listens for query events from Step 1</li>\n<li><code>emits: ['log-to-sheets']</code> triggers logging after generating response</li>\n</ul>\n<h3 id=\"user-content-rag-query-process\">RAG Query Process</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> agentResponse = <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title class_\">HarvestLogbookService</span>.<span class=\"hljs-title function_\">queryWithAgent</span>({\n  query,\n  conversationHistory\n});\n</code></pre>\n<p>The service orchestrates the RAG pipeline: embedding the query, searching Pinecone for similar vectors, extracting context from top matches, and generating a response using the LLM.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/index.ts\">View RAG orchestration service →</a></p>\n<h3 id=\"user-content-vector-search\">Vector Search</h3>\n<p>The query is embedded using OpenAI and searched against Pinecone to find the top 5 most similar chunks. Each result includes a similarity score and the original text.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/pinecone-service.ts\">View Pinecone query implementation →</a></p>\n<h3 id=\"user-content-llm-response-generation\">LLM Response Generation</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">await</span> state.<span class=\"hljs-title function_\">set</span>(<span class=\"hljs-string\">'agent-responses'</span>, entryId, {\n  query,\n  <span class=\"hljs-attr\">response</span>: agentResponse.<span class=\"hljs-property\">response</span>,\n  <span class=\"hljs-attr\">sources</span>: agentResponse.<span class=\"hljs-property\">sources</span>,\n  <span class=\"hljs-attr\">timestamp</span>: agentResponse.<span class=\"hljs-property\">timestamp</span>\n});\n</code></pre>\n<p>The LLM generates a response using the retrieved context. The system supports both OpenAI (default) and HuggingFace, controlled by the <code>USE_OPENAI_CHAT</code> environment variable. The response includes source citations showing which harvest entries informed the answer.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/openai-chat-service.ts\">View OpenAI chat service →</a><br>\n<a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/huggingface-service.ts\">View HuggingFace service →</a></p>\n<h3 id=\"user-content-event-emission-1\">Event Emission</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">await</span> <span class=\"hljs-title function_\">emit</span>({\n  <span class=\"hljs-attr\">topic</span>: <span class=\"hljs-string\">'log-to-sheets'</span>,\n  <span class=\"hljs-attr\">data</span>: {\n    entryId,\n    query,\n    <span class=\"hljs-attr\">response</span>: agentResponse.<span class=\"hljs-property\">response</span>,\n    <span class=\"hljs-attr\">sources</span>: agentResponse.<span class=\"hljs-property\">sources</span>\n  }\n});\n</code></pre>\n<p>After generating the response, the step emits a logging event to create an audit trail of all queries and responses.</p>\n<h3 id=\"user-content-test-the-step-2\">Test the Step</h3>\n<p>Step 3 runs automatically when you include a <code>query</code> field in the Step 1 request. To test it:</p>\n<ol>\n<li>Send a request to <code>POST /harvest_logbook</code> with a query:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"content\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"Harvested 500kg of tomatoes from field A. Weather was sunny.\"</span><span class=\"hljs-punctuation\">,</span>\n     <span class=\"hljs-attr\">\"farmId\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"farm_1\"</span><span class=\"hljs-punctuation\">,</span>\n     <span class=\"hljs-attr\">\"query\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"What crops did we harvest?\"</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"2\">\n<li>\n<p>In the Workbench, watch the <code>QueryAgent</code> step activate</p>\n</li>\n<li>\n<p>Check the <strong>Logs</strong> tab to see:</p>\n<ul>\n<li>Query embedding generation</li>\n<li>Vector search in Pinecone</li>\n<li>LLM response generation</li>\n<li>Source citations</li>\n</ul>\n</li>\n</ol>\n<p>The step completes when you see the AI-generated response in the logs. The query and response are automatically logged by Step 5.</p>\n<p><lite-youtube videoid=\"tOFO-Dp-atY\" playlabel=\"Motia + SpiceDB RAG\"></lite-youtube></p>\n<h2 id=\"user-content-step-4-query-only-endpoint\">Step 4: Query-Only Endpoint</h2>\n<h3 id=\"user-content-what-were-building-3\">What We're Building</h3>\n<p>This API endpoint allows users to query their existing harvest data without storing new entries. It's a separate endpoint dedicated purely to RAG queries.</p>\n<h3 id=\"user-content-why-this-step-matters-3\">Why This Step Matters</h3>\n<p>While Step 1 handles both storing and optionally querying data, users often need to just ask questions about their existing harvest logs. This dedicated endpoint keeps the API clean and focused - one endpoint for data entry, another for pure queries.</p>\n<p>This separation also makes it easier to apply different rate limits or permissions between data modification and read-only operations.</p>\n<h3 id=\"user-content-create-the-step-file-3\">Create the Step File</h3>\n<p>Create a new file at <code>steps/harvest-logbook/query-only.step.ts</code>.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/steps/harvest-logbook/query-only.step.ts\">View the complete Step 4 code on GitHub →</a></p>\n<p>Now let's understand the key parts you'll be implementing:</p>\n<h3 id=\"user-content-input-validation-1\">Input Validation</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> bodySchema = z.<span class=\"hljs-title function_\">object</span>({\n  <span class=\"hljs-attr\">query</span>: z.<span class=\"hljs-title function_\">string</span>().<span class=\"hljs-title function_\">min</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'Query cannot be empty'</span>),\n  <span class=\"hljs-attr\">farmId</span>: z.<span class=\"hljs-title function_\">string</span>().<span class=\"hljs-title function_\">min</span>(<span class=\"hljs-number\">1</span>, <span class=\"hljs-string\">'Farm ID is required for authorization'</span>),\n  <span class=\"hljs-attr\">conversationHistory</span>: z.<span class=\"hljs-title function_\">array</span>(z.<span class=\"hljs-title function_\">object</span>({\n    <span class=\"hljs-attr\">role</span>: z.<span class=\"hljs-title function_\">enum</span>([<span class=\"hljs-string\">'user'</span>, <span class=\"hljs-string\">'assistant'</span>, <span class=\"hljs-string\">'system'</span>]),\n    <span class=\"hljs-attr\">content</span>: z.<span class=\"hljs-title function_\">string</span>()\n  })).<span class=\"hljs-title function_\">optional</span>()\n});\n</code></pre>\n<p>The request requires a query and farm ID. Conversation history is optional for multi-turn conversations.</p>\n<h3 id=\"user-content-step-configuration-3\">Step Configuration</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">export</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-attr\">config</span>: <span class=\"hljs-title class_\">ApiRouteConfig</span> = {\n  <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">'api'</span>,\n  <span class=\"hljs-attr\">name</span>: <span class=\"hljs-string\">'QueryHarvestLogbook'</span>,\n  <span class=\"hljs-attr\">path</span>: <span class=\"hljs-string\">'/harvest_logbook/query'</span>,\n  <span class=\"hljs-attr\">method</span>: <span class=\"hljs-string\">'POST'</span>,\n  <span class=\"hljs-attr\">middleware</span>: [errorHandlerMiddleware, harvestQueryMiddleware],\n  <span class=\"hljs-attr\">emits</span>: [<span class=\"hljs-string\">'query-agent'</span>]\n};\n</code></pre>\n<ul>\n<li><code>path: '/harvest_logbook/query'</code> creates a dedicated query endpoint</li>\n<li><code>harvestQueryMiddleware</code> checks for <code>query</code> permission (not <code>edit</code>)</li>\n<li><code>emits: ['query-agent']</code> triggers the same RAG query handler as Step 3</li>\n</ul>\n<h3 id=\"user-content-authorization-middleware\">Authorization Middleware</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-attr\">middleware</span>: [errorHandlerMiddleware, harvestQueryMiddleware]\n</code></pre>\n<p>The <code>harvestQueryMiddleware</code> checks SpiceDB for <code>query</code> permission. This is less restrictive than <code>edit</code> - viewers can query but cannot modify data.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/middlewares/authz.middleware.ts\">View authorization middleware →</a></p>\n<h3 id=\"user-content-handler-logic-1\">Handler Logic</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">export</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-attr\">handler</span>: <span class=\"hljs-title class_\">Handlers</span>[<span class=\"hljs-string\">'QueryHarvestLogbook'</span>] = <span class=\"hljs-title function_\">async</span> (req, { emit, logger }) => {\n  <span class=\"hljs-keyword\">const</span> { query, farmId } = bodySchema.<span class=\"hljs-title function_\">parse</span>(req.<span class=\"hljs-property\">body</span>);\n  <span class=\"hljs-keyword\">const</span> queryId = <span class=\"hljs-string\">`query-<span class=\"hljs-subst\">${<span class=\"hljs-built_in\">Date</span>.now()}</span>`</span>;\n  \n  <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title function_\">emit</span>({\n    <span class=\"hljs-attr\">topic</span>: <span class=\"hljs-string\">'query-agent'</span>,\n    <span class=\"hljs-attr\">data</span>: { <span class=\"hljs-attr\">entryId</span>: queryId, query }\n  });\n  \n  <span class=\"hljs-keyword\">return</span> {\n    <span class=\"hljs-attr\">status</span>: <span class=\"hljs-number\">200</span>,\n    <span class=\"hljs-attr\">body</span>: { <span class=\"hljs-attr\">success</span>: <span class=\"hljs-literal\">true</span>, queryId }\n  };\n};\n</code></pre>\n<p>The handler generates a unique query ID and emits the same <code>query-agent</code> event used in Step 1. This reuses the RAG pipeline from Step 3 without duplicating code.</p>\n<p>The API returns immediately with the query ID. The actual processing happens in the background, and results are logged by Step 5.</p>\n<h3 id=\"user-content-test-the-step-3\">Test the Step</h3>\n<p>This is the dedicated query endpoint. Test it directly:</p>\n<ol>\n<li>Click on <code>POST /harvest_logbook/query</code> in the Workbench</li>\n<li>Add the header:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"x-user-id\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"user_alice\"</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"3\">\n<li>Add the body:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"query\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"What crops did we harvest?\"</span><span class=\"hljs-punctuation\">,</span>\n     <span class=\"hljs-attr\">\"farmId\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"farm_1\"</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"4\">\n<li>Click <strong>Send</strong></li>\n</ol>\n<p>You'll see a <code>200 OK</code> response with the query ID. In the <strong>Logs</strong> tab, watch for:</p>\n<ul>\n<li><code>QueryHarvestLogbook</code> - Authorization and query received</li>\n<li><code>QueryAgent</code> - Querying AI agent</li>\n<li><code>QueryAgent</code> - Agent query completed</li>\n</ul>\n<p>The query runs in the background and results are logged by Step 5. This endpoint is perfect for read-only query operations without storing new data.</p>\n<p><lite-youtube videoid=\"tOFO-Dp-atY\" playlabel=\"Motia + SpiceDB RAG\"></lite-youtube></p>\n<h2 id=\"user-content-step-5-log-to-sheets\">Step 5: Log to Sheets</h2>\n<h3 id=\"user-content-what-were-building-4\">What We're Building</h3>\n<p>This event handler creates an audit trail by logging every query and its AI-generated response. It supports both local CSV files (for development) and Google Sheets (for production).</p>\n<h3 id=\"user-content-why-this-step-matters-4\">Why This Step Matters</h3>\n<p>Audit logs are essential for understanding how users interact with your system. They help with debugging, monitoring usage patterns, and maintaining compliance. By logging queries and responses, you can track what questions users ask, identify common patterns, and improve the system over time.</p>\n<p>The dual logging strategy (CSV/Google Sheets) gives you flexibility, use CSV locally for quick testing, then switch to Google Sheets for production without changing code.</p>\n<h3 id=\"user-content-create-the-step-file-4\">Create the Step File</h3>\n<p>Create a new file at <code>steps/harvest-logbook/log-to-sheets.step.ts</code>.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/steps/harvest-logbook/log-to-sheets.step.ts\">View the complete Step 5 code on GitHub →</a></p>\n<p>Now let's understand the key parts you'll be implementing:</p>\n<h3 id=\"user-content-input-schema-2\">Input Schema</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> inputSchema = z.<span class=\"hljs-title function_\">object</span>({\n  <span class=\"hljs-attr\">entryId</span>: z.<span class=\"hljs-title function_\">string</span>(),\n  <span class=\"hljs-attr\">query</span>: z.<span class=\"hljs-title function_\">string</span>(),\n  <span class=\"hljs-attr\">response</span>: z.<span class=\"hljs-title function_\">string</span>(),\n  <span class=\"hljs-attr\">sources</span>: z.<span class=\"hljs-title function_\">array</span>(z.<span class=\"hljs-title function_\">string</span>()).<span class=\"hljs-title function_\">optional</span>()\n});\n</code></pre>\n<p>The step receives the query, AI response, and optional source citations from Step 3.</p>\n<h3 id=\"user-content-step-configuration-4\">Step Configuration</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">export</span> <span class=\"hljs-keyword\">const</span> <span class=\"hljs-attr\">config</span>: <span class=\"hljs-title class_\">EventConfig</span> = {\n  <span class=\"hljs-attr\">type</span>: <span class=\"hljs-string\">'event'</span>,\n  <span class=\"hljs-attr\">name</span>: <span class=\"hljs-string\">'LogToSheets'</span>,\n  <span class=\"hljs-attr\">subscribes</span>: [<span class=\"hljs-string\">'log-to-sheets'</span>],\n  <span class=\"hljs-attr\">emits</span>: [],\n  <span class=\"hljs-attr\">input</span>: inputSchema\n};\n</code></pre>\n<ul>\n<li><code>subscribes: ['log-to-sheets']</code> listens for logging events from Step 3</li>\n<li>No emits - this is the end of the workflow</li>\n</ul>\n<h3 id=\"user-content-logging-service-selection\">Logging Service Selection</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">const</span> useCSV = process.<span class=\"hljs-property\">env</span>.<span class=\"hljs-property\">USE_CSV_LOGGER</span> === <span class=\"hljs-string\">'true'</span> || !process.<span class=\"hljs-property\">env</span>.<span class=\"hljs-property\">GOOGLE_SHEETS_ID</span>;\n\n<span class=\"hljs-keyword\">await</span> <span class=\"hljs-title class_\">HarvestLogbookService</span>.<span class=\"hljs-title function_\">logToSheets</span>(query, response, sources);\n</code></pre>\n<p>The service automatically chooses between CSV and Google Sheets based on environment variables. This keeps the step code simple while supporting different deployment scenarios.</p>\n<p><a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/csv-logger.ts\">View CSV logger →</a><br>\n<a href=\"https://github.com/MotiaDev/motia-examples/blob/main/examples/harvest-logbook-rag/src/services/harvest-logbook/sheets-service.ts\">View Google Sheets service →</a></p>\n<h3 id=\"user-content-error-handling\">Error Handling</h3>\n<pre><code class=\"hljs language-typescript\"><span class=\"hljs-keyword\">try</span> {\n  <span class=\"hljs-keyword\">await</span> <span class=\"hljs-title class_\">HarvestLogbookService</span>.<span class=\"hljs-title function_\">logToSheets</span>(query, response, sources);\n  logger.<span class=\"hljs-title function_\">info</span>(<span class=\"hljs-string\">`Successfully logged to <span class=\"hljs-subst\">${destination}</span>`</span>);\n} <span class=\"hljs-keyword\">catch</span> (error) {\n  logger.<span class=\"hljs-title function_\">error</span>(<span class=\"hljs-string\">'Failed to log query response'</span>);\n  <span class=\"hljs-comment\">// Don't throw - logging failures shouldn't break the main flow</span>\n}\n</code></pre>\n<p>The step catches logging errors without throwing. This ensures that even if logging fails, the main workflow completes successfully. Users get their query results even if the audit log has issues.</p>\n<h3 id=\"user-content-csv-output-format\">CSV Output Format</h3>\n<p>The CSV logger saves entries to <code>logs/harvest_logbook.csv</code> with these columns:</p>\n<ul>\n<li>Timestamp</li>\n<li>Query</li>\n<li>Response</li>\n<li>Sources (comma-separated)</li>\n</ul>\n<p>Each entry is automatically escaped to handle quotes and commas in the content.</p>\n<h3 id=\"user-content-test-the-step-4\">Test the Step</h3>\n<p>Step 5 runs automatically after Step 3 completes. To verify it's working:</p>\n<ol>\n<li>Run a query using <code>POST /harvest_logbook/query</code></li>\n<li>Check the <strong>Logs</strong> tab for <code>LogToSheets</code> entries</li>\n<li>Verify the CSV file was created:</li>\n</ol>\n<pre><code class=\"hljs language-bash\">   <span class=\"hljs-built_in\">cat</span> logs/harvest_logbook.csv\n</code></pre>\n<p><lite-youtube videoid=\"lnftmYJ-YD4\" playlabel=\"Motia + SpiceDB RAG\"></lite-youtube></p>\n<p>You should see your query and response logged with a timestamp. Each subsequent query appends a new row to the CSV file.</p>\n<p><img src=\"https://hackmd.io/_uploads/SyShZjJJZe.png\" alt=\"image\"></p>\n<h2 id=\"user-content-testing-the-system\">Testing the System</h2>\n<p>Now that all steps are built, let's test the complete workflow using the Motia Workbench.</p>\n<h3 id=\"user-content-start-the-server\">Start the Server</h3>\n<pre><code class=\"hljs language-bash\">npm run dev\n</code></pre>\n<p>Open <code>http://localhost:3000</code> in your browser to access the Workbench.</p>\n<h3 id=\"user-content-test-1-store-harvest-data\">Test 1: Store Harvest Data</h3>\n<ol>\n<li>Select the <code>harvest-logbook</code> flow from the dropdown</li>\n<li>Find the <code>POST /harvest_logbook</code> endpoint in the workflow</li>\n<li>Click on it to open the request panel</li>\n<li>Add the authorization header:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"x-user-id\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"user_alice\"</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"5\">\n<li>Set the request body:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"content\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"Harvested 500kg of tomatoes from field A. Weather was sunny, no pest damage observed.\"</span><span class=\"hljs-punctuation\">,</span>\n     <span class=\"hljs-attr\">\"farmId\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"farm_1\"</span><span class=\"hljs-punctuation\">,</span>\n     <span class=\"hljs-attr\">\"metadata\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">{</span>\n       <span class=\"hljs-attr\">\"field\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"A\"</span><span class=\"hljs-punctuation\">,</span>\n       <span class=\"hljs-attr\">\"crop\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"tomatoes\"</span><span class=\"hljs-punctuation\">,</span>\n       <span class=\"hljs-attr\">\"weight_kg\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">500</span>\n     <span class=\"hljs-punctuation\">}</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"6\">\n<li>Click <strong>Play</strong> Button</li>\n</ol>\n<p>Watch the workflow execute in real-time. You'll see:</p>\n<ul>\n<li>Authorization check passes (user_alice has edit permission)</li>\n<li>Text chunked into embeddings</li>\n<li>Vectors stored in Pinecone</li>\n<li>Success response returned</li>\n</ul>\n<h3 id=\"user-content-test-2-query-the-data\">Test 2: Query the Data</h3>\n<ol>\n<li>Find the <code>POST /harvest_logbook/query</code> endpoint</li>\n<li>Add the authorization header:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"x-user-id\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"user_alice\"</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"3\">\n<li>Set the request body:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"farmId\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"farm_1\"</span><span class=\"hljs-punctuation\">,</span>\n     <span class=\"hljs-attr\">\"query\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"What crops did we harvest recently?\"</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"4\">\n<li>Click <strong>Send</strong></li>\n</ol>\n<p>Watch the RAG pipeline execute:</p>\n<ul>\n<li>Query embedded via OpenAI</li>\n<li>Similar vectors retrieved from Pinecone</li>\n<li>AI generates response with context</li>\n<li>Query and response logged to CSV</li>\n</ul>\n<h3 id=\"user-content-test-3-verify-authorization\">Test 3: Verify Authorization</h3>\n<p>Try querying as a user without permission:</p>\n<ol>\n<li>Use the same query endpoint</li>\n<li>Change the header:</li>\n</ol>\n<pre><code class=\"hljs language-json\">   <span class=\"hljs-punctuation\">{</span>\n     <span class=\"hljs-attr\">\"x-user-id\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"user_unauthorized\"</span>\n   <span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<ol start=\"3\">\n<li>Click <strong>Send</strong></li>\n</ol>\n<p>You'll see a 403 Forbidden response to verify if authorization works correctly.\nYou can also create different users with different levels of access and see fine-grained authorization in action.</p>\n<h3 id=\"user-content-view-the-logs\">View the Logs</h3>\n<p>Check the audit trail:</p>\n<pre><code class=\"hljs language-bash\"><span class=\"hljs-built_in\">cat</span> logs/harvest_logbook.csv\n</code></pre>\n<p>You'll see all queries and responses logged with timestamps.</p>\n<p>The Workbench also provides trace visualization showing exactly how data flows through each step, making debugging straightforward.</p>\n<h2 id=\"user-content-conclusion\">Conclusion</h2>\n<p>You've built a complete RAG system with multi-tenant authorization using Motia's event-driven framework. You learned how to:</p>\n<ol>\n<li>Build event-driven workflows with Motia steps</li>\n<li>Implement RAG with text chunking, embeddings, and vector search</li>\n<li>Add fine-grained authorization using SpiceDB's relationship model</li>\n<li>Handle async operations with event emission</li>\n<li>Integrate multiple services (OpenAI, Pinecone, SpiceDB)</li>\n</ol>\n<p>Your system now handles:</p>\n<ul>\n<li>Semantic search over harvest data with AI-powered embeddings</li>\n<li>Natural language querying with contextually relevant answers</li>\n<li>Multi-tenant access control with role-based permissions</li>\n<li>Event-driven processing for fast API responses</li>\n<li>Audit logging for compliance and debugging</li>\n<li>Flexible LLM options (OpenAI or HuggingFace)</li>\n</ul>\n<p>Your RAG system is ready to help farmers query their harvest data naturally while keeping data secure with proper authorization.</p>\n<h2 id=\"user-content-final-thoughts\">Final Thoughts</h2>\n<p>This was a fun exercise in tackling a complex authorization problem and also building something production-grade. I also got to play out some of my Stardew Valley fancies IRL. Maybe it's time I actually move to a cozy farm and grow my own crops (so long as the farm has a good Internet connection!)</p>\n<p><img src=\"/images/upload/stardew.png\" alt=\"stardew\"></p>\n<p>The repository can be <a href=\"https://github.com/MotiaDev/motia-examples/tree/main/examples/harvest-logbook-rag/src/services/harvest-logbook\">found on the Motia GitHub</a>.</p>\n<p>Feel free to reach out to us on LinkedIn or jump into the <a href=\"https://authzed.com/discord\">SpiceDB Discord</a> if you have any questions. Happy farming!</p>",
            "url": "https://authzed.com/blog/building-a-multi-tenant-rag-with-fine-grain-authorization-using-motia-and-spicedb",
            "title": "Build a Multi-Tenant RAG with Fine-Grain Authorization using Motia and SpiceDB",
            "summary": "Learn how to build a complete retrieval-augmented generation pipeline with multi-tenant authorization using Motia's event-driven framework, OpenAI embeddings, Pinecone vector search, SpiceDB permissions, and natural language querying.",
            "image": "https://authzed.com/images/blogs/motia-spicedb.png",
            "date_modified": "2025-11-18T22:56:00.000Z",
            "date_published": "2025-11-18T17:30:00.000Z",
            "author": {
                "name": "Sohan Maheshwar",
                "url": "https://www.linkedin.com/in/sohanmaheshwar/"
            }
        },
        {
            "id": "https://authzed.com/blog/terraform-and-opentofu-provider-for-authzed-dedicated",
            "content_html": "<p>Today, AuthZed is excited to introduce the Terraform and OpenTofu Provider for AuthZed Dedicated, giving customers a powerful way to manage their authorization infrastructure using industry standard best practices.</p>\n<p>With this new provider, teams can define, version, and automate their resources in the AuthZed Cloud Platform - entirely through declarative infrastructure-as-code. This makes it easier than ever to integrate authorization management into existing operational workflows.</p>\n<h2 id=\"user-content-why-it-matters\">Why It Matters</h2>\n<p>Modern infrastructure teams rely on Terraform and OpenTofu to manage everything from compute resources to networking and identity. With the new AuthZed provider, you can now manage your authorization layer in the same way — improving consistency, reducing manual configuration, and enabling repeatable deployments across environments.</p>\n<h2 id=\"user-content-what-you-can-manage\">What You Can Manage</h2>\n<p>The Terraform and OpenTofu provider automates key components of your AuthZed Dedicated environment, including:</p>\n<ul>\n<li>Service Accounts - Create and manage programmatic access to your permission systems</li>\n<li>API Tokens - Securely provision and rotate tokens for authentication</li>\n<li>Roles and Policies - Define and apply fine-grained access control</li>\n<li>Permissions System Configuration - Maintain visibility and control over your authorization models</li>\n</ul>\n<p>And we’re working to support additional resources in AuthZed Dedicated environments, including managing Permissions Systems.</p>\n<h3 id=\"user-content-example-usage\">Example Usage</h3>\n<p>Below is a simple example of how to create a service account using the AuthZed Terraform provider:</p>\n<pre><code class=\"hljs language-hcl\">provider \"authzed\" {\n  token = var.authzed_token\n}\n\nresource \"authzed_service_account\" \"example\" {\n  name        = \"ci-cd-access\"\n  description = \"Service account for CI/CD pipeline\"\n}\n</code></pre>\n<p>This snippet demonstrates how straightforward it is to manage AuthZed resources alongside your existing infrastructure definitions.</p>\n<h2 id=\"user-content-seamless-integration\">Seamless Integration</h2>\n<p>The introduction of the Terraform and OpenTofu provider makes it effortless to manage authorization infrastructure as code — ensuring your permission systems evolve safely and consistently as your organization scales.</p>\n<p>For AuthZed customers interested in using the Terraform and OpenTofu provider, please contact your account manager for access.</p>\n<p>To explore the provider and get started, visit the <a href=\"https://github.com/authzed/terraform-provider-authzed\">AuthZed Terraform Provider on GitHub</a>.</p>\n<p>Not an AuthZed customer, but want to take the technology for a spin? Sign up for <a href=\"https://authzed.com/cloud/signup\">AuthZed Cloud</a> today to try it out.</p>",
            "url": "https://authzed.com/blog/terraform-and-opentofu-provider-for-authzed-dedicated",
            "title": "Terraform and OpenTofu Provider for AuthZed Dedicated",
            "summary": "AuthZed now supports Terraform and OpenTofu. You can manage service accounts, API tokens, roles, and permission system configuration as code, just like your other infrastructure. Define resources declaratively, version them in git, and automate deployments across environments without manual configuration steps.",
            "image": "https://authzed.com/images/blogs/opentofu-terraform-blog-image.png",
            "date_modified": "2025-10-30T10:40:00.000Z",
            "date_published": "2025-10-30T10:40:00.000Z",
            "author": {
                "name": "Veronica Lopez",
                "url": "https://www.linkedin.com/in/veronica-lopez-8ba1b1256/"
            }
        },
        {
            "id": "https://authzed.com/blog/why-were-not-renaming-the-company-authzed-ai",
            "content_html": "<p>It has become popular for companies to align themselves with AI. For good reason! AI has the potential, and ever increasing likelihood, of fundamentally transforming the way that companies work. The hype is out of control! People breathlessly compare AI to the internet and the industrial revolution. And who knows; they could even be right!</p>\n<p>At AuthZed, a rapidly growing segment of our customers are AI first companies, including OpenAI. As we work with more AI companies on authorization for AI systems, we often get asked if we will rebrand as an AI company.</p>\n<p>Companies have realigned themselves to varying degrees. SalesForce may one day soon be called AgentForce. As an April Fool’s joke, <a href=\"https://www.tweaktown.com/news/104354/nvidia-is-rebranding-to-nvid-ai-embracing-the-companys-leadership/index.html\">one company started a rumor</a> that Nvidia was going to rebrand as Nvid<strong>AI</strong>, and I think a lot of people probably thought to themselves: “yeah, that tracks.” Mega corps such as Google, Meta, and IBM have .ai top level websites that outline their activities in the AI space.</p>\n<p>It can make a lot of sense! After all, unprecedented shifts require unprecedented attention, and a rising tide floats all boats. Well: <strong>we’re not</strong>. In this post I will lay out some of the pros and cons of going all in on AI branding and alignment, and explain our reasons for keeping our brand in place.</p>\n<h2 id=\"user-content-a-rational-choice\">A Rational Choice</h2>\n<p>When considering such a drastic change, I believe each company is looking at the upsides and downsides of a rebrand given their specific situation (revenue, brand value, momentum, staff, etc.) and making a calculated choice that may only apply in their specific context. So what are some of the upsides and downsides?</p>\n<p><img src=\"/images/upload/post-to-ai-or-not.png\" alt=\"\" title=\"To AI or not to AI (courtesy of Google Gemini)\"></p>\n<h3 id=\"user-content-risks\">Risks</h3>\n<p>The risks that I’ve been able to identify boil down to two areas: brand value and perception. Let’s start with brand value.</p>\n<p>Companies spend a lot of time and effort building their brand value. It is an intangible asset for companies that pays dividends in areas such as awareness, customer acquisition costs, and reach, just to name a few. Apple is widely considered to have the most valuable brand in the world, and <a href=\"https://brandfinance.com/press-releases/apple-is-the-2025-most-valuable-brand-in-the-world-nvidia-breaks-into-top-ten\">BrandFinance currently values their brand at $575 <em><strong>billion</strong></em></a>, with a b. That’s approximately 15% of their $3.7 trillion market cap.</p>\n<p>When you rebrand by changing your company’s name, you can put all of that hard work at risk. By changing your name, you need to regain any lost brand mindshare. When you change your web address, you need to re-establish SEO and domain authority that was hard fought and hard won. If Apple rebranded to <a href=\"http://treefruit.ai\">treefruit.ai</a> (dibs btw) tomorrow, we would expect their sales, mindshare, and even email deliverability to go down.</p>\n<p>The second major risk category is around perception. By rebranding around AI you’re signaling a few things to the market. First, you're weighing the upside of being aligned with AI heavily. Second, you signal that you’re willing and able to follow the hype. These factors combined may change the perception of your company to potential buyers: from established, steady, successful, to trendy, fast-moving, up and coming.</p>\n<p>On a longer time horizon, we’ve also seen many such trends come and go. Web 1.0, Web 2.0, SoLoMo, Cloud, Crypto, VR/AR, and now AI. In all cases these hype movements have had a massive effect on the way people perceive technology, but they have also become less hyped over time, as a new trend has arrived to supplant them. With AI, I can guarantee that at some point we will achieve an equilibrium where the value prop has been mostly established, and the hype adjusts to fit. Do you want to be saddled with an AI-forward brand when that happens? Will you have been able to ride the wave long and high enough to establish an enduring company that can survive on its own? One of my favorite quotes from Warren Buffet may apply here: “Only when the tide goes out do you discover who's been swimming naked.”</p>\n<h3 id=\"user-content-rewards\">Rewards</h3>\n<p>There are many upsides that companies can expect to reap as well! Hype is its own form of reality distortion field, and it causes a lot of people to act in ways that they might not have otherwise. FOMO, or fear of missing out, is a well established phenomenon that we can leverage to our benefit. Let’s take a look at who is acting differently in this hype cycle.</p>\n<p><strong>Investors</strong>. If you are a startup that’s hoping to raise capital, you had better have either: insane fundamentals or an AI story. Carta recently <a href=\"https://carta.com/data/ai-fundraising-trends-2024/\">released an analysis on how AI is affecting fundraising</a>, with the TL;DR being that AI companies are absorbing a ton of the money, and that growing round sizes can primarily be attributed to the AI companies that are raising. Counter to all of the hype, user Xodarap over at <a href=\"http://LessWrong.com\">LessWrong.com</a> has produced an analysis on YC companies post GenAI hitting the scene, that <a href=\"https://www.lesswrong.com/posts/hxYiwSqmvxzCXuqty/generative-ai-is-not-causing-ycombinator-companies-to-grow\">paints a less rosy picture</a> of the outcomes associated with primarily AI-based companies so far. It’s possible (probable?) that we are just too early in the cycle to have identified the clear winners and losers for AI.</p>\n<p><strong>Vendors</strong>. If partnerships are a big part of your model, there are a lot of dollars floating around for partnerships that revolve around AI. I've had a marketing exec from a vendor tell me straight up: “all of our marketing dollars are earmarked <strong>only</strong> for AI related initiatives right now.” If you can tell a compelling story here, you will be able to find someone willing to help you amplify it.</p>\n<p><strong>Businesses</strong>. Last, and certainly not least, businesses are also changing their behavior. If you’re a B2B company, your customers are all figuring out what <em>their</em> AI story is too. That means opportunity. They’re looking for vendors, partners, analysts, really anyone who can help them be successful with AI. Their boss told them: “We need an AI story or we’re going to get our lunch eaten! Make it happen!” So they’re out there trying to make it happen. Unfortunately, <a href=\"https://fortune.com/2025/08/18/mit-report-95-percent-generative-ai-pilots-at-companies-failing-cfo/\">a study out of MIT</a> recently proclaimed that “95% of generative AI pilots at companies are failing.”</p>\n<h3 id=\"user-content-mitigating-factors\">Mitigating Factors</h3>\n<p>The world is never quite as cut and dry as we think it might be. The good news is, that you can still reap some of the reward without a full rebrand. At AuthZed, we’ve found that you can still tell your AI story, and court customers who are looking to advance their AI initiatives even if you’re not completely AI-native, or all-aboard the hype train. Unfortunately, I don’t have intuition or data for what the comparative advantage is of a rebrand compared to attempting to make waves under a more neutral brand.</p>\n<h2 id=\"user-content-our-calculus\">Our Calculus</h2>\n<p>At AuthZed, our context-specific decision not to rebrand was based primarily on how neutral our solution is. While many companies, both AI and traditional, are having success with using AuthZed to secure RAG pipelines and AI agents, we also serve many customers who want to protect their data from unauthorized access by humans. Or to build that new sharing workflow that is going to unlock new revenue. Or break into the enterprise. Put succinctly: we think we would be doing the world a great disservice if our technology was only being used for AI-adjacent purposes.</p>\n<p>The other, less important reason why we’re not rebranding, is that at AuthZed we often take a slightly contrarian or longer view than whatever the current hype cycle might dictate. We try not to <a href=\"https://en.wikipedia.org/wiki/Cargo_cult\">cargo-cult</a> our business decisions. Following the pack is almost by definition a median-caliber decision. Median-caliber decisions are likely to sum up to a median company outcome. The median startup outcome is death or an unprofitable exit. At AuthZed, we think that the opportunity that we have to reshape the way that the world thinks about authorization shouldn’t be wasted.</p>\n<p>With that said, I’ve been wrong many times in the past. Too many to count even. “Never say never” are words to live by! Hopefully if and when the time comes where our personal calculus shifts in favor of a big rebrand, I can recognize the changing landscape and we can do what’s right for the company. What’s a little egg on your face when you’re on a mission to fix the way that companies across the world do authorization.</p>",
            "url": "https://authzed.com/blog/why-were-not-renaming-the-company-authzed-ai",
            "title": "Why we’re not renaming the company AuthZed.ai",
            "summary": "Should your company rebrand as an AI company? We decided not to.\nAI companies attract outsized funding and partnership dollars. Yet rebranding means trading established brand value and customer mindshare for alignment with today's hottest trend.\nWe stayed brand-neutral because our authorization solution serves both AI and non-AI companies alike. Limiting ourselves to AI-only would be a disservice to our broader mission and the diverse customers who depend on us.",
            "image": "https://authzed.com/images/blogs/authzed-ai-bg.png",
            "date_modified": "2025-10-27T11:45:00.000Z",
            "date_published": "2025-10-27T11:45:00.000Z",
            "author": {
                "name": "Jake Moshenko",
                "url": "https://www.linkedin.com/in/jacob-moshenko-381161b/"
            }
        },
        {
            "id": "https://authzed.com/blog/authzed-adds-microsoft-azure-support",
            "content_html": "<p>Today, AuthZed is announcing support for Microsoft Azure in AuthZed Dedicated to provide more authorization infrastructure deployment options for customers.\nAuthZed now provides customers the opportunity to choose from all major cloud providers - AWS, Google Cloud and/or Microsoft Azure.</p>\n<p><img src=\"/images/blogs/Blog-AuthZed-Azure-Support@2x.png\" alt=\"Authzed connection to Azure\"></p>\n<p>AuthZed customers can now deploy authorization infrastructure to 23+ Azure regions to support their globally distributed applications.\nThis ensures fast, consistent permission decisions regardless of where your users are located.</p>\n<blockquote>\n<p>\"I have been following the development of SpiceDB and AuthZed on how they are providing authorization infrastructure to companies of all sizes,\" said Lachlan Evenson, Principal PDM Manager, Azure Cloud Native Ecosystem.\n\"It's great to see their support for Microsoft Azure and we look forward to collaborating with AuthZed as they work with more Azure customers moving forward.\"</p>\n</blockquote>\n<p>This launch is the direct result of customer demand. Many teams asked for Azure support, and now they have the ability to deploy authorization infrastructure in the cloud of their choice.</p>\n<p><img src=\"/images/blogs/AuthZed-Azure-Dropdown@2x.png\" alt=\"AuthZed Azure Support Dropdown\"></p>\n<h2 id=\"user-content-what-is-authzed-dedicated\">What is AuthZed Dedicated?</h2>\n<p><a href=\"https://authzed.com/products/authzed-dedicated\">AuthZed Dedicated</a> is our managed service that provides fully private deployments of our cloud platform in our customer’s provider and regions of choice.\nThis gives users the benefits of a proven, production-ready authorization system—without the burden of building and maintaining it themselves.</p>\n<p>Industry leaders such as <a href=\"https://authzed.com/customers/openai\">OpenAI</a>, <a href=\"https://authzed.com/customers/workday\">Workday</a>, and <a href=\"https://authzed.com/customers/turo\">Turo</a> rely on AuthZed Dedicated for their authorization infrastructure:</p>\n<blockquote>\n<p>“We decided to buy instead of build early on.\nThis is an authorization system with established patterns.\nWe didn’t want to reinvent the wheel when we could move fast with a proven solution.”\n— Member of Technical Staff, OpenAI</p>\n</blockquote>\n<h2 id=\"user-content-get-started\">Get Started</h2>\n<p>With Azure now available, you can deploy AuthZed Dedicated on the cloud of your choice.\n<a href=\"https://authzed.com/call\">Book a call</a> with our team to learn how AuthZed can power your authorization infrastructure.</p>",
            "url": "https://authzed.com/blog/authzed-adds-microsoft-azure-support",
            "title": "AuthZed Dedicated Now Available on Microsoft Azure",
            "summary": "AuthZed now supports Microsoft Azure, giving customers the opportunity to choose from all major cloud providers - AWS, Google Cloud, and Microsoft Azure. Deploy authorization infrastructure to 23+ Azure regions for globally distributed applications.\n",
            "image": "https://authzed.com/images/blogs/authzed-azure-support-og.png",
            "date_modified": "2025-10-21T16:00:00.000Z",
            "date_published": "2025-10-21T16:00:00.000Z",
            "author": {
                "name": "Jimmy Zelinskie",
                "url": "https://twitter.com/jimmyzelinskie"
            }
        },
        {
            "id": "https://authzed.com/blog/extended-t-augment-your-design-craft-with-ai-tools",
            "content_html": "<blockquote>\n<p><strong>TL;DR</strong><br>\nAI doesn't replace design judgment. It widens my T-shaped skill set by surfacing on-brand options quickly. It's still on me to uphold craft, taste, and standards for what ships.</p>\n</blockquote>\n<p>Designers on small teams, especially at startups, default to being T-shaped: deep in a core craft and broad enough to support adjacent disciplines. My vertical is brand and visual identity, while my horizontal spans marketing, product, illustration, creative strategy, and execution. Lately, AI tools have pushed that horizontal reach further than the usual constraints allow.</p>\n<p>At AuthZed, I use AI to explore ideas that would normally be blocked by time or budget: 3D modeling, character variation, and light manufacturing for physical pieces. The point is not to replace design craft with machine output. It is to expand the number of viable ideas I can evaluate, then curate and polish a final product that meets our design standard.</p>\n<h2 id=\"user-content-exploration-vs-curation-what-actually-changed\">Exploration vs. curation: what actually changed</h2>\n<p>Previous tools mostly sped up execution. AI speeds up exploration. When you can generate twenty plausible directions in minutes, the scarce skill is not pushing Bézier handles. It is knowing which direction communicates the right message, and why.</p>\n<p>Concrete example: Photoshop made retouching faster, but great photography still depends on eye and intent. Figma made collaboration faster, but good product design still depends on hierarchy, flows, and clarity. AI widens the search field so designers can spend more time on curation instead of setup.</p>\n<blockquote>\n<p><strong>Volume before polish</strong><br>\nWhile at SVA we focused on volume before refinement. We would thumbnail dozens (sometimes a hundred) poster concepts before committing to one. That practice shaped how I use AI today: explore wide, then curate down to find the right solution. Richard Wilde's program emphasized iterative problem-solving and visual literacy long before today's tools made rapid exploration this easy.</p>\n</blockquote>\n<h2 id=\"user-content-expanding-horizontally-with-ai-without-losing-the-vertical\">Expanding horizontally with AI without losing the vertical</h2>\n<p>AI works best when it is constrained by the systems you already trust, whether that is the permission model that controls who can view a file or the rules you enforce when writing code. Clarity is what turns an AI model from a toy into a multiplier. When we developed our mascot, Dibs, I knew we would eventually need dozens of consistent, reference-accurate variations: expressions, poses, environments. Historically, that meant a lot of sketching and cleanup before we could show anything.</p>\n<p>With specific instructions and a set of reference illustrations, I can review a new variation every few moments. None of those are final, but they land close while surfacing design choices I might not have explored on my own. I still adjust typography, tweak poses, and rebalance compositions before anything ships, so we stay on brand and accessible.</p>\n<p>This mirrors every major tool shift. Photoshop did not replace photographers. Figma did not replace designers. AI does not replace design thinking. It gives you a broader search field so you can make better choices earlier.</p>\n<p><img src=\"/images/blogs/shipping-more-design-work/Golden-Dibs-Process-1@2x.png\" alt=\"Dibs mascot exploration and refinement process\" title=\"Dibs mascot exploration grid showing iterations from rough to refined; variations test pose, expression, and silhouette consistency.\"></p>\n<h2 id=\"user-content-case-study-turning-2d-into-3d-trophies\">Case study: turning 2D into 3D trophies</h2>\n<p>For our offsite hackathon, I wanted trophies the team would be proud to earn and motivated to chase next time. Our mascot, Dibs, was the obvious hero. I started with approved 2D art and generated a character turn that covered front, side, back, and top views. From there I used a reconstruction tool (Meshy has been the most reliable lately) to get a starter mesh before moving into Blender for cleanup, posing, and print prep.</p>\n<p><img src=\"/images/blogs/shipping-more-design-work/Golden-Dibs-Process-2@2x.png\" alt=\"Mesh cleanup and sculpting\" title=\"Auto-generated base mesh next to the 2D turn; early sculpt pass to fix proportions and topology.\"></p>\n<p>I am not a Blender expert, but I have made a donut or two. With the starting mesh it was straightforward to get a printable file: repair holes, smooth odd vertices, and thicken delicate areas. When I hit something rusty, I leaned on documentation and the right prompts to fill the gaps. Before doing any of that refinement, I printed the raw export on my Bambu Lab P1P in PLA, cleaned up the supports, and dropped the proof on a teammate's desk. We went from concept to a physical artifact in under a day.</p>\n<p>We ended up producing twelve trophies printed in PETG with a removable base that hides a pocket for added weight (or whatever ends up in there). I finished them by hand with Rub 'n Buff, a prop-maker staple, to get a patinated metallic look. Once the pipeline was dialed in, I scaled it down for a sleeping Dibs keychain so everyone could bring something home, even if they were not on the podium. Small lift, real morale boost.</p>\n<p><img src=\"/images/blogs/shipping-more-design-work/Golden-Dibs-Process-3@2x.png\" alt=\"Prints and final Golden Dibs trophies\" title=\"Cleaned mesh in Blender with pose tweaks for printability; supported ears, thicker tail tuft, reinforced ankles.\"></p>\n<p><img src=\"/images/blogs/shipping-more-design-work/Golden-Dibs-Process-4@2x.png\" alt=\"Dibs keychains, Blender pose, in-progress and final prints\" title=\"Finished metallic trophies and mini keychains with subtle patina and weighted bases.\"></p>\n<h2 id=\"user-content-why-this-matters-for-t-shaped-designers\">Why this matters for T-shaped designers</h2>\n<p>When anyone can produce a hundred logos or pose variations, the value as a designer shifts to selection with intent. Brand expertise tells you which pose reads playful versus chaotic, which silhouette will hold up at small sizes, and which material choice survives handling at an event. The models handle brute-force trial. You own the taste, the narrative, and the necessary constraints.</p>\n<p>The result is horizontal expansion without vertical compromise. Consistency improves because character work starts from reference-accurate sources instead of ad-hoc one-offs. Physical production becomes realistic because you can iterate virtually before committing to materials and time.</p>\n<p>With newer models, I can get much closer to production-ready assets with far less back-and-forth prompting. I render initial concepts, select top options based on color, layout, expression, and composition, then create a small mood board for stakeholders to review before building the final production-ready version. The goal is not to outsource taste. It is to see more viable paths sooner, pick one, and refine by hand so the final assets stay original and on-brand.</p>\n<h2 id=\"user-content-the-guardrails-that-help-keep-quality-high\">The guardrails that help keep quality high</h2>\n<ul>\n<li>Define what success looks like before you generate anything, and decide what \"done\" means.</li>\n<li>Capture the non-negotiables: character traits, palette, typography, voice.</li>\n<li>Provide references instead of adjectives.</li>\n<li>Call out the exact angles, poses, or compositions you need.</li>\n<li>Keep a human in the loop for selection, edits, and distribution.</li>\n<li>Stay ethical: use your own IP, avoid mimicking living artists, and be transparent about where AI fits.</li>\n</ul>\n<h2 id=\"user-content-mini-checklist-stretch-your-own-t\">Mini checklist: stretch your own T</h2>\n<ul>\n<li>Pick one adjacent skill that will unlock an upcoming launch.</li>\n<li>Codify the source material: references, palettes, schema, constraints.</li>\n<li>Pair one AI assist with that project and track what you keep, edit, or reject.</li>\n<li>Close with critique: share the work, gather feedback, and refine the pipeline for next time.</li>\n</ul>\n<blockquote>\n<p><strong>Process note:</strong> I drafted the outline and core ideas, then used an editor to tighten phrasing and proofread. Same pattern as the rest of my work: widen the search, keep the taste.</p>\n</blockquote>\n<hr>\n<h2 id=\"user-content-faqs\">FAQs</h2>\n<p><strong>What is a T-shaped designer?</strong><br>\nA designer with deep expertise in one area (the vertical) and working knowledge across adjacent disciplines (the horizontal).</p>\n<p><strong>How does AI help T-shaped designers?</strong><br>\nAI quickly generates plausible options so you can evaluate more directions, then apply judgment to pick, refine, and ship the best one.</p>\n<p><strong>How do I keep brand consistency with AI images?</strong><br>\nDefine non-negotiables (proportions, palette, silhouette), use reference images, and keep a human finish pass for polish.</p>\n<p><strong>Which tools did you use in this workflow?</strong><br>\nModel-guided image generation (e.g., Midjourney or a tuned model with references), a 2D-to-3D reconstruction step for a starter mesh (Rodin/Hyper3D or Meshy), Blender for cleanup, and a Bambu Lab P1P to slice G-code and print.</p>\n<hr>\n<script>\n{\n  \"@context\": \"https://schema.org\",\n  \"@graph\": [\n    {\n      \"@type\": \"Article\",\n      \"@id\": \"https://authzed.com/blog/extended-t-augment-your-design-craft-with-ai-tools#article\",\n      \"mainEntityOfPage\": \"https://authzed.com/blog/extended-t-augment-your-design-craft-with-ai-tools\",\n      \"headline\": \"Extended T: Augment your design craft with AI tools\",\n      \"description\": \"A designer's playbook for using AI to widen skills, stay on-brand, and ship without lowering quality.\",\n      \"image\": \"https://authzed.com/images/blogs/shipping-more-design-work/Blog-Design-Dibs-Trophy-OG@2x.png\",\n      \"datePublished\": \"2025-10-03T16:00:00.000Z\",\n      \"author\": \"Corey Thomas\",\n      \"publisher\": {\n        \"@type\": \"Organization\",\n        \"name\": \"AuthZed\",\n        \"logo\": {\n          \"@type\": \"ImageObject\",\n          \"url\": \"https://authzed.com/images/social/authzed-og-default.jpg\"\n        }\n      }\n    },\n    {\n      \"@type\": \"FAQPage\",\n      \"@id\": \"https://authzed.com/blog/extended-t-augment-your-design-craft-with-ai-tools#faq\",\n      \"mainEntity\": [\n        {\n          \"@type\": \"Question\",\n          \"name\": \"What is a T-shaped designer?\",\n          \"acceptedAnswer\": {\n            \"@type\": \"Answer\",\n            \"text\": \"A designer with deep expertise in one area and working knowledge across adjacent disciplines.\"\n          }\n        },\n        {\n          \"@type\": \"Question\",\n          \"name\": \"How does AI help T-shaped designers?\",\n          \"acceptedAnswer\": {\n            \"@type\": \"Answer\",\n            \"text\": \"AI accelerates exploration by generating plausible options so designers can evaluate more directions and then refine the best choice.\"\n          }\n        },\n        {\n          \"@type\": \"Question\",\n          \"name\": \"How do I keep brand consistency with AI images?\",\n          \"acceptedAnswer\": {\n            \"@type\": \"Answer\",\n            \"text\": \"Define non-negotiable references and standards, constrain outputs, and keep a human finish pass.\"\n          }\n        },\n        {\n          \"@type\": \"Question\",\n          \"name\": \"Which tools did you use in this workflow?\",\n          \"acceptedAnswer\": {\n            \"@type\": \"Answer\",\n            \"text\": \"Model-guided image generation, a 2D-to-3D reconstruction step for a starter mesh, Blender for cleanup, and a consumer FDM printer for output and finishing.\"\n          }\n        }\n      ]\n    }\n  ]\n}\n</script>",
            "url": "https://authzed.com/blog/extended-t-augment-your-design-craft-with-ai-tools",
            "title": "Extended T: Augment your design craft with AI tools",
            "summary": "How a startup designer makes the T wide, expanding into 3D, rapid iteration, and small-batch production without lowering the quality bar.",
            "image": "https://authzed.com/images/blogs/shipping-more-design-work/Blog-Design-Dibs-Trophy@2x.png",
            "date_modified": "2025-10-03T16:00:00.000Z",
            "date_published": "2025-10-03T16:00:00.000Z",
            "author": {
                "name": "Corey Thomas",
                "url": "https://www.linkedin.com/in/cor3ythomas/"
            }
        },
        {
            "id": "https://authzed.com/blog/introducing-authzeds-mcp-servers",
            "content_html": "<p>We're excited to announce the launch of two new MCP servers that bring SpiceDB resources closer to your AI workflow, making it easier to learn and get started using SpiceDB for your application permissions: the <strong><a href=\"https://authzed.com/docs/mcp/authzed/authzed-mcp-server\">AuthZed MCP Server</a></strong> and the <strong><a href=\"https://authzed.com/docs/mcp/authzed/spicedb-dev-mcp-server\">SpiceDB Dev MCP Server</a></strong>.</p>\n<h2 id=\"user-content-two-servers-complementary-use-cases\">Two Servers, Complementary Use Cases</h2>\n<p>The <strong>AuthZed MCP Server</strong> brings comprehensive documentation and learning resources directly into your AI tools. Whether you're exploring SpiceDB concepts, looking up API references, or searching for schema examples, this server provides instant access to all SpiceDB and AuthZed documentation pages, complete API method definitions, and a curated collection of authorization pattern examples. It's designed to make learning and referencing SpiceDB documentation seamless, right where you're already working.</p>\n<p>The <strong>SpiceDB Dev MCP Server</strong> takes things further by integrating directly into your development workflow. It connects to a sandboxed SpiceDB instance, allowing your AI coding assistant to help you learn and experiment with schema development, relationship testing, and permission checking. Need to validate a schema change? Want to test whether a specific permission check will work? Your AI assistant can now interact with SpiceDB on your behalf, making development faster and more intuitive.</p>\n<p>Ready to try them out? Head over to <a href=\"http://authzed.com/docs/mcp\">authzed.com/docs/mcp</a> to get started with both servers.</p>\n<p><img src=\"/images/upload/chat-with-authzed-mcp.png\" alt=\"\"></p>\n<h2 id=\"user-content-our-mcp-journey-from-prototypes-to-production\">Our MCP Journey: From Prototypes to Production</h2>\n<p>We've been experimenting with MCP since the first specification was published. Back when the term \"vibe coding\" was just starting to circulate, we built an <a href=\"https://github.com/samkim/spicedb-mcp-server\">early prototype MCP server for SpiceDB</a>. The results were eye-opening. We were pleasantly surprised by how effectively LLMs could use the tools we provided, and delighted by the potential of being able to \"talk\" to SpiceDB through natural language.</p>\n<p>That initial prototype sparked conversations across the SpiceDB community. We connected with others who were equally excited about the possibilities, <a href=\"https://www.youtube.com/watch?v=DG0J_zcNy6M\">sharing ideas</a> and <a href=\"https://youtu.be/TaCB4oGBtbk?feature=shared&#x26;t=1053\">exploring use cases</a> together. Those early discussions helped shape our thinking about what MCP servers for SpiceDB could become.</p>\n<p>As the MCP specification continued evolving (particularly around enterprise readiness and authorization), we wanted to deeply understand these new capabilities. This led us to build a <a href=\"https://github.com/authzed/mcp-server-reference\">reference implementation of a remote MCP server</a> using open source solutions. That reference implementation became a testbed for understanding the authorization aspects of the spec and exploring best practices for building production-ready MCP servers.</p>\n<h2 id=\"user-content-why-we-built-these-servers\">Why We Built These Servers</h2>\n<p>Through our <a href=\"https://authzed.com/blog/coding-with-ai-my-personal-experience\">own experience with AI coding tools</a>, we've seen firsthand how valuable it is to have the right resources and tools available directly in your AI workflow. Our team's usage of AI assistants has steadily increased, and we know the difference it makes when information and capabilities are just a prompt away.</p>\n<p>For AuthZed and SpiceDB users, we wanted to bring learning and development resources closer to where you're already working. Whether you're learning SpiceDB concepts, building a new schema, or debugging permissions logic, having immediate access to documentation, examples, and a sandbox SpiceDB instance can dramatically speed up the development process.</p>\n<p>That's why we built both servers: the AuthZed MCP Server puts knowledge at your fingertips, while the SpiceDB Dev MCP Server puts your development environment directly into your AI assistant's toolkit.</p>\n<h2 id=\"user-content-building-responsibly-authorization-in-mcp\">Building Responsibly: Authorization in MCP</h2>\n<p>We're still actively building and experimenting with MCP. While the specification provides guidance for authorization, there's significant responsibility on MCP server developers to implement appropriate access controls for resources and accurate permissions around tools.</p>\n<p>This is particularly important as MCP servers become more powerful and gain access to sensitive systems. We're learning as we build, and we'll be sharing new tools and lessons around building authorization into MCP servers as we discover them. We believe the combination of SpiceDB for MCP permissions and AuthZed for authorization infrastructure is especially well-suited for defining and enforcing the complex permissions that enterprise MCP servers require.</p>\n<p>In the meantime, we encourage you to try out our MCP servers. The documentation for each includes detailed use cases and security guidelines to help you use them safely and effectively.</p>\n<p>If you're building an enterprise MCP server and would like help integrating permissions and authorization, we'd love to chat. <a href=\"https://authzed.com/call\">Book a call</a> with our team and let's explore how we can help.</p>\n<hr>\n<p>Happy coding, and we can't wait to see what you build with these new tools! 🚀</p>",
            "url": "https://authzed.com/blog/introducing-authzeds-mcp-servers",
            "title": "Introducing AuthZed's MCP Servers",
            "summary": "We're launching two MCP servers to bring SpiceDB closer to your AI workflow. The AuthZed MCP Server provides instant access to documentation and examples, while the SpiceDB Dev MCP Server integrates with your development environment. Learn about our MCP journey from early prototypes to production, and discover how these tools can speed up your SpiceDB development.",
            "image": "https://authzed.com/images/upload/chat-with-authzed-mcp.png",
            "date_modified": "2025-09-30T10:45:00.000Z",
            "date_published": "2025-09-30T10:45:00.000Z",
            "author": {
                "name": "Sam Kim",
                "url": "https://github.com/samkim"
            }
        },
        {
            "id": "https://authzed.com/blog/the-dual-write-problem-in-spicedb-a-deep-dive-from-google-and-canva-experience",
            "content_html": "<p><em>This talk was part of the Authorization Infrastructure event hosted by AuthZed on August 20, 2025.</em></p>\n<h2 id=\"user-content-how-we-are-solving-the-dual-write-problem-at-canva\">How We Are Solving the Dual Write Problem at Canva</h2>\n<p>In this technical deep-dive, Canva software engineer Artie Shevchenko draws on five years of experience with centralized authorization systems—first with Google's Zanzibar and now with SpiceDB—to tackle one of the most challenging aspects of authorization system implementation: the dual-write problem.</p>\n<p>The dual-write problem emerges when data must be replicated between your main database (like Postgres or Spanner) and SpiceDB, creating potential inconsistencies due to network failures, race conditions, and system bugs. These inconsistencies can lead to false negatives (blocking legitimate access) or false positives (security vulnerabilities).</p>\n<p>However, as Shevchenko explains, \"the good news is centralized authorization systems, they actually do simplify things quite a bit.\" Unlike traditional event-driven architectures where teams publish events hoping others interpret them correctly, \"with SpiceDB, you're fully in control\" of the entire replication process.</p>\n<p>SpiceDB offers several key advantages: \"you're not replicating aggregates. Most often, it's simple booleans or relationships,\" making inconsistencies easier to reason about. Additionally, \"the volume of replication is also much smaller\" since authorization data can live primarily in SpiceDB, and you're \"replicating just to SpiceDB, not to 10 other services.\"</p>\n<p>The talk explores four solution approaches—from cron sync jobs to transactional outboxes—with real-world examples from Google and Canva. Shevchenko's key insight: \"dual write is not a SpiceDB problem. It's a data replication problem,\" but \"SpiceDB makes the dual write problem, and ultimately the data integrity problem, much more manageable.\"</p>\n<h3 id=\"user-content-on-ownership-and-control\">On Ownership and Control</h3>\n<blockquote>\n<p>\"First of all, as a team now, you own the whole replication process. Because you own both copies of the data. Which makes a huge difference. You're not just publishing an event that other teams would hopefully correctly interpret and apply to their data stores.\"</p>\n</blockquote>\n<p><strong>Takeaway:</strong> SpiceDB gives you complete control over your authorization data replication, eliminating dependencies on other teams and reducing coordination overhead.</p>\n<h3 id=\"user-content-on-proven-scale\">On Proven Scale</h3>\n<blockquote>\n<p>\"And then feed it as an input to our MapReduce style sync job, which would sync data for 100 millions of users in just a couple of hours.\"</p>\n</blockquote>\n<p><strong>Takeaway:</strong> SpiceDB's approach has been battle-tested at Google scale, handling hundreds of millions of users efficiently.</p>\n<h3 id=\"user-content-on-technical-advantages\">On Technical Advantages</h3>\n<blockquote>\n<p>\"But, the first three approaches without Zanzibar or SpiceDB would be really tricky, if not impossible. Not only because of the data ownership problem, but also because of aggregates. With event-driven replication, you're probably not replicating simple atomic facts.\"</p>\n</blockquote>\n<p><strong>Takeaway:</strong> SpiceDB's simple data model (booleans and relationships) makes dual-write problems significantly more manageable compared to traditional event-driven architectures that deal with complex aggregates.</p>\n<p><lite-youtube videoid=\"uz_gxz3whS0\" playlabel=\"Solving the Dual Write Problem\" params=\"start=1715\"></lite-youtube></p>\n<hr>\n<h2 id=\"user-content-full-transcript\">Full Transcript</h2>\n<p><em>Talk by Artie Shevchenko, Software Engineer at Canva</em></p>\n<h3 id=\"user-content-introduction\">Introduction</h3>\n<p>All right, let's talk about the dual-write problem. My name is Artie Shevchenko, and I'm a software engineer at Canva. My first experience with systems like SpiceDB was actually with Zanzibar at Google in 2017. And now I'm working on SpiceDB integration at Canva. So, yeah, almost five years working with this piece of tech.</p>\n<h3 id=\"user-content-why-spicedb-simplifies-authorization\">Why SpiceDB Simplifies Authorization</h3>\n<p>And from my experience, there are two hard things in centralized authorization systems. It's dual-writes and data backfills. But neither of them is unique to Zanzibar or SpiceDB. In fact, dual-write is a fairly standard problem. And when we're talking about replication to another database, it is always challenging. Whether it's a permanent replication of some data to another microservice, or migration to a new database with zero downtime, or even replication to SpiceDB.</p>\n<p>The good news is centralized authorization systems, they actually do simplify things quite a bit. First of all, as a team now, you own the whole replication process. Because you own both copies of the data. Which makes a huge difference. You're not just publishing an event that other teams would hopefully correctly interpret and apply to their data stores. With SpiceDB, you're fully in control.</p>\n<p>Secondly, with SpiceDB, you're not replicating aggregates. Most often, it's simple booleans or relationships. Which makes it much easier to reason about the possible inconsistencies.</p>\n<p>And finally, the volume of replication is also much smaller. For two reasons. First, most of the authorization data you can store in SpiceDB only, once the migration is done. And second, with SpiceDB, you need to replicate just to SpiceDB, not to 10 other services. Well, there are also search indexes, but they're very special for multiple reasons. And the good news is search indexes, you don't need to solve them on the client side. Mostly, you can just delegate this to tools that materialize.</p>\n<p>But that said, even with replication to SpiceDB, there is a lot of essential complexity there that first, you need to understand. And second, you need to decide which approach you're going to use to solve the dual-write problem.</p>\n<h3 id=\"user-content-talk-structure-and-definitions\">Talk Structure and Definitions</h3>\n<p>The structure of this talk, unlike the topic itself, is super simple. I don't have any ambition to make the dual-write problem look simple. It's not. But I do hope to make it clear. So, the goal of this talk is to make the problems and the underlying causes clear. And we're going to spend quite a lot of time unpacking what are the practical problems we're solving. And then, talking about the solution space, the goal is to make it clear what works and what doesn't. And, of course, the pros and cons of the different alternatives.</p>\n<p>But let's start with a couple of definitions. Almost obvious definitions aside, let's take a look at the left side of the slide, at the diagrams. Throughout the talk, we'll be looking into storing the same piece of data in two databases. Of course, ideally, you would store it in exactly one of them. But in practice, unfortunately, it's not always possible, even with SpiceDB.</p>\n<p>So, when information in one database does not match the information in another database, we'll call it a discrepancy or inconsistency. Or I'll simply say that databases are out of sync.</p>\n<p>When talking about the dual-write problem in general, I'll be using the term \"source of truth\" for the database that is kind of primary in the replication process. And the second database I'll call the second database. I was thinking about calling them primary and replica or maybe master and slave. But the problem is, these terms are typically used to describe replication within the same system. But I want to emphasize that these are different databases. And also, the same piece of knowledge may take very different forms in them. So, I'll stick to the terms \"source of truth\" and just some other second database. That's when I talk about the dual-write problem in general.</p>\n<p>But not to be too abstract, we'll be mostly looking at the dual-write problem in the context of data replication to SpiceDB, not just to some other abstract second database. And in this case, instead of using the term \"source of truth,\" I'll be using the term \"main database,\" referring to the traditional transactional database where you store most of your data, like Postgres, Dynamo, or Spanner. Because for the purposes of this talk, we'll assume that the main database is a source of truth for any replicated piece of data. Yes, theoretically, replicating in the other direction is also an option, but we won't consider that. We're replicating from the main database to SpiceDB.</p>\n<p>So, in different contexts, I'll refer to the database on the left side of this giant white replication arrow as either \"source of truth\" or \"main database\" or, even more specifically, Postgres or Spanner. Please keep this in mind.</p>\n<p>And finally, don't get confused when I call SpiceDB a database. Maybe I can blame the name. Of course, it's more than just a database. It is a centralized authorization system. But in this talk, we actually care about the underlying database only. So, hopefully, that doesn't cause any confusion.</p>\n<h3 id=\"user-content-defining-the-dual-write-problem\">Defining the Dual-Write Problem</h3>\n<p>All right. We're done with these primitive definitions. Now, let's define what the dual-write problem is. And let's start with an oversimplified but real example from home automation.</p>\n<p>Let's say there are two types of resources, homes and devices. Users can be members of multiple homes, and they have access to all the devices in their homes. So, whether a device is in one home or another, that information obviously has to be stored both in the main database, in this case, Spanner, and in SpiceDB.</p>\n<p>And if you want to move a device from one home to another, now you need to update the device's home in both databases. If you get a task to implement that, you would probably start with these two lines of code. You first write to the source of truth, which is Spanner, and then write to the second database, which is SpiceDB. The problem is you cannot write to both data stores in the same transaction, because these are literally different systems.</p>\n<p>So, a bunch of things can go wrong. If the first write fails, it's easy. You just let the error propagate to the client, and they can retry. But what about the second write? What if that one fails? Do you try to revert the first write and return an error to the client? But what if reverting the first one fails? It's getting complicated.</p>\n<p>Another idea. Maybe open a Spanner transaction and write to SpiceDB with the Spanner transaction open. I won't spend time on exploring this option, but it also doesn't solve anything, and in fact, just makes things worse. The truth is, none of the obvious workarounds actually make things better.</p>\n<p>So, we'll use these two simple lines of code as a starting point, and just acknowledge that there is a problem for us to solve there. The second write may fail for different reasons. It's either because of a network problem, or a problem with SpiceDB, or even the machine itself terminating after the first line. In all of these scenarios, the two databases become out of sync with each other. One of them will think that the device is in Home 1, and another will think that it is in Home 2.</p>\n<h3 id=\"user-content-data-integrity-problems-false-negatives-and-false-positives\">Data Integrity Problems: False Negatives and False Positives</h3>\n<p>The second write failing can create two types of data integrity problems. It's either SpiceDB is too restrictive. It doesn't allow access to someone who should have access, which is called a false negative on the slides. Or the opposite. SpiceDB can be too permissive, allowing access to someone who shouldn't have access. False negatives are more visible. It's more likely you would get a bug report for it from a customer. But false positives are actually more dangerous, because that's potentially a security issue.</p>\n<p>We've already tried several obvious workarounds, and none of them worked. But let's give it one last shot, given that it is false positives that are the main issue here. Maybe there is a simple way to get rid of those. Let's try a special write operations ordering. Namely, let's do SpiceDB deletes first. Then, in the same transaction, make all the changes to the main database. And then, do SpiceDB upserts.</p>\n<p>So, in our example, the device is first removed from home 1 in SpiceDB. And then, after the Spanner write, the device is added to home 2 in SpiceDB. And it actually does the trick. And it's easy to prove that it works not only in this example, but in general. If there are no negations in the schema, such an ordering of writes ensures no false positives from SpiceDB. So, now the dual write problem looks like this. Much better, isn't it? No security issues anymore.</p>\n<p>Let me play devil's advocate here. If the second or the third write fails, let's say, 100 times per month, we would probably hear from nobody. Or maybe one user. But for one user, can you fix it manually? But aren't we missing something here?</p>\n<h3 id=\"user-content-the-race-condition-problem\">The Race Condition Problem</h3>\n<p>The problem is, there is a whole class of issues we've ignored so far. It's race conditions. In this scenario from the slide, we're doing writes in the order that was supposed to totally eliminate the false positives. But as a result of these two requests from Alice and Bob, we get a false positive for Tom. That's because we're no longer talking about failing writes. None of the writes failed in this scenario. It is race conditions that caused the data integrity problem here.</p>\n<p>So, we have identified two causes or two sources of discrepancies between the two databases. The first is failing writes. And the second is race conditions. So, unfortunately, yet another workaround doesn't really make much difference. Back to our initial simple starting point. Two consecutive writes. First write to the main database. And then write to SpiceDB. Probably in a try-catch like here.</p>\n<p>And one last note looking at this diagram. Often people think about the dual write problem very simplistically. They think if they can make all the writes eventually succeed, that would solve the problem for them. So, all they need is a transactional outbox or a CDC, change data capture, or something like this. But that's not exactly the case. Because at the very least, there are also race conditions. And as we'll see very soon, it's even more than that.</p>\n<h3 id=\"user-content-adding-backfill-complexity\">Adding Backfill Complexity</h3>\n<p>And now, let's add backfill to the picture. If you're introducing a new field, a new type of information that you want to be present in multiple databases, you just make the schema changes, implement the dual write logic, and that's it. You can immediately start reading from the new field or a new column in all the databases. But if it's not a new type of information, if there is pre-existing data, then the data needs to be backfilled.</p>\n<p>Then the new column, field, or relation goes through these three phases. You can say there is a lifecycle. First, the schema definition changes. New column is created or something like this. Then, dual write is enabled. And finally, we do a backfill, which iterates through all of the existing data and writes it to the second database. And once the backfill is done, the data in the second database is ready to use. It's ready for reads and ready for access checks if we're talking about SpiceDB.</p>\n<p>And as it's easy to see from the backfill pseudocode, backfill also contributes to race conditions. Simply because the data may change between the read and write operations. And again, welcome false positives.</p>\n<p>Okay. So far, we've done two things. We've defined the problem. And we've examined multiple tempting workarounds just to find that they don't really solve anything. Now, let's take a look at several approaches used at Google and Canva that actually do work. And, of course, discuss their trade-offs.</p>\n<h3 id=\"user-content-solution-approaches\">Solution Approaches</h3>\n<h4 id=\"user-content-approach-1-cron-sync-jobs-googles-solution\">Approach 1: Cron Sync Jobs (Google's Solution)</h4>\n<p>First of all, doing nothing about it is probably not a good idea in most cases. Because authorization data integrity is really important. It's not only false negatives. It is false positives as well, which, as you remember, can be a security issue. The good news is there are multiple options to choose from if you want to solve the dual-write problem.</p>\n<p>And let's start with a solution we used in our team at Google, which is pretty simple. We just had a cron sync job. That job would run several times per day and fix all the discrepancies between our Spanner instance and Zanzibar. Looking at the code on the right side, because of the sync job, we can keep the dual-write code itself very, very simple. It's basically the two lines of code we started with.</p>\n<p>Sync jobs at Google are super common. And what made it even easier for us here is consistent snapshots. We could literally have a snapshot of both Spanner and Zanzibar for exactly the same instant. And then feed it as an input to our MapReduce style sync job, which would sync data for 100 millions of users in just a couple of hours.</p>\n<p>And interestingly, sync jobs are the only solution that truly guarantees eventual consistency, no matter what. Because in addition to write failures and races, there is also a third problem here. It is bugs in the data replication logic.</p>\n<p>Now, the most interesting part is how did it perform in practice? And thanks to our sync job, we actually know for sure how did it go. Visibility into the data integrity is a huge, huge benefit. We not only knew that all the discrepancies get fixed within several hours, but we also knew how many of them we actually had. And interestingly, the number of discrepancies was really high only when we had bugs in our replication logic. Race conditions and failed writes, they did cause some inconsistencies too. But even at our scale, there were a small number of them, typically tens or hundreds per day.</p>\n<p>Now, talking about the downsides of this approach, there are two main downsides. The first one is there are always some transient discrepancies, which can be there for several hours. Because we're not trying to address race conditions or failing writes in real time. And the second problem is infra costs. Running a sync job for a large database almost continuously is really, really expensive.</p>\n<h4 id=\"user-content-transactional-outbox-pattern-foundation\">Transactional Outbox Pattern Foundation</h4>\n<p>All right. We're done with the sync jobs. Now, all the other approaches we'll be looking at, they leverage the transactional outbox pattern. For some of those approaches, you could achieve similar results with CDC, change data capture, instead of the outbox. But outbox is more flexible, so we'll stick to it.</p>\n<p>And at its core, the transactional outbox pattern is really, really simple. When writing changes to the main database, in the same transaction, we also store a message saying, \"please write something to SpiceDB.\" And unlike traditional message queues outside of the main database, such an approach truly guarantees for us at-least-once delivery. And then there is a worker running continuously that pulls the messages from the outbox and acts upon them, makes the SpiceDB writes. Note that I mentioned a Zedtoken here in the code, but these are orthogonal to our topics, so I'll just skip them on the next slides.</p>\n<p>As I already mentioned, the problem the transactional outbox solves for us is reliable message delivery. Once SpiceDB and the network are in a healthy state, all the valid SpiceDB writes will eventually succeed. One less problem for us to worry about. But similar to CDC, it doesn't solve any of the other problems. It obviously doesn't provide any safety nets for the bugs in the data replication logic. And as it's easy to see from these examples, the transactional outbox is also subject to race conditions. Unless there are some extra properties guaranteed, which we'll talk very, very soon about.</p>\n<p>Okay. Now that we've set the stage with transactional outboxes, let's take a look at several solutions. The second approach to solving the dual-write problem is what I would call micro-syncs. Not sure if there's a proper term for it, but let me explain what I mean. In many ways, it's very similar to the first approach, cron sync jobs. But instead of doing a sync for the whole databases, we would be doing targeted syncs for specific relationships only.</p>\n<p>For example, if Bob's role in Team X changed, we would completely resync Bob's membership in that team, including all his roles. So in the worker, we would pull the message from the outbox, then read the data from both databases, and fix it in SpiceDB if there are any discrepancies.</p>\n<p>To make it scale, instead of writing it to SpiceDB from the worker directly, we can pull those messages in batches and just put them into another durable queue, for example, into Amazon SQS. And then we can have as many workers as we need to process those messages.</p>\n<p>But aren't these micro-syncs subject to races themselves? They are. Here on this diagram, you can see an example of such a race condition creating a discrepancy. But adding just a several-seconds delay makes such races highly unlikely. And for our own peace of mind, we can even process the same message again, let's say in one hour. Then races become practically impossible. I mean, yes, in theory, the internet is a weird thing that doesn't make any guarantees. But in practice, even TCP retransmissions, they won't take an hour.</p>\n<p>So the race conditions are solved with significantly delayed micro-syncs. And you can even do multiple syncs for the same message with different delays.</p>\n<p>Now, what about bugs in the data replication logic? And in practice, that's the only difference with the first approach, is that micro-syncs, they do not cover some types of bugs. Specifically, let's say you're introducing a new flow that modifies the source of truth, but then you simply forget to update SpiceDB in that particular flow. Obviously, if there is no message sent, there is no micro-sync, and there would be a discrepancy. But apart from that, there are no other substantial downsides in micro-syncs. They provide you with almost the same set of benefits as normal sync jobs, and even fix discrepancies on average much, much faster, which is pretty exciting.</p>\n<p>And finally, let's take a look at a couple of options that do not rely on syncs between the databases. Let's introduce a version field for each replicated field. In our home automation example, it would be a home version column in the devices table, and a corresponding home version relation in the SpiceDB device definition. And then we must ensure that each write to the home ID field in Spanner increments the device home version value. And then in the message itself, we also provide this new version value so that when the worker writes to SpiceDB, it can do a conditional write to make sure it doesn't override a newer home value with an older one.</p>\n<p>And there are different options for how to implement this. But none of them are really simple. So introducing a bug in the replication logic, honestly, is pretty easy. And the worst thing is, unlike sync jobs or even micro-syncs, this approach doesn't provide you with any safety nets. When you introduce a bug, it won't even make it visible. So yeah, that's the three downsides of this approach. It's complexity, no visibility into the replication consistency, and no safety nets. And the main benefit is, it does guarantee there would be no inconsistencies from race conditions or failed writes.</p>\n<p>And the last option is here more for completeness. To explore the idea that lies on the surface and, in fact, almost works, but there are a lot of nuances, limitations, and pitfalls to avoid there. And that's the only option where we solve the dual write problem by actually abandoning the dual write logic. So let's say we have a transactional outbox. And the only thing the service code does, it writes to the main database and the transactional outbox. No SpiceDB writes there. So there is no dual write.</p>\n<p>And there is just a single worker that processes a single message at a time, the oldest message available in the transactional outbox, and then it attempts to make a SpiceDB write until it succeeds. So the transactional outbox is basically a queue. And that by itself guarantees eventual consistency. I'll give you some time to digest this statement.</p>\n<p>You can prove that as long as there are no bugs, the transactional outbox is a queue, and there is a single consumer, eventual consistency between the main database and SpiceDB is guaranteed. Because it's FIFO, first in, first out, and there are no SpiceDB writes from service code.</p>\n<p>However, a single worker processing one message at a time from a queue wouldn't provide us with a high throughput. So you might be tempted to, instead of writing to SpiceDB directly from the worker, to put it into another durable queue. But I'm sure you can see the problem with this change, right? We've lost the FIFO property. So now it's subject to races. Unless that second queue is FIFO as well, of course. But if it's FIFO, guess what? We're not increasing throughput.</p>\n<p>So yeah, if we're relying on the FIFO property to address race conditions, there is literally no reason to transfer messages into another durable queue. If you want to increase the throughput, just use bulk SpiceDB writes]. But you would need to preprocess them to make sure there are no conflicts within the same batch. Yes, there is no horizontal scalability, but maybe that's not a problem for you.</p>\n<p>Yet, what would probably be a problem for most use cases is that a single problematic write can stop the whole replication process. And once we actually experienced exactly this issue, a single malformed SpiceDB write halting the whole replication process for us. And that's pretty annoying, as it requires manual intervention and is pretty urgent.</p>\n<p>And yet another class of race conditions is introduced by backfills. Because FIFO is a property of the transactional outbox. But backfill writes, fundamentally, they do not go through the outbox. So, yeah. While it's possible to introduce a delay to the transactional outbox specifically for the backfill phase, to address it, I would say the overall amount of problems with this approach is already pretty catastrophic.</p>\n<p>So, let's do a quick summary. We've explored four different approaches to solving the dual write problem. And here is a trade-off table with the pros and cons of each of them. The obvious loser is the last FIFO transactional outbox option. And probably conditional writes with the version field are not the most attractive solution as well. Mostly because of their complexity and lack of visibility into the replication consistency.</p>\n<p>So, the two options we're probably choosing from are the first and the second one. It's two types of syncs. Either a classic cron sync job or micro syncs. And, yeah. You can totally combine most of these approaches with each other if you want.</p>\n<p>We're almost done. I just wanted to reiterate the fact that dual write is not a SpiceDB problem. It's a data replication problem. So, let's say you're doing event-driven replication. Strictly speaking, there are no dual writes, same as in the last FIFO option. But, ultimately, there are two writes to two different systems, to two different databases. So, we're facing exactly the same set of problems.</p>\n<p>Adding a transactional outbox can kind of ensure that all the valid writes will eventually succeed. But, probably only if you own the other end of the replication process. Then, you can also add the FIFO property to address race conditions, which is option four. But, the first three approaches without Zanzibar or SpiceDB would be really tricky, if not impossible. Not only because of the data ownership problem, but also because of aggregates. With event-driven replication, you're probably not replicating simple atomic facts.</p>\n<p>So, yeah. SpiceDB makes the dual write problem, and ultimately the data integrity problem, much more manageable.</p>\n<p>And that's it. Hopefully, this presentation brought some clarity into the highly complex dual write problem.</p>",
            "url": "https://authzed.com/blog/the-dual-write-problem-in-spicedb-a-deep-dive-from-google-and-canva-experience",
            "title": "The Dual-Write Problem in SpiceDB: A Deep Dive from Google and Canva Experience",
            "summary": "In this technical deep-dive, Canva software engineer Artie Shevchenko draws on five years of experience with centralized authorization systems, first with Google's Zanzibar and now with SpiceDB, to tackle one of the most challenging aspects of authorization system implementation: the dual-write problem. This talk was part of the Authorization Infrastructure event hosted by AuthZed on August 20, 2025.",
            "image": "https://authzed.com/images/blogs/a5-recap-canva.png",
            "date_modified": "2025-09-16T08:00:00.000Z",
            "date_published": "2025-09-16T08:00:00.000Z",
            "author": {
                "name": "Artie Shevchenko",
                "url": "https://au.linkedin.com/in/artie-shevchenko-67845a4b"
            }
        },
        {
            "id": "https://authzed.com/blog/turos-spicedb-success-story-how-the-leading-car-sharing-platform-transformed-authorization",
            "content_html": "<p><em>This talk was part of the Authorization Infrastructure event hosted by AuthZed on August 20, 2025.</em></p>\n<h2 id=\"user-content-hosting-teams-a-case-for-spicedb\">Hosting Teams: A Case for SpiceDB</h2>\n<p>Andre, a software engineer at <a href=\"https://turo.com\">Turo</a>, shared how the world's leading car-sharing platform solved critical security and scalability challenges by implementing SpiceDB with managed hosting from <a href=\"/products/authzed-dedicated\">AuthZed Dedicated</a>. Faced with fleet owners having to share passwords due to rigid ownership-based permissions, Turo built a relationship-based authorization system enabling fine-grained, team-based access control. The results speak for themselves: <strong>\"SpiceDB made it trivial to design and implement the solution compared to traditional relational databases\"</strong> while delivering <strong>\"much higher performance and throughput.\"</strong> The system proved remarkably adaptable—adding support for inactive team members required <strong>\"literally one single line of code\"</strong> to change in the schema. AuthZed's managed hosting proved equally impressive, with only one incident in over two years of production use. As Andre noted, <strong>\"ultimately hosting with AuthZed saved us money in the long run\"</strong> by eliminating the need for dedicated infrastructure engineering, allowing Turo to focus on their core business while maintaining a <strong>\"blistering fast\"</strong> authorization system.</p>\n<p><strong>On Reliability and Expert Support:</strong></p>\n<blockquote>\n<p>\"In over two years [...] of operations in production, we had a single incident. And even then in that event, they demonstrated the capacity to recover from faults very, very quickly.\"</p>\n</blockquote>\n<p><strong>On Business Focus:</strong></p>\n<blockquote>\n<p>\"For over two years, Turo has used AuthZed's [Dedicated] offering where they're responsible for deploying and maintaining all the infrastructure required by the SpiceDB clusters. And that gives us time back to focus on growing our business, which is our primary concern.\"</p>\n</blockquote>\n<p><lite-youtube videoid=\"uz_gxz3whS0\" playlabel=\"Hosting Teams: A Case for SpiceDB\" params=\"start=4258\"></lite-youtube></p>\n<hr>\n<h2 id=\"user-content-full-transcript\">Full Transcript</h2>\n<p><em>Talk by Andre Sanches, Software Engineer at Turo</em></p>\n<p>Hello, everyone, and welcome. I'm Andre, a software engineer at Turo, working with SpiceDB for just over two years now. I'm here to share a bit of our experience with SpiceDB as a product and AuthZed as a hosting partner. Congratulations, by the way, to AuthZed for its five-year anniversary. It's a privilege to be celebrating this milestone together. So let's get started.</p>\n<h3 id=\"user-content-introduction-to-turo\">Introduction to Turo</h3>\n<p>First, a quick introduction to those who don't know Turo. We're the leading car-sharing platform in the world, operating in most of the US and four other countries. Our mission is to put the world's 1.5 billion cars to better use. Our business model is similar to popular home-sharing platforms you may be familiar with, with a fundamental difference. Vehicles are less expensive compared to homes, so it's common that hosts build up fleets of vehicles on Turo. In fact, many of our hosts build successful businesses with our help, and therein lies a challenge that we solved with SpiceDB.</p>\n<h3 id=\"user-content-the-challenge\">The Challenge</h3>\n<p>Hosts have responsibilities, such as communicating with guests in a timely manner, taking pictures of vehicles prior to handoff, and again, upon return of the vehicle to resolve disputes that may happen, managing vehicle schedules, etc. These things take time and effort, and as you scale up your business, fleet owners often hire people to help. And the problem is, in the past, Turo had a flat, ownership-based permission model. You could only interact with the vehicles you own, so hosts had no other choice but to share their accounts and their passwords. It's safe to say that folks in the target audience of this event understand how big of a problem that can be.</p>\n<p>Moreover, third-party companies started sprouting all over the place to bridge that gap, to manage teams by way of calling our backend, which adds yet another potential attack vector by accessing Turo's customer data. So, it had become a large enough risk and a feature gap that we set out to solve that problem.</p>\n<h3 id=\"user-content-the-solution\">The Solution</h3>\n<p>The solution was to augment the flat, ownership-based model with a team-based approach, where admin hosts, meaning the fleet owner, can create teams that authorize individual drivers to perform specific actions, really fine-grained, on one or more of the vehicles that they own. Members are invited to join teams via email, which gives them the opportunity to sign up for a Turo account if they don't yet have one.</p>\n<p>So, the solution from a technical standpoint is a graph-based solution that enables our backend to determine very quickly, can Driver ABC perform a certain action on vehicle XYZ? In this case right here, can Driver ABC communicate with guests that booked that certain vehicle? SpiceDB made it trivial to design and implement the solution compared to traditional relational databases, which is most of our backend. Moreover, it offloaded our monolithic database with a tool that offers much higher performance and throughput.</p>\n<h3 id=\"user-content-implementation-details\">Implementation Details</h3>\n<p>Anecdotally, the simplicity of SpiceDB helped implement a last-minute requirement that crept in late in the development cycle—support for inactive team members, the ones who are pending invitation acceptance. Prior to that, the invitation system was purely controlled in MySQL. And we realized, you know what, if we're storing the team in SpiceDB, why not make it so that we can store inactive users too? And the reason I'm mentioning this is this impressed everybody who was working on that feature at the time, because it was literally one single line of code that we had to change in the schema to enable this.</p>\n<p>So I'll talk more about this in a second where I show some technical things. But the graph that I just mentioned then roughly translates to this schema. So this is a simplified but still accurate rendition of what our SpiceDB schema looks like. Hopefully this clarifies how driver membership propagates to permissions on vehicles, if you're familiar with SpiceDB schemas.</p>\n<p>Some noteworthy mentions here are self-referencing relations, this one up here, or all the way up there. So basically, this is how we implemented the inactive users. If you notice that there, there's the member role and then an active member role. And by way of adding a single record that connects the member role with an active member role in the hosting team, you can enable and disable drivers. So this was so incredibly impressive at the time, because we thought we're going to have to change the entire schema and a whole bunch of other changes. And no, that's all it took.</p>\n<p>And again, it's one of those things that once it clicked, if you're familiar with the SpiceDB but not with the self-referencing relation, looking at this, that #member role and pointing to a relation in the same definition, it kind of looks a little daunting. It did to me. I don't know—you're probably smarter than I am, but it was daunting. But then one day it just clicked and I'm like, hmm, okay, that's how it is. And I was super stoked to continue working with SpiceDB and I'm going to implement more and more of the features. And help the feature team, actually, because it was a separate feature team that was working on this. So that self-referencing was interesting.</p>\n<h3 id=\"user-content-namespacing-feature\">Namespacing Feature</h3>\n<p>The other noteworthy mention here is the same namespaces. If you notice in front of the definition, there's a hosting teams forward slash. This is how we separate the schema into multiple copies of the same schema in the same cluster. So we have an ephemeral test environment in which we create and destroy on command sandbox replicas of our entire backend system. This enables us to deploy dozens, if not hundreds, of isolated copies of the schema, along with everything else in our backend, to test new features in a controlled environment that we can break, that we can modify as we see fit without affecting customers. And the namespacing feature in SpiceDB allowed us to use the same cluster for all those copies and save us some money. So we don't have to stand up a new server. We, you know, there's no computational costs or delays or any of that in provisioning computing resources and this and that.</p>\n<p>So the feature was released the week of, you know, us going pre-live, in a test environment. And we were probably the first adopters of this and it was really cool.</p>\n<h3 id=\"user-content-performance\">Performance</h3>\n<p>So let me see at a high level, this is how our hosting team feature works. You can see, let me use the mouse here. You can see how permissions propagate to teams. So, team pricing and availability goes to the relation of the team in the hosting team. Hosting team has the pricing and availability for active member roles or admin role. Plus sign, as you all know, is a or, and then it connects to the driver. Simple, fast. This is blistering fast.</p>\n<p>One other query that we make to SpiceDB very, very often—matter of fact, this is the single most, you know, issued query to SpiceDB at any given time—is, is the currently logged in user a cohost. And that's done for everybody. Even if you're not a cohost, this is how we determine whether you're a cohost or not. That will then drive UI, you know, decisions, what, what widgets to show. You know, only if you're, if it's pertinent to you, if you're a cohost, if not, then there's no, no reason to. To pollute the UI with, you know, cohosting features. Yeah.</p>\n<h3 id=\"user-content-user-interface\">User Interface</h3>\n<p>And this is what the UI looks like. So, you, on a team, you have cohosts and you can add or invite, here's an interesting thing. The code name of the project was cohosting. It ended up being hosting teams because we then used the nomenclature cohosts to add people to teams. So, here you have your cohosts. You can invite them by email. They get an, an email that points them to sign up to Turo. If they already have an account, they can just log in. And the moment they log in, it automatically accepts the invitation.</p>\n<p>Next you have the fine grain permissions of what your group can, or your team can do. In this case, we have trip management enabled. This is the base actually, you know, the base permission that you have to grant to everybody on the team. And then there's pricing and availability that allows you to set prices for vehicles, discounts, you know, see finances and all that stuff. So you can imagine why that's, you know, why it's very nice to be able to toggle this and not let, you know, just any cohost that has no business looking at your finances, you know, just hiding it from them by way of untoggling the permission here. And then you have your vehicles. The list shows all the vehicles you own. You just toggle the ones you want, save, and you're off to the races. Your hosting team is in place and working.</p>\n<h3 id=\"user-content-authzed-hosting-partnership\">AuthZed Hosting Partnership</h3>\n<p>So also that as a hosting partner, when you're considering using, you know, a big challenge of adopting a new system is setting it up and running it in a scalable and reliable way. You have to manage, you know, security issues. You have to manage our scaling. You have to manage all kinds of, you know, infrastructure challenges. And that costs money. In this day and age, it's really hard to find engineers who understand infrastructure well enough to manage all the moving parts of a highly scalable system such as SpiceDB.</p>\n<p>For over two years, Turo has used AuthZed's fully hosted cloud offering where they're responsible for deploying and maintaining all the infrastructure required by the SpiceDB clusters. And that gives us time back to focus on growing our business, which is our primary concern. So this is a great opportunity actually to give AuthZed a shout out for their excellent reliability.</p>\n<p>In over two years, over two years and three months now, actually of operations in production, we had a single incident. And even then in that event, they demonstrated the capacity to recover from faults very, very quickly to pinpoint the problem incredibly quickly. And, you know, take care of it. I think the outage was, we were out for like 38 minutes, something like that. It was, you know, we've had other partners that things were much, much more challenging. So, and once in two years, the root cause, the entire handling of the outage was very, very, you know, nice to see. Because it involved thorough analysis, post-mortems and making sure that it doesn't happen again, putting in safeguards to ensure that it doesn't happen again.</p>\n<p>So everything was, you know, systems fail. We understand that. And how we deal with it is how, is what shows how, you know, how good you are. And with AuthZed, we rest, you know, easy knowing that we're well taken care of. And ultimately hosting with AuthZed saved us money in the long run because it would otherwise take a lot of engineering time and effort just to keep the clusters running. So if your company is considering adopting SpiceDB, I would highly encourage you to have a chat with AuthZed about hosting as well. From our experience, it's well worth the investment.</p>",
            "url": "https://authzed.com/blog/turos-spicedb-success-story-how-the-leading-car-sharing-platform-transformed-authorization",
            "title": "Turo's SpiceDB Success Story: How the Leading Car-Sharing Platform Transformed Authorization",
            "summary": "Andre, a software engineer at Turo, shared how the world's leading car-sharing platform solved critical security and scalability challenges by implementing SpiceDB with managed hosting from AuthZed Dedicated. This talk was part of the Authorization Infrastructure event hosted by AuthZed on August 20, 2025.",
            "image": "https://authzed.com/images/blogs/a5-recap-turo.png",
            "date_modified": "2025-09-15T13:49:00.000Z",
            "date_published": "2025-09-15T13:49:00.000Z",
            "author": {
                "name": "Andre Sanches",
                "url": "https://www.linkedin.com/in/ansanch"
            }
        },
        {
            "id": "https://authzed.com/blog/authzed-is-5-event-recap-authorization-infrastructure-insights",
            "content_html": "<p>Last month we celebrated AuthZed's fifth birthday with our first-ever \"Authorization Infrastructure Event\" - a deep dive\ninto the technical challenges and innovations shaping the future of access control.</p>\n<p>The livestream brought together industry experts from companies like Canva and Turo to share real-world experiences with\nauthorization at scale, featured major product announcements including the launch of AuthZed Cloud, and included\nfascinating discussions with database researchers about the evolution of data infrastructure. From solving the\ndual-write consistency problem to powering OpenAI's document processing, we covered the full spectrum of modern\nauthorization challenges.</p>\n<p><strong><a href=\"https://youtu.be/uz_gxz3whS0\">Watch the full event recording</a></strong> (2.5 hours)</p>\n<h2 id=\"user-content-the-big-news\">The Big News</h2>\n<p>Before we dive into the technical talks, let's start with the big announcements:</p>\n<h3 id=\"user-content-authzed-cloud-is-live\">AuthZed Cloud is Live</h3>\n<p>We finally launched <a href=\"https://authzed.com/cloud\">AuthZed Cloud</a> - a self-service platform that allows you to provision,\nmanage, and scale your\nauthorization infrastructure on demand. Sign up with a credit card, get your permission system running in minutes, and\nscale as needed - authorization that runs like cloud infrastructure. Through\nour <a href=\"https://authzed.com/cloud/starter-program\">AuthZed Cloud Starter Program</a>, we're\nalso providing credits to help teams try out the platform.</p>\n<p><strong><a href=\"https://youtu.be/uz_gxz3whS0?t=197\">Watch Jake's keynote</a></strong></p>\n<h3 id=\"user-content-authzed-powers-openais-data-connectors\">AuthZed Powers OpenAI's Data Connectors</h3>\n<p><a href=\"https://authzed.com/customers/openai\">OpenAI</a> securely connects enterprise knowledge with ChatGPT by using AuthZed to\nhandle permissions for their corporate data connectors - when ChatGPT connects to your company's Google Drive or\nSharePoint. They've built connectors to process and search over <strong>37 billion documents</strong> for more than 5 million\nbusiness users while respecting existing data permissions using AuthZed's authorization infrastructure.</p>\n<p>This demonstrates how authorization infrastructure has become critical for AI systems that need to understand and\nrespect complex organizational data permissions at massive scale.</p>\n<h2 id=\"user-content-technical-deep-dives-and-customer-stories\">Technical Deep Dives and Customer Stories</h2>\n<h3 id=\"user-content-real-talk-the-dual-write-problem\">Real Talk: The Dual-Write Problem</h3>\n<p><strong>Artie Shevchenko from Canva</strong> delivered an excellent explanation of the dual-write problem that many authorization\nteams face. Anyone who has tried to keep data consistent between two different databases (such as your main database +\nSpiceDB) will recognize this challenge. <strong><a href=\"https://youtu.be/uz_gxz3whS0?t=1715\">Watch Artie's full talk</a></strong></p>\n<p>Artie was direct about the reality: the dual-write problem is hard. Here's what teams need to understand:</p>\n<p><strong>Things Will Go Wrong</strong></p>\n<ul>\n<li>Network calls fail between your database writes</li>\n<li>Race conditions happen when multiple requests hit at once</li>\n<li>Backfill processes create their own special category of chaos</li>\n</ul>\n<p><strong>Four Ways to Deal With It</strong></p>\n<ol>\n<li><strong>Sync jobs</strong> - Run periodic cleanup to fix inconsistencies. Expensive but reliable.</li>\n<li><strong>Micro-syncs</strong> - Target specific relationships when they change. Faster than full syncs.</li>\n<li><strong>Version fields</strong> - Add versioning to prevent overwriting newer data. Complex but prevents races.</li>\n<li><strong>FIFO queues</strong> - Process everything in order. Simple but doesn't scale well.</li>\n</ol>\n<p>Canva uses sync jobs as their safety net. Artie's team found that most inconsistencies actually came from bugs in their replication logic, not from the network problems everyone worries about. The sync jobs caught everything and gave them visibility into what was actually happening.</p>\n<p><strong>The Real Lesson</strong>: Don't try to be clever. Pick an approach, implement it well, and have monitoring so you know when things break.</p>\n<h3 id=\"user-content-how-turo-built-authorization-that-actually-works\">How Turo Built Authorization That Actually Works</h3>\n<p><strong>Andre Sanches from Turo</strong> told the story of how they moved from \"just share your password with your employees\" to\naccurate fine-grained access controls. <strong><a href=\"https://youtu.be/uz_gxz3whS0?t=4247\">Watch Andre's talk</a></strong></p>\n<p><strong>The Problem Was Real</strong>\nTuro hosts were sharing account credentials with their team members. Fleet owners needed help managing vehicles, but\nTuro's permission system only understood \"you own it or you don't.\" This created significant security challenges.</p>\n<p><strong>The Solution Was Surprisingly Straightforward</strong>\nAndre's team built a relationship-based permission system using SpiceDB that supports:</p>\n<ul>\n<li>Teams with admin and member roles</li>\n<li>Fine-grained permissions (who can message guests vs. who can see finances)</li>\n<li>Vehicle-level access controls</li>\n<li>Support for pending team invitations</li>\n</ul>\n<p>The best part? When they needed to add support for inactive team members late in development, it was literally a\none-line schema change. This exemplifies the utility of SpiceDB schemas and authorization as infrastructure.</p>\n<p><strong>Two Years Later</strong>\nTuro has had exactly one incident with their AuthZed Dedicated deployment in over two years - and that lasted 38 minutes. Andre made it clear: letting AuthZed handle the infrastructure complexity was absolutely worth it. His team focuses on building features, not babysitting databases.</p>\n<h3 id=\"user-content-database-philosophy-and-spicy-takes\">Database Philosophy and Spicy Takes</h3>\n<p><strong>Professor Andy Pavlo from Carnegie Mellon</strong> joined our co-founder <strong>Jimmy Zelinskie</strong> for a chat about databases, AI,\nand why new data models keep trying to kill SQL. <strong><a href=\"https://youtu.be/uz_gxz3whS0?t=6707\">Watch the fireside chat</a></strong></p>\n<p><strong>The SQL Cycle</strong>\nAndy's been watching this pattern for decades:</p>\n<ol>\n<li>Someone announces SQL is dead and their new data model is the future</li>\n<li>Everyone gets excited about the revolutionary approach</li>\n<li>Turns out the new thing solves some problems but creates others</li>\n<li>SQL absorbs the useful parts and keeps trucking</li>\n</ol>\n<p>Vector databases? Being absorbed into PostgreSQL. Graph databases? SQL 2024 added property graph queries. NoSQL? Most of those companies quietly added SQL interfaces.</p>\n<p><strong>The Spiciest Take</strong>\nJimmy dropped this one: \"The PostgreSQL wire protocol needs to die.\"</p>\n<p>His argument: Everyone keeps reimplementing PostgreSQL compatibility thinking they'll get all the client library benefits for free. But what actually happens is you inherit all the complexity of working around a pretty terrible wire protocol, and you never know how far down the rabbit hole you'll need to go.</p>\n<p>Andy agreed it's terrible, but pointed out there's not enough incentive for anyone to build something better. Classic tech industry problem.</p>\n<p><strong>AI and Databases</strong>\nThey both agreed that current AI hardware isn't radically different from traditional computer architecture - it's just specialized accelerators. The real revolution will come from new hardware designs that change how we think about data processing entirely.</p>\n<h2 id=\"user-content-sneak-peeks-from-the-authzed-lab\">Sneak Peeks from the AuthZed Lab</h2>\n<h3 id=\"user-content-postgresql-foreign-data-wrapper\">PostgreSQL Foreign Data Wrapper</h3>\n<p><strong>Joey Schorr (our CTO)</strong> showed off something that made me genuinely excited: a way to make SpiceDB look like regular\nPostgreSQL tables. <strong><a href=\"https://youtu.be/uz_gxz3whS0?t=3461\">Watch Joey's demo</a></strong></p>\n<p>You can literally write SQL like this:</p>\n<pre><code class=\"hljs language-sql\"><span class=\"hljs-keyword\">SELECT</span> <span class=\"hljs-operator\">*</span> <span class=\"hljs-keyword\">FROM</span> documents\n<span class=\"hljs-keyword\">JOIN</span> permissions <span class=\"hljs-keyword\">ON</span> documents.id <span class=\"hljs-operator\">=</span> permissions.resource_id\n<span class=\"hljs-keyword\">WHERE</span> permissions.subject_id <span class=\"hljs-operator\">=</span> <span class=\"hljs-string\">'user:jerry'</span> <span class=\"hljs-keyword\">AND</span> permissions.permission <span class=\"hljs-operator\">=</span> <span class=\"hljs-string\">'view'</span>\n<span class=\"hljs-keyword\">ORDER</span> <span class=\"hljs-keyword\">BY</span> documents.title <span class=\"hljs-keyword\">DESC</span>;\n</code></pre>\n<p>The foreign data wrapper handles the SpiceDB API calls behind the scenes, and PostgreSQL's query planner figures out the optimal way to fetch the data. Authorization-aware queries become just... queries.</p>\n<h3 id=\"user-content-authzed-materialize-gets-real\">AuthZed Materialize Gets Real</h3>\n<p><strong>Victor Roldán Betancort</strong> demonstrated AuthZed Materialize, which precomputes complex permission decisions so SpiceDB\ndoesn't have to traverse complex relationship graphs in real-time. <strong><a href=\"https://youtu.be/uz_gxz3whS0?t=5940\">Watch Victor's demo</a></strong></p>\n<p>The demo showed streaming permission updates into DuckDB, then running SQL queries against the materialized permission\nsets. This creates a real-time index of who can access what, without the performance penalty of traversing permission\nhierarchies on every query.</p>\n<h3 id=\"user-content-authorization-and-mcp-servers\">Authorization and MCP Servers</h3>\n<p><strong>Sam Kim</strong> talked about authorization for Model Context Protocol servers and released a reference implementation for a\nMCP server with fine-grained authorization support build in. <strong><a href=\"https://youtu.be/uz_gxz3whS0?t=9640\">Watch Sam's MCP talk</a></strong></p>\n<p>The key insight: if you don't build official MCP servers for your APIs, someone else will. And you probably won't like how they handle authorization. Better to get ahead of it with proper access controls baked in.</p>\n<h2 id=\"user-content-what-were-thinking-about\">What We're Thinking About</h2>\n<p><strong>Irit Goihman (our VP of Engineering)</strong> shared some thoughts on how we approach building software. <strong><a href=\"https://youtu.be/uz_gxz3whS0?t=9139\">Watch Irit's insights</a></strong></p>\n<ul>\n<li><strong>Bottom-up innovation</strong>: Engineers who talk to customers and operate what they build make better decisions</li>\n<li><strong>Responsible AI adoption</strong>: We use AI tools extensively, but with humans in the loop and measurable outcomes</li>\n<li><strong>Test coverage through AI</strong>: AI-generated test cases with human review have significantly improved our coverage</li>\n</ul>\n<p>Remote-first engineering teams need different approaches to knowledge sharing and innovation.</p>\n<h2 id=\"user-content-community-love\">Community Love</h2>\n<p>We recognized the contributors who make SpiceDB a thriving open source project. The community response has been\nexceptional:</p>\n<p><strong>Core SpiceDB Contributors</strong>:</p>\n<ul>\n<li><strong>Kartikay Saxena</strong> - Student contributor who's been consistently improving the codebase</li>\n<li><strong>Braden Groom</strong> from Reddit - Bringing real-world production experience back to the project</li>\n<li><strong>Jesse White</strong> from RELiON - Infrastructure and reliability improvements</li>\n<li><strong>Sean Bryant</strong> from GitHub - Core functionality enhancements</li>\n<li><strong>Nicolas Barbey</strong> from leboncoin - International perspective and contributions</li>\n<li><strong>Chris Kellendonk</strong> from PagerDuty - Monitoring and observability improvements</li>\n<li><strong>Lex Cao</strong> - Performance and optimization work</li>\n<li><strong>Meyazhagan</strong> - Documentation and developer experience improvements</li>\n</ul>\n<p><strong>Client Library Heroes</strong> (making SpiceDB accessible everywhere):</p>\n<ul>\n<li><strong>Danh Tran Thanh</strong> - AuthZed-codegen for type-safe Go code generation</li>\n<li><strong>Michael Tanczos</strong> - SpiceDB.net bringing authorization to the .NET ecosystem</li>\n<li><strong>Shubham Gupta</strong> - AuthZed_ex for Elixir developers</li>\n<li><strong>Ioannis Canellos</strong> - Quarkus-AuthZed-client for Java/Quarkus apps</li>\n<li><strong>Lauren (Lurian)</strong> - SpiceDB-rust client for the Rust community</li>\n<li><strong>Thomas Richner</strong> - SpiceGen for Java client generation</li>\n<li><strong>David Alsbury &#x26; Michael O'Connell</strong> - Chipotle-rest PHP client (amazing name)</li>\n<li><strong>Link Orb team</strong> - Both spicedb-php HTTP client and spicedb-bundle Symfony integration</li>\n</ul>\n<p><strong>Community Tooling Builders</strong> (the ecosystem enablers):</p>\n<ul>\n<li><strong>Mohd Ejaz Siddiqui</strong> - SpiceDB UI for visual management</li>\n<li><strong>Chris Roemmich &#x26; Eytan Hanig</strong> - SpiceDB operator Helm charts for Kubernetes</li>\n<li><strong>Nicole Hubbard/Infratographer</strong> - Permissions API service layer</li>\n<li><strong>Guilherme Cassolato</strong> from Red Hat - Authorino-SpiceDB integration</li>\n<li><strong>Thomas Darimont</strong> - OPA-SpiceDB experiments bridging policy engines</li>\n<li><strong>Dominik Guhr</strong> from INNOQ - Keycloak-SpiceDB event listener</li>\n<li><strong>Chip</strong> - VS Code syntax highlighting for .zed files</li>\n<li><strong>Mike Leone</strong> - Tree-sitter grammar for AuthZed schema</li>\n</ul>\n<p>Every single one of these folks saw a gap and decided to fill it. That's what makes open source communities amazing.</p>\n<h2 id=\"user-content-looking-back-looking-forward\">Looking Back, Looking Forward</h2>\n<p>Five years ago, application authorization was often something that was DIY and hard to scale. Today, companies are\nprocessing billions of permission checks through purpose-built infrastructure.</p>\n<p>The next five years? AI agents are going to need authorization systems that don't exist yet. Real-time permission materialization will become table stakes. Integration with existing databases will get so seamless you won't think about it.</p>\n<h2 id=\"user-content-key-takeaways\">Key Takeaways</h2>\n<p>If you take anything away from our fifth birthday celebration, let it be this:</p>\n<ol>\n<li><strong>Managed authorization infrastructure</strong> lets you focus on building features instead of managing database operations</li>\n<li><strong>Relationship-based access control</strong> can express complex permissions elegantly instead of trying to force everything\ninto roles</li>\n<li><strong>Community-driven development</strong> makes everyone's authorization better</li>\n<li><strong>AI and authorization</strong> are going to become inseparable as AI agents are given access to more business data</li>\n</ol>\n<p>Authorization infrastructure has gone from \"development requirement\" to \"strategic advantage.\" The companies that figure\nthis out first will have a significant edge in keeping pace with quickening development cycles and heightene security\nneeds.</p>\n<p>Thanks to everyone who joined AuthZed for the celebration, and here's to the next five years of fixing access control\nfor everyone.</p>\n<hr>\n<p>Want to try AuthZed Cloud? <a href=\"https://authzed.com/cloud\">Sign up here</a> and get started in minutes.</p>\n<p>Join our community on <a href=\"https://discord.gg/authzed\">Discord</a> and\nstar <a href=\"https://github.com/authzed/spicedb\">SpiceDB on GitHub</a>.</p>",
            "url": "https://authzed.com/blog/authzed-is-5-event-recap-authorization-infrastructure-insights",
            "title": "AuthZed is 5: What We Learned from Our First Authorization Infrastructure Event",
            "summary": "We celebrated our 5th birthday with talks from Canva, Turo, and Carnegie Mellon. Here's what we learned about the dual-write problem, scaling authorization in production, and why everyone keeps reimplementing the PostgreSQL wire protocol.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-09-02T18:00:00.000Z",
            "date_published": "2025-09-02T18:00:00.000Z",
            "author": {
                "name": "Corey Thomas",
                "url": "https://www.linkedin.com/in/cor3ythomas/"
            }
        },
        {
            "id": "https://authzed.com/blog/authzed-cloud-is-now-available",
            "content_html": "<p>Today marks a special milestone for AuthZed: we're celebrating our 5th anniversary! There are honestly too many thoughts and reflections swirling through my mind to fit into a single blog post. The reality is that most startups don't make it to 5 years, and I'm extremely proud of what we've built together as a team and community.</p>\n<p>If you want to hear me reflect on the journey of the past 5 years, I'm giving a talk today about exactly that, and we'll post a link to the recording here when it's ready. But today isn't just about looking back, it's also about looking forward, and I’ve personally been looking forward to launching our next iteration of authorization infrastructure: <a href=\"https://authzed.com/cloud\">AuthZed Cloud</a>.</p>\n<p>In this blog post, I'll cover what we've built and why, but if you don't need that context and just want to dive in, feel free to bail on this post and <a href=\"/cloud/signup\">sign up right now</a>!</p>\n<h2 id=\"user-content-authzed-dedicated-is-the-perfect-product\">AuthZed Dedicated is the Perfect Product</h2>\n<p>To understand why we built AuthZed Cloud, I need to first talk about <a href=\"https://authzed.com/products/authzed-dedicated\">AuthZed Dedicated</a>, because in many ways, Dedicated represents our vision of the perfect authorization infrastructure product.</p>\n<p>AuthZed Dedicated is nearly infinitely scalable: capable of handling millions of queries per second when you need it. It's co-located with your workloads, which means there's no internet or cross-cloud latency penalty for your authorization decisions, which are often in the critical path for user interactions. It can run nearly anywhere on earth, with support for all three major cloud providers, giving you the flexibility to deploy where your business needs demand.</p>\n<p>Perhaps most importantly, Dedicated provides total isolation for each customer across datastore, network, and compute layers. It marries the best permissions database in the world (<a href=\"https://authzed.com/spicedb\">SpiceDB</a>) with the best infrastructure design (<a href=\"https://kubernetes.io/\">Kubernetes</a> + operators) to create what we believe is the best authorization infrastructure in the world.</p>\n<p>So how did we improve on this formula? We made it more accessible!</p>\n<p>AuthZed Dedicated's biggest challenge isn't technical: it's the enterprise procurement cycle that comes with it. The question we kept asking ourselves was: how can we bring these powerful concepts to more companies, especially those who need enterprise-grade authorization but can't navigate lengthy procurement processes?</p>\n<h2 id=\"user-content-introducing-authzed-cloud\">Introducing AuthZed Cloud</h2>\n<p>AuthZed Cloud takes the most powerful concepts from AuthZed Dedicated and makes them available in a self-service product that you can start using today.</p>\n<p>We've also made several key improvements over what’s available in Dedicated today:</p>\n<p><strong>Self-service registration and deployment</strong>: No more waiting weeks for procurement approvals or implementation calls. Sign up, configure your permissions system, and start building. Scale when you need to!</p>\n<p><strong>Roles</strong>: We've added granular access controls that let you limit who can access and change things within your AuthZed organizations. This was a frequent request from teams who needed to federate access to our platform in different ways. You’ll be happy to know that this feature is, of course, also powered by SpiceDB.</p>\n<p><strong>Usage-based billing</strong>: Instead of committing to fixed infrastructure costs upfront, you can spin up resources on-demand and pay for what you actually use.</p>\n<p>The best part? <strong>These improvements will also be landing in Dedicated soon, so all our customers benefit!</strong></p>\n<p>Delivering on this vision does require some compromises. AuthZed Cloud uses a shared control plane and operates in pre-selected regions (though please let us know if you need a region we don't support today!). But honestly, that's about it for compromises.</p>\n<h2 id=\"user-content-for-whom\">For Whom?</h2>\n<p>AuthZed Cloud is designed for companies of all sizes. Despite the shared infrastructure approach, we've maintained high isolation standards. Your SpiceDB runs as separate Kubernetes deployments, and datastores are dedicated per permissions system. You still get the same scalable technology from Dedicated that allows you to scale up to millions of queries per second when needed, and the same enterprise-grade reliability.</p>\n<p>What makes Cloud special is how attainable it is. The base price is a fraction of our base Dedicated deployment price, opening up AuthZed's capabilities to a much broader range of companies.</p>\n<p>That said, some organizations should still consider Dedicated. You might consider dedicated if you need higher isolation requirements like an isolated control plane or private networking, or if you need higher flexibility around custom legal terms or deployment in cloud provider regions that AuthZed Cloud doesn't yet support.</p>\n<h2 id=\"user-content-early-access-reception\">Early Access Reception</h2>\n<p>The response during our early access period has been incredible. There was clearly pent-up demand for a product like this! We've had several long-time AuthZed customers already making the move to Cloud.</p>\n<p>Lita Cho, CTO at moment.dev, had this to say:</p>\n<blockquote>\n<p>“We love Authzed—it makes evolving our permissions model effortless, with a powerful schema language, makes rapid\nprototyping possible along with rock-solid production performance, all without heavy maintenance. <strong>Authzed Cloud\ndelivers the power and reliability of Dedicated at a startup-friendly price, without the hassle of running SpiceDB. That\nlets me focus on building our modern docs platform, confident our authorization is secure, fast, and future-proof.”</strong></p>\n</blockquote>\n<h2 id=\"user-content-get-started-today\">Get Started Today</h2>\n<p>The best part about AuthZed Cloud is that you can sign up immediately and get started building. We've also set up a program where you can <a href=\"/cloud/starter-program\">apply for credits</a> to help with your initial implementation and testing.</p>\n<p>As we celebrate five years of AuthZed, I'm more excited than ever about the problems we're solving and the direction we're heading. Authorization remains one of the most critical and complex challenges in modern software development, and we're committed to making it accessible to every team that needs it.</p>\n<p>Here's to the next five years of building the future of authorization together.</p>",
            "url": "https://authzed.com/blog/authzed-cloud-is-now-available",
            "title": "AuthZed Cloud is Now Available!",
            "summary": "Bringing the power of AuthZed Dedicated to more with our new shared infrastructure, self-service offering: AuthZed Cloud.",
            "image": "https://authzed.com/images/upload/AuthZed-Cloud-Blog@2x.png",
            "date_modified": "2025-08-20T16:00:00.000Z",
            "date_published": "2025-08-20T16:00:00.000Z",
            "author": {
                "name": "Jake Moshenko",
                "url": "https://www.linkedin.com/in/jacob-moshenko-381161b/"
            }
        },
        {
            "id": "https://authzed.com/blog/predicting-the-latest-owasp-top-10-with-cve-data",
            "content_html": "<h2 id=\"user-content-predicting-the-latest-owasp-top-10-with-cve-data-from-2022-2025\"><strong>Predicting the latest OWASP Top 10 with CVE Data from 2022-2025</strong></h2>\n<p>OWASP is set to release their first Top 10 update since 2021, and this year’s list is one of the most awaited because of the generational shift that is AI. The security landscape has fundamentally shifted thanks to AI being embedded in production systems across enterprises from RAG pipelines to autonomous agents. I thought it would be a fun little exercise to look at CVE data from 2022-2025 and make predictions on what the top 5 in the updates list would look like. Read on to find out what I found.</p>\n<h2 id=\"user-content-the-owasp-top-10-list\"><strong>The OWASP Top 10 List</strong></h2>\n<p>The OWASP Top 10 is a regularly updated list of the most critical security risks to web applications. It’s a go-to reference for organizations looking to prioritize their security efforts. We’ve always had a keen eye on this list as it’s our mission to fix <a href=\"https://owasp.org/Top10/A01_2021-Broken_Access_Control/\">broken access control</a>.</p>\n<p>The last 4 lists have been released in 2010, 2013, 2017 and 2021 with the next list scheduled for release soon, in Q3 2025.</p>\n<p>The OWASP Foundation builds this list using a combination of large-scale vulnerability data, community surveys, and expert input. The goal is to create a snapshot of the most prevalent and impactful categories of web application risks. So I thought I’ll crunch some numbers from CVE data that is publicly available.</p>\n<hr>\n<h3 id=\"user-content-methodology\"><strong>Methodology</strong></h3>\n<p>This was <em>not</em> a scientific study — I’m not a data scientist, just an enthusiast in the cloud and security space. The aim here was to explore the data, learn more about how OWASP categories relate to CVEs and CWEs, and see if the trends point toward likely candidates for the upcoming list.</p>\n<p>Here’s the process I followed to get some metrics around the most common CVEs:</p>\n<ol>\n<li>\n<p><strong>Collect CVEs from 2022–2025</strong></p>\n<ul>\n<li>I pulled data from the <a href=\"https://nvd.nist.gov/vuln/data-feeds\">NVD</a> (National Vulnerability Database) API. Yearly JSON feeds are available with data from all the CVEs</li>\n<li>Since the last list came out in 2021, I limited the dataset to CVEs with a published date between January 1, 2022 and July 31, 2025.</li>\n</ul>\n</li>\n<li>\n<p><strong>Map CWEs to OWASP Top 10 Categories</strong></p>\n<ul>\n<li>Each CVE is linked to one or more <a href=\"https://cwe.mitre.org/\">CWE</a> (Common Weakness Enumeration) entries. A CWE is a community-developed list of common software and hardware weaknesses.</li>\n<li>I used <a href=\"https://cwe.mitre.org/data/definitions/1344.html\">OWASP’s official CWE mapping</a> (when available) to map the CWEs to entries in the OWASP list.</li>\n</ul>\n</li>\n</ol>\n<p>For example:</p>\n<p><a href=\"https://cwe.mitre.org/data/definitions/201.html\">CWE-201</a> - ‘Insertion of Sensitive Information Into Sent Data’ maps to ‘Broken Access Control’.</p>\n<p><img src=\"/images/upload/cwe-owasp.png\" alt=\"\"></p>\n<ul>\n<li>I extracted all the CWE IDs from the dataset and mapped a list of CWE IDs (e.g., \"CWE-201\") to their corresponding OWASP categories</li>\n</ul>\n<pre><code class=\"hljs language-py\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">map_cwe_to_owasp</span>(<span class=\"hljs-params\">cwe_ids</span>):\n   owasp_set = <span class=\"hljs-built_in\">set</span>()\n   <span class=\"hljs-keyword\">for</span> cwe <span class=\"hljs-keyword\">in</span> cwe_ids:\n       <span class=\"hljs-keyword\">try</span>:\n           cwe_num = <span class=\"hljs-built_in\">int</span>(cwe.replace(<span class=\"hljs-string\">\"CWE-\"</span>, <span class=\"hljs-string\">\"\"</span>))\n           <span class=\"hljs-keyword\">if</span> cwe_num <span class=\"hljs-keyword\">in</span> CWE_TO_OWASP:\n               owasp_set.add(CWE_TO_OWASP[cwe_num])\n       <span class=\"hljs-keyword\">except</span> ValueError:\n           <span class=\"hljs-keyword\">continue</span>\n   <span class=\"hljs-keyword\">return</span> <span class=\"hljs-built_in\">list</span>(owasp_set)\n</code></pre>\n<ul>\n<li>Here’s the mapping I made for the Top 8 categories from the 2021 list (truncated for readability, the full code is in the repo)</li>\n</ul>\n<pre><code class=\"hljs language-py\">CWE_TO_OWASP = {\n   <span class=\"hljs-comment\"># A01: Broken Access Control</span>\n   <span class=\"hljs-number\">22</span>: <span class=\"hljs-string\">\"A01:2021 - Broken Access Control\"</span>,\n   <span class=\"hljs-number\">23</span>: <span class=\"hljs-string\">\"A01:2021 - Broken Access Control\"</span>,\n   <span class=\"hljs-comment\"># ...</span>\n   <span class=\"hljs-number\">1275</span>: <span class=\"hljs-string\">\"A01:2021 - Broken Access Control\"</span>,\n\n\n   <span class=\"hljs-comment\"># A02: Cryptographic Failures</span>\n   <span class=\"hljs-number\">261</span>: <span class=\"hljs-string\">\"A02:2021 - Cryptographic Failures\"</span>,\n   <span class=\"hljs-number\">296</span>: <span class=\"hljs-string\">\"A02:2021 - Cryptographic Failures\"</span>\n   <span class=\"hljs-comment\"># ...,</span>\n   <span class=\"hljs-number\">916</span>: <span class=\"hljs-string\">\"A02:2021 - Cryptographic Failures\"</span>,\n\n\n   <span class=\"hljs-comment\"># A03: Injection</span>\n   <span class=\"hljs-number\">20</span>: <span class=\"hljs-string\">\"A03:2021 - Injection\"</span>,\n   <span class=\"hljs-number\">74</span>: <span class=\"hljs-string\">\"A03:2021 - Injection\"</span>,\n   <span class=\"hljs-comment\"># ...</span>\n   <span class=\"hljs-number\">917</span>: <span class=\"hljs-string\">\"A03:2021 - Injection\"</span>,\n\n\n   <span class=\"hljs-comment\"># A04 Insecure Design</span>\n   <span class=\"hljs-number\">73</span>: <span class=\"hljs-string\">\"A04:2021 - Insecure Design\"</span>,\n   <span class=\"hljs-number\">183</span>: <span class=\"hljs-string\">\"A04:2021 - Insecure Design\"</span>,\n   <span class=\"hljs-comment\"># ...</span>\n   <span class=\"hljs-number\">1173</span>: <span class=\"hljs-string\">\"A04:2021 - Insecure Design\"</span>,\n\n\n   <span class=\"hljs-comment\"># A05 Security Misconfiguration</span>\n   <span class=\"hljs-number\">2</span>: <span class=\"hljs-string\">\"A05:2021 - Security Misconfiguration\"</span>,\n   <span class=\"hljs-number\">11</span>: <span class=\"hljs-string\">\"A05:2021 - Security Misconfiguration\"</span>,\n   <span class=\"hljs-comment\"># ...</span>\n   <span class=\"hljs-number\">1032</span>: <span class=\"hljs-string\">\"A05:2021 - Security Misconfiguration\"</span>,\n    \n   <span class=\"hljs-comment\"># A05 Security Misconfiguration</span>\n   <span class=\"hljs-number\">937</span>: <span class=\"hljs-string\">\"A06:2021 - Vulnerable and Outdated Components\"</span>,\n   <span class=\"hljs-comment\"># ...   </span>\n   <span class=\"hljs-number\">1104</span>: <span class=\"hljs-string\">\"A06:2021 - Vulnerable and Outdated Components\"</span>,\n\n\n   <span class=\"hljs-comment\"># A07:2021 - Identification and Authentication Failures</span>\n   <span class=\"hljs-number\">255</span>: <span class=\"hljs-string\">\"A07:2021 - Identification and Authentication Failures\"</span>,\n   <span class=\"hljs-number\">259</span>: <span class=\"hljs-string\">\"A07:2021 - Identification and Authentication Failures\"</span>,\n   <span class=\"hljs-comment\"># ...</span>\n   <span class=\"hljs-number\">1216</span>: <span class=\"hljs-string\">\"A07:2021 - Identification and Authentication Failures\"</span>,\n\n\n   <span class=\"hljs-comment\"># A08:2021 - Software and Data Integrity Failures</span>\n   <span class=\"hljs-number\">345</span>: <span class=\"hljs-string\">\"A08:2021 - Software and Data Integrity Failures\"</span>,\n   <span class=\"hljs-number\">353</span>: <span class=\"hljs-string\">\"A08:2021 - Software and Data Integrity Failures\"</span>,\n   <span class=\"hljs-comment\"># ...   </span>\n   <span class=\"hljs-number\">915</span>: <span class=\"hljs-string\">\"A08:2021 - Software and Data Integrity Failures\"</span>,\n</code></pre>\n<ol start=\"3\">\n<li>\n<p><strong>Map CVEs to CWEs</strong></p>\n<ul>\n<li>The NVD 2.0 dataset embeds weaknesses under <code>cve.weaknesses[].description[].value</code> with CWE IDs like <a href=\"https://cwe.mitre.org/data/definitions/201.html\">CWE-201</a>. I wrote a script to process the JSON containing NVD vulnerability data to extract CWE IDs for each CVE, and then map it to OWASP categories.</li>\n</ul>\n</li>\n</ol>\n<pre><code class=\"hljs language-py\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">process_nvd_file</span>(<span class=\"hljs-params\">input_path, output_path</span>):\n   <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(input_path, <span class=\"hljs-string\">\"r\"</span>) <span class=\"hljs-keyword\">as</span> f:\n       data = json.load(f)\n\n\n   results = []\n   <span class=\"hljs-keyword\">for</span> entry <span class=\"hljs-keyword\">in</span> data[<span class=\"hljs-string\">\"vulnerabilities\"</span>]:\n       cve_id = entry.get(<span class=\"hljs-string\">\"cve\"</span>, {}).get(<span class=\"hljs-string\">\"id\"</span>, <span class=\"hljs-string\">\"UNKNOWN\"</span>)\n       cwe_ids = []\n\n\n       <span class=\"hljs-comment\"># Extract CWE IDs from weaknesses</span>\n       <span class=\"hljs-keyword\">for</span> problem <span class=\"hljs-keyword\">in</span> entry.get(<span class=\"hljs-string\">\"cve\"</span>, {}).get(<span class=\"hljs-string\">\"weaknesses\"</span>, []):\n           <span class=\"hljs-keyword\">for</span> desc <span class=\"hljs-keyword\">in</span> problem.get(<span class=\"hljs-string\">\"description\"</span>, []):\n               cwe_id = desc.get(<span class=\"hljs-string\">\"value\"</span>)\n               <span class=\"hljs-keyword\">if</span> cwe_id <span class=\"hljs-keyword\">and</span> cwe_id != <span class=\"hljs-string\">\"NVD-CWE-noinfo\"</span>:\n                   cwe_ids.append(cwe_id)\n\n\n       mapped_owasp = map_cwe_to_owasp(cwe_ids)\n\n\n       results.append({\n           <span class=\"hljs-string\">\"cve_id\"</span>: cve_id,\n           <span class=\"hljs-string\">\"cwe_ids\"</span>: cwe_ids,\n           <span class=\"hljs-string\">\"owasp_categories\"</span>: mapped_owasp\n       })\n\n\n   <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(output_path, <span class=\"hljs-string\">\"w\"</span>) <span class=\"hljs-keyword\">as</span> f:\n       json.dump(results, f, indent=<span class=\"hljs-number\">2</span>)\n\n\n   <span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">f\"Wrote <span class=\"hljs-subst\">{<span class=\"hljs-built_in\">len</span>(results)}</span> CVE entries with OWASP mapping to <span class=\"hljs-subst\">{output_path}</span>\"</span>)\n</code></pre>\n<p>We now have a new JSON file with mapped outputs that has all the CVEs mapped to OWASP categories (if there’s a match). This is what it looks like:</p>\n<pre><code class=\"hljs language-json\"><span class=\"hljs-punctuation\">{</span>\n    <span class=\"hljs-attr\">\"cve_id\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"CVE-2024-0185\"</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"cwe_ids\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n      <span class=\"hljs-string\">\"CWE-434\"</span><span class=\"hljs-punctuation\">,</span>\n      <span class=\"hljs-string\">\"CWE-434\"</span>\n    <span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"owasp_categories\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n      <span class=\"hljs-string\">\"A04:2021 - Insecure Design\"</span>\n    <span class=\"hljs-punctuation\">]</span>\n  <span class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-punctuation\">{</span>\n    <span class=\"hljs-attr\">\"cve_id\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"CVE-2024-0186\"</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"cwe_ids\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n      <span class=\"hljs-string\">\"CWE-640\"</span>\n    <span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n    <span class=\"hljs-attr\">\"owasp_categories\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n      <span class=\"hljs-string\">\"A07:2021 - Identification and Authentication Failures\"</span>\n    <span class=\"hljs-punctuation\">]</span>\n  <span class=\"hljs-punctuation\">}</span><span class=\"hljs-punctuation\">,</span>\n</code></pre>\n<p>I ran this code snippet for each data set from 2022-2025 and had separate JSON files for each year.</p>\n<ol start=\"4\">\n<li><strong>Analyze the Data</strong></li>\n</ol>\n<p>Now that we have this data of mapped outputs, we can run some data analysis to find the most common occurrences per year.</p>\n<ul>\n<li>I essentially counted the number of CVEs per OWASP category for each year.</li>\n</ul>\n<pre><code class=\"hljs language-py\"><span class=\"hljs-keyword\">for</span> filename <span class=\"hljs-keyword\">in</span> os.listdir(DATA_DIR):\n\n<span class=\"hljs-comment\"># Loads the JSON data from the file, which contains a list of CVE entries.</span>\n\n    year = filename.replace(<span class=\"hljs-string\">\"mapped_output_\"</span>, <span class=\"hljs-string\">\"\"</span>).replace(<span class=\"hljs-string\">\".json\"</span>, <span class=\"hljs-string\">\"\"</span>)\n    year_path = os.path.join(DATA_DIR, filename)\n\n    <span class=\"hljs-keyword\">with</span> <span class=\"hljs-built_in\">open</span>(year_path, <span class=\"hljs-string\">\"r\"</span>) <span class=\"hljs-keyword\">as</span> f:\n        entries = json.load(f)\n\n    <span class=\"hljs-keyword\">for</span> entry <span class=\"hljs-keyword\">in</span> entries:\n        <span class=\"hljs-keyword\">for</span> category <span class=\"hljs-keyword\">in</span> entry.get(<span class=\"hljs-string\">\"owasp_categories\"</span>, []):\n            yearly_data[year][category] += <span class=\"hljs-number\">1</span>\n</code></pre>\n<ul>\n<li>To get some visualizations around the data, I sorted and added graphs to see which categories were trending.</li>\n</ul>\n<pre><code class=\"hljs language-py\"><span class=\"hljs-comment\"># Convert to a DataFrame</span>\ndf = pd.DataFrame(yearly_data).fillna(<span class=\"hljs-number\">0</span>).astype(<span class=\"hljs-built_in\">int</span>).sort_index()\ndf = df.T.sort_index()  <span class=\"hljs-comment\"># years as rows</span>\n\n<span class=\"hljs-comment\"># Save summary</span>\ndf.to_csv(<span class=\"hljs-string\">\"owasp_counts_by_year.csv\"</span>)\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\\nSaved summary to owasp_counts_by_year.csv\"</span>)\n\n<span class=\"hljs-comment\"># Also print</span>\n<span class=\"hljs-built_in\">print</span>(<span class=\"hljs-string\">\"\\n=== OWASP Category Counts by Year ===\"</span>)\n<span class=\"hljs-built_in\">print</span>(df.to_string())\n\n<span class=\"hljs-comment\"># Plot OWASP trends over time</span>\nplt.figure(figsize=(<span class=\"hljs-number\">12</span>, <span class=\"hljs-number\">7</span>))\n\n<span class=\"hljs-keyword\">for</span> column <span class=\"hljs-keyword\">in</span> df.columns:\n    plt.plot(df.index, df[column], marker=<span class=\"hljs-string\">'o'</span>, label=column)\n\nplt.title(<span class=\"hljs-string\">\"OWASP Top 10 Category Trends (2022–2025)\"</span>)\nplt.xlabel(<span class=\"hljs-string\">\"Year\"</span>)\nplt.ylabel(<span class=\"hljs-string\">\"Number of CVEs\"</span>)\nplt.xticks(rotation=<span class=\"hljs-number\">45</span>)\nplt.legend(title=<span class=\"hljs-string\">\"OWASP Category\"</span>, bbox_to_anchor=(<span class=\"hljs-number\">1.05</span>, <span class=\"hljs-number\">1</span>), loc=<span class=\"hljs-string\">'upper left'</span>)\nplt.tight_layout()\nplt.grid(<span class=\"hljs-literal\">True</span>)\nplt.show()\n</code></pre>\n<p><strong>This is what it looked like:</strong></p>\n<p><img src=\"/images/upload/owasp-chart.png\" alt=\"\"></p>\n<p>Here’s a table with all the data:</p>\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<table><thead><tr><th></th><th>A01: Broken Access Control</th><th>A02:\u000b Cryptographic Failures</th><th>A03: \u000bInjection</th><th>A04: Insecure Design</th><th>A05:\u000b Security Misconfiguration</th><th>A06: Vulnerable &#x26; Outdated Components</th><th>A07: Identification &#x26; Authentication Failures</th><th>A08: Software &#x26; Data Integrity Failures</th></tr></thead><tbody><tr><td><strong>2022</strong></td><td>4004</td><td>370</td><td>6496</td><td>1217</td><td>151</td><td>1</td><td>1233</td><td>334</td></tr><tr><td><strong>2023</strong></td><td>5498</td><td>411</td><td>8846</td><td>1480</td><td>178</td><td>1</td><td>1357</td><td>468</td></tr><tr><td><strong>2024</strong></td><td>7182</td><td>447</td><td>13280</td><td>1922</td><td>163</td><td>4</td><td>1430</td><td>584</td></tr><tr><td><strong>2025</strong></td><td>4314</td><td>209</td><td>7563</td><td>1056</td><td>90</td><td>2</td><td>774</td><td>418</td></tr><tr><td><strong>Totals</strong></td><td>20998</td><td>1437</td><td>36185</td><td>5675</td><td>582</td><td>8</td><td>4794</td><td>1804</td></tr></tbody></table>\n<p>So looking at purely the number of incidences in CVEs, the Top 5 would look like this:</p>\n<p>#5 Software and Data Integrity Failures<br>\n#4 Identification &#x26; Authentication Failures<br>\n#3 Insecure Design<br>\n#2 Broken Access Control<br>\n#1 Injection</p>\n<p>But wait, <a href=\"https://www.owasptopten.org/thedata\">OWASP’s methodology</a> in compiling the list involves not just the frequency (how common) but the severity or impact of each weakness. Also, 2 out of the 10 in the list are chosen from a community survey among application security professionals, to compensate for the gaps in public data. In the past OWASP has also merged categories to form a new category. So based on that here’s my prediction for the Top 5</p>\n<h3 id=\"user-content-prediction-time\"><strong>Prediction Time</strong></h3>\n<p>There’s absolutely no doubt in my mind that the security implications of AI will have a big impact on the list. One point of note is that OWASP released a <a href=\"https://genai.owasp.org/resource/owasp-top-10-for-llm-applications-2025/\">Top 10 list of LLM</a> in November 2024. Whether they decided to keep the two lists separate or have overlap will largely determine the Top 10 this year.</p>\n<p>So looking at the CVE data above (<strong>Broken Access Control</strong> and <strong>Injection</strong> had the most occurrences), and the rise of AI in production, here’s what I think will be the Top 5 in the OWASP list this year:</p>\n<p>#5 Software and Data Integrity Failures<br>\n#4 Security Misconfigurations<br>\n#3 Insecure Design<br>\n#2 Injection<br>\n#1 Broken Access Control</p>\n<p>With enterprises implementing AI Agents, RAG Pipelines and Model Context Protocol (MCP) in production, access control becomes a priority. Broken Access Control topped the list in 2021, and we’ve seen a slew of high profile data breaches recently so I think it will sit atop the list this year as well.</p>\n<p>I asked Jake Moshenko, CEO of AuthZed about his Predictions for the list and while we agreed on the #1 position on the list, there were also a couple of things where we disagreed. Watch the video to find out what Jake thought the Top 5 would look like and which category he thinks might drop out of the Top 10 altogether.</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/YA-F4qnavE8\" title=\"YouTube video player\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"></iframe>\n<hr>\n<h3 id=\"user-content-caveats\"><strong>Caveats</strong></h3>\n<p>As I mentioned before, I’m not a data scientist so please feel free to improve upon this methodology in the <a href=\"https://github.com/sohanmaheshwar/cve-owasp-mapping\">Github Repo</a>. I also need to state that:</p>\n<ul>\n<li>CVE data doesn’t represent all real-world vulnerabilities (e.g., business logic flaws are underreported). Also, vulnerabilities not related to web apps (eg: buffer overflow) were not considered.</li>\n<li>This approach only looks at vulnerability frequency, not impact or exploitability which are factors that OWASP also considers.</li>\n<li>OWASP’s real methodology includes community surveys, and telemetry from industry partners - which wasn’t part of this experiment.</li>\n</ul>\n<hr>\n<h3 id=\"user-content-your-turn\"><strong>Your Turn</strong></h3>\n<p>What do you think the 2025 OWASP Top 10 will look like?<br>\nDo you agree with these trends, or do you think another category will spike?<br>\nI’d love to hear your thoughts in the comments on <a href=\"https://www.linkedin.com/company/authzed/\">LinkedIn</a>, <a href=\"https://bsky.app/profile/authzed.com\">BlueSky</a> or <a href=\"https://x.com/authzed\">Twitter</a></p>\n<p>If you want to replicate this yourself, I’ve put the dataset links and <a href=\"https://github.com/sohanmaheshwar/cve-owasp-mapping\">code snippets on GitHub</a>.</p>",
            "url": "https://authzed.com/blog/predicting-the-latest-owasp-top-10-with-cve-data",
            "title": "Predicting the latest OWASP Top 10 with CVE data ",
            "summary": "OWASP is set to release their first Top 10 update since 2021, and this year’s list is one of the most awaited because of the generational shift that is AI. The security landscape has fundamentally shifted thanks to AI being embedded in production systems across enterprises from RAG pipelines to autonomous agents. I thought it would be a fun little exercise to look at CVE data from 2022-2025 and make predictions on what the top 5 in the updates list would look like. Read on to find out what I found.",
            "image": "https://authzed.com/images/blogs/authzed-predict-owasp.png",
            "date_modified": "2025-08-13T18:50:00.000Z",
            "date_published": "2025-08-13T18:50:00.000Z",
            "author": {
                "name": "Sohan Maheshwar",
                "url": "https://www.linkedin.com/in/sohanmaheshwar/"
            }
        },
        {
            "id": "https://authzed.com/blog/prevent-ai-agents-from-accessing-unauthorized-data",
            "content_html": "<p>I just attended the Secure Minds Summit in Las Vegas, where security and application development experts shared lessons learned from applying AI in their fields. Being adjacent to Black Hat 2025, it's not surprising that a common theme was the security risks of AI agents and MCP (Model Context Protocol). There's an anxious excitement in the community about AI's potential to revolutionize how organizations operate through faster, smarter decision-making, while grappling with the challenge of doing it securely.</p>\n<h2 id=\"user-content-why-permissions-matter-in-the-age-of-ai\">Why Permissions Matter in the Age of AI</h2>\n<p>As organizations explore AI agent deployment, one thing is clear: neither employees nor AI agents should have access to all data. You wouldn't want a marketing AI agent accessing raw payroll data, just as you wouldn't want an HR agent viewing confidential product roadmaps. Without proper access controls, AI agents can create chaos just as easily as they deliver value, since they don't inherently understand which data they should or shouldn't access.</p>\n<p>This is where robust permissions systems become critical. Proper access controls ensure AI agents operate within organizational policy boundaries, accessing only data they're explicitly authorized to use.</p>\n<h2 id=\"user-content-watch-how-to-implement-access-controls-for-ai-agents\">Watch: How to Implement Access Controls for AI Agents</h2>\n<p>Sohan, our Lead Developer Advocate at AuthZed, recently explored this topic on the <a href=\"https://www.youtube.com/@authzed\">AuthZed YouTube channel</a> with a live demo of implementing AI-aware permissions systems.</p>\n<p>Watch the demo here:</p>\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/UjKa5T1dIVw?si=wgiJKeDVEef8-pb0\" title=\"YouTube video player\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"></iframe>\n<h2 id=\"user-content-authorization-infrastructure-for-ai-built-for-scale-and-safety\">Authorization Infrastructure for AI: Built for Scale and Safety</h2>\n<p>In June, we launched AuthZed's <a href=\"https://authzed.com/ai-authorization\">Authorization Infrastructure for AI</a>, purpose-built to ensure AI systems respect permissions, prevent data leaks, and maintain comprehensive audit trails.</p>\n<p>AuthZed's infrastructure is powered by <a href=\"https://authzed.com/spicedb\">SpiceDB</a>, our open-source project based on Google's Zanzibar. SpiceDB's scale and speed make it an ideal authorization solution for supporting AI's demanding performance requirements.</p>\n<p>Our infrastructure delivers:</p>\n<ul>\n<li><strong>Billions</strong> of access control lists (ACLs)</li>\n<li><strong>Millions</strong> of authorization checks per second</li>\n<li><strong>Global</strong> replication across data centers</li>\n</ul>\n<p>Want to learn more about the future of AuthZed and authorization infrastructure for AI? Join us on August 20th for \"AuthZed is 5: The Authorization Infrastructure Event.\" <a href=\"https://authzed.com/authzed-is-5\">Register here</a>.</p>",
            "url": "https://authzed.com/blog/prevent-ai-agents-from-accessing-unauthorized-data",
            "title": "Prevent AI Agents from Accessing Unauthorized Data",
            "summary": "AI agents promise to revolutionize enterprise operations, but without proper access controls, they risk exposing sensitive data to unauthorized users. Learn how AuthZed's Authorization Infrastructure for AI prevents data leaks while supporting millions of authorization checks per second. Watch our live demo on implementing AI-aware permissions systems.\n\n",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-08-08T15:46:00.000Z",
            "date_published": "2025-08-08T15:46:00.000Z",
            "author": {
                "name": "Sam Kim",
                "url": "https://github.com/samkim"
            }
        },
        {
            "id": "https://authzed.com/blog/authzed-is-5-authorization-infrastructure-event",
            "content_html": "<h2 id=\"user-content-the-authorization-infrastructure-event\">The Authorization Infrastructure Event</h2>\n<p>AuthZed is turning five years old, and we're throwing a celebration! On Wednesday, August 20th, we're hosting \"The Authorization Infrastructure Event\" by bringing together experts in authorization and database technology to talk about where this space is headed.</p>\n<p><strong><a href=\"https://authzed.com/authzed-is-5\">RSVP Here</a></strong></p>\n<h2 id=\"user-content-what-we-have-planned\">What We Have Planned</h2>\n<p>You'll hear from industry experts who've been shaping how we think about authorization:</p>\n<ul>\n<li>Artie Shevchenko, Software Engineer at Canva</li>\n<li>Andy Pavlo, Professor, Databaseology, Carnegie Mellon University</li>\n<li>Andre Sanches, Software Engineer, Turo</li>\n</ul>\n<p>And the AuthZed team will be sharing what we've been building—new product announcements, plus a peek into our lab:</p>\n<ul>\n<li>Jake Moshenko, CEO of AuthZed</li>\n<li>Irit Goihman, VP of Engineering at AuthZed</li>\n<li>Sohan Maheshwar, Developer Advocate at AuthZed</li>\n</ul>\n<p>We’ll be announcing new products that I think will genuinely change how people approach authorization infrastructure and I’m particularly excited to finally share about what we've been exploring in our lab, experimental work that could shape the future of access control.</p>\n<h2 id=\"user-content-a-personal-reflection\">A Personal Reflection</h2>\n<p>It's hard to believe but five years have gone by so fast. Back when I joined Jake, Jimmy, and Joey as the first employee, they had this clear understanding of why application authorization was such a pain point for developers, the <a href=\"https://authzed.com/z/google-zanzibar-annotated-paper\">Google Zanzibar</a> paper as their guide, and an ambitious vision: bring better authorization infrastructure to everyone who needed it.</p>\n<p><img src=\"/images/upload/authzed-offsite-1.jpg\" alt=\"\" title=\"AuthZed Offsite #1\"></p>\n<p><em>Photo from our first team offsite in 2021. Not pictured: me because I'm taking the photo</em></p>\n<p>Looking back at our journey, some moments that stand out:</p>\n<ul>\n<li><strong><a href=\"https://authzed.com/blog/introducing-authzed\">Our very first blog post</a></strong> introducing AuthZed to the world</li>\n<li><strong><a href=\"https://authzed.com/blog/spicedb-is-open-source-zanzibar\">Open sourcing SpiceDB</a></strong> - making Zanzibar-style authorization accessible to everyone</li>\n<li><strong><a href=\"https://authzed.com/zanzibar\">Publishing our annotated Zanzibar paper</a></strong> to help developers understand the concepts that drive our approach</li>\n<li><strong><a href=\"https://authzed.com/blog/a-closer-look-at-authzed-dedicated\">Launching AuthZed Dedicated</a></strong> for teams that needed enterprise-grade infrastructure</li>\n</ul>\n<p>We've grown from that small founding team to a group of people who genuinely care about solving authorization the right way. Along the way, we've had the privilege of helping everyone from early-stage startups to large enterprises build and scale their applications without the usual authorization headaches.</p>\n<h2 id=\"user-content-join-us\">Join Us</h2>\n<p>This event is our chance to share our latest work with the community that's supported us, celebrate how far we've all come together, and get a glimpse of what's ahead.</p>\n<p>Whether you've been following our journey from the beginning or you're just discovering what we're about, we'd love to have you there. It's going to be the kind of event where you leave with new ideas, maybe some useful insights, and definitely a better sense of where authorization infrastructure is headed.</p>\n<p><strong>Want to share a birthday message with us?</strong> <a href=\"https://app.memento.com/authzed-is-5/1azYJMsmXa/record\">Record a short message here</a>—we'd genuinely love to hear from you and share some of them during the event.</p>\n<p>See you on August 20th!</p>",
            "url": "https://authzed.com/blog/authzed-is-5-authorization-infrastructure-event",
            "title": "Celebrate With Us: AuthZed is 5!",
            "summary": "AuthZed is turning five years old! Join us Wednesday, August 20th for our Authorization Infrastructure Event, where we're bringing together industry experts and sharing exciting new product developments plus experimental work from our lab.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-07-23T09:36:00.000Z",
            "date_published": "2025-07-23T09:36:00.000Z",
            "author": {
                "name": "Sam Kim",
                "url": "https://github.com/samkim"
            }
        },
        {
            "id": "https://authzed.com/blog/coding-with-ai-my-personal-experience",
            "content_html": "<p>I’ve been in tech for over 20 years. I’ve written production code in everything from Fortran to Go, and for the last five of those years, I’ve been a startup founder and CEO. These days, I spend most of my time operating the business, not writing code. But recently, I dipped back in. I needed a new demo built, and fast.</p>\n<p>It wasn’t a simple side project. This demo would ideally have multiple applications, all wired into SpiceDB, built with an obscure UI framework, and designed to show off what a real-world, multi-language, permission-aware system looks like. Naturally, I started thinking about who should build it.</p>\n<p>Should I ask engineering? Probably not a good idea since I didn’t want to interrupt core product work. What about an intern? Too late in the year for that. Maybe a contractor? I’ve had mixed results there. Skills tend to be oversold, results can fall short, and just finding and vetting someone would take time I didn’t have.</p>\n<p>Just prior to this, Anthropic had just released Claude Code and Claude 4. A teammate (with good taste) had good things to say about the development experience, and internet consensus seems to be that (for today at least) Claude is kind for coding models, so I figured I’d give it a try. I’m no novice to working with AI: I have been a paying customer of OpenAI’s since Dall-E and ChatGPT had their first public launches. At AuthZed we also make extensive use of the AI features that are built into some of our most beloved tools, such as: Notion, Zoom, Figma, and GitHub. Many of these features have been helpful, but none felt like a game changer.</p>\n<h2 id=\"user-content-getting-started\">Getting Started</h2>\n<p>At first, I wasn’t sure how much Claude Code could take on. I didn’t know how to structure my prompts or how detailed I needed to be. I started small: scaffold a project, get a “hello world” working, and set up the build system. It handled all of that cleanly.</p>\n<p>Encouraged, I got a little overconfident. My prompts grew larger and fuzzier. The quality of output dropped quickly. I also didn’t have a source control strategy in place, and when Claude Code wandered off track, I lost a lot of work. It’s fantastically bad at undoing what it just did! It was a painful but valuable learning experience.</p>\n<h2 id=\"user-content-figuring-out-a-process\">Figuring Out a Process</h2>\n<p>Eventually, I found my rhythm. I started treating Claude Code like a highly capable but inexperienced intern. I wrote prompts as if they were JIRA tickets: specific, structured, and assuming zero context. I broke the work down into small, clear deliverables. I committed complete features as I went. When something didn’t feel right, I aborted early, git reverted, and started fresh.</p>\n<p><img src=\"/images/upload/coding-with-ai-1.png\" alt=\"\"></p>\n<p>That approach worked really well.</p>\n<p><img src=\"/images/upload/coding-with-ai-3.png\" alt=\"\"></p>\n<p><img src=\"/images/upload/coding-with-ai-2.png\" alt=\"\"></p>\n<p>By the end of the project, Claude Code and I had built three application analogues for tools that exist in the Google Workspace suite, in three different languages! We wrote a Docs-like in Java, a Groups-like in Go, and a Gmail-like in Javascript, and a frontend coded up in a wacky wireframe widget library called <a href=\"https://wiredjs.com\">Wired Elements</a>. Each one was connected through SpiceDB, shared a unified view of group relationships, and included features like email permission checks and a share dialog in the documents app. It all ran in Docker with a single command. The entire effort cost me around $75 in API usage.</p>\n<p>Check it out for yourself: <a href=\"https://github.com/authzed/multi-app-demo\">https://github.com/authzed/multi-app-demo</a></p>\n<p>Could I have done this on my own? Sure, in theory. But I’m not a UI expert, and switching between backend languages would have eaten a lot of time. If I’d gone the solo route, I would’ve likely over-engineered the architecture to minimize how much code I had to write, which might have resulted in something more maintainable, but also something unfinished and way late.</p>\n<h2 id=\"user-content-phenomenal-cosmic-power\">Phenomenal Cosmic Power</h2>\n<p><img src=\"/images/upload/coding-with-ai-4.png\" alt=\"\"></p>\n<p>This was a different experience than I’d had with GitHub Copilot. Sometimes people describe Copilot as “spicy autocomplete”, and that feels apt. Claude Code felt like having a pair programmer who could actually build features with me.</p>\n<p>My buddy Jason Hall from Chainguard <a href=\"https://www.linkedin.com/posts/imjasonh_ai-coding-agents-are-like-giving-everyone-activity-7338321865783869440-07AL\">put it best</a> in a post on LinkedIn: “AI coding agents are like giving everyone their own mech suit.” and “...if someone drops one off in my driveway I'm going to find a way to use it.”</p>\n<p><img src=\"/images/upload/coding-with-ai-5.png\" alt=\"\"></p>\n<p>For the first time in a long while, I felt like I could create again. As a CEO, that felt energizing. It also made me start wondering what else I could personally accelerate.</p>\n<p>Of course, I had some doubts. Maybe this only worked because it was greenfield. Maybe I’d regret not being the expert on the codebase. But the feeling of empowerment was real.</p>\n<h2 id=\"user-content-the-next-project-crm-migration\">The Next Project: CRM Migration</h2>\n<p>At the same time, we had a growing need to migrate our sales CRM. We’d built a bespoke system in Notion, modeled loosely after Salesforce. Meanwhile, all of our marketing data already lived in HubSpot. It was time to unify everything.</p>\n<p>On paper, this looked straightforward: export from Notion, import into HubSpot. In reality, it was anything but. Traditional CRM migrations are done with flattened CSV files; that wouldn’t play nicely with the highly relational structure we’d built. And with so much existing marketing data in HubSpot, this was more of a merge than a migration.</p>\n<p>I’ve been through enough migrations to know better than to try a one-shot cutover. It never goes right the first time, and data is always messier than expected. So I came up with a different plan: build a continuous sync tool.</p>\n<p>The idea was to keep both systems aligned while we gradually refined the data. That gave us time to validate everything and flip the switch only when we were ready. Both Notion and HubSpot have rich APIs, so I turned again to Claude Code.</p>\n<h2 id=\"user-content-the-results\">The Results</h2>\n<p>Over the course of a week, Claude Code and I wrote about 5,000 lines of JavaScript. The tool matched Notion records to HubSpot objects using a mix of exact matching and fuzzy heuristics. We used <a href=\"https://en.wikipedia.org/wiki/Levenshtein_distance\">Levenshtein distance</a> to help with tricky matches caused by accented names or alternate spellings. The tool handled property synchronization and all the API interactions needed to link objects across systems.</p>\n<p>The cost came in at around $50 in Claude Code credits.</p>\n<p>Could I have done it myself? Technically, yes. But it would have taken me a lot longer. I’m not fluent in JavaScript, and if I had been writing by hand, I would’ve insisted on TypeScript and clean abstractions. That would have been a waste of time for something we were planning to throw away after the migration.</p>\n<h2 id=\"user-content-my-take-on-ai-coding\">My Take on AI Coding</h2>\n<p>Our current generation of coding agents are undeniably powerful. Yes, they’re technically still just a next-token predictor, but that description misses the point. It’s like saying <a href=\"https://www.youtube.com/watch?v=azEvfD4C6ow\">Bagger 288</a> is “just a big shovel.” Sure, but it’s a shovel that can eat mountains.</p>\n<p>I now feel confident taking on software projects again in my limited spare time. That’s not something I expected to feel again as a full-time CEO. And the most exciting part? This is probably the worst that these tools will ever be. From here, the tools only get better. Companies like OpenAI, with Codex, and Superblocks are already riffing on other possible user experiences for coding agents. I’m keen to see where the industry goes.</p>\n<p>It also seems clear that AI will play a bigger and bigger role in how code gets written. As an API provider, we’re going to need to design for that reality. In the not-too-distant future, our primary users will likely be coding agents, not just humans.</p>\n<h2 id=\"user-content-looking-ahead\">Looking Ahead</h2>\n<p>We’re in the middle of a huge transformation, not just in software, but across the broader economy. The genie is out of the bottle. Even if the tools stopped improving tomorrow (and I don’t think they will) there’s already enough capability to change the way software gets built.</p>\n<p>I’ll admit, it’s a little bittersweet. For most of my career, I have self-identified as a computer whisperer: someone who can speak just the right incantations to make computers (or sometimes whole datacenters) do what I need. But like most workplace superpowers, this one also turned out to be a time-limited arbitrage opportunity.</p>\n<p>What hasn’t changed is the need for control. As AI gets more capable, the need for clear, enforceable boundaries becomes more important than ever. The answer to “what should this AI be allowed to do?” isn’t “more AI.” It’s strong, principled authorization.</p>\n<p>That’s exactly what we’re building at AuthZed. And you’ll be seeing more from us soon about how we’re thinking about AI-first developer experience and AI-native authorization.</p>\n<p>Stay tuned.</p>",
            "url": "https://authzed.com/blog/coding-with-ai-my-personal-experience",
            "title": "Coding with AI: My Personal Experience",
            "summary": "AuthZed CEO Jake Moshenko shares his experience coding with AI.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-07-16T08:21:00.000Z",
            "date_published": "2025-07-16T08:21:00.000Z",
            "author": {
                "name": "Jake Moshenko",
                "url": "https://www.linkedin.com/in/jacob-moshenko-381161b/"
            }
        },
        {
            "id": "https://authzed.com/blog/authzed-cloud-is-coming-soon",
            "content_html": "<p>Here at AuthZed, we are counting down the days until we launch AuthZed Cloud because we are so eager to bring the power of our authorization infrastructure to every company, large and small. If you're just as excited as we are about AuthZed Cloud, sign up for the waitlist. We will be in touch with AuthZed Cloud news, and you'll be the first to know when the product launches.</p>\n<div class=\"text-center\">\n  <a href=\"https://authzed.com/z/authzed-cloud-waitlist\">\n   Sign up for the AuthZed Cloud Waitlist\n  </a>\n</div>\n<h2 id=\"user-content-from-enterprise-focus-to-self-service\">From Enterprise Focus to Self-Service</h2>\n<p><img src=\"/images/upload/public-dedicated-private-cloud-timeline-2x.png\" alt=\"\"></p>\n<p>From the start of our journey, we have had a strong focus on serving the needs of authorization at enterprise businesses. Our most popular product, <a href=\"https://authzed.com/products/authzed-dedicated\">AuthZed Dedicated</a>, is a reflection of that focus as it caters to those looking for dedicated hardware resources and fully-isolated deployment environments. However, not everyone has such strict requirements, and there are many companies who prefer a self-service product where they can sign up, manage their deployments from a single, shared control plane with other users, and pay for dynamic usage with a credit card. The latter is how we consumed most of our high-value services at our last startup when we were building the first enterprise container registry: Quay.io. In fact, you can read more about our journey from Quay to AuthZed <a href=\"https://authzed.com/blog/introducing-authzed\">here</a>.</p>\n<h2 id=\"user-content-setting-new-standards-for-authorization-security\">Setting New Standards for Authorization Security</h2>\n<p>The most gratifying part of creating AuthZed has been working alongside so many amazing companies that are changing the landscape of various industries. It's truly validating to see them come to the same conclusion: homegrown authorization solutions are not sufficient for modern businesses. With AuthZed Cloud, we expect to expand the number of companies we can work alongside to set a new standard of security that ensures the safety of all of our private data by fixing access control.</p>",
            "url": "https://authzed.com/blog/authzed-cloud-is-coming-soon",
            "title": "AuthZed Cloud is Coming Soon",
            "summary": "AuthZed Cloud is coming soon, expanding beyond enterprise-only solutions to offer self-service authorization infrastructure for companies of all sizes. Join our waitlist to be first in line when we launch this game-changing platform.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-07-03T10:31:00.000Z",
            "date_published": "2025-07-03T10:31:00.000Z",
            "author": {
                "name": "Jimmy Zelinskie",
                "url": "https://twitter.com/jimmyzelinskie"
            }
        },
        {
            "id": "https://authzed.com/blog/authzed-brings-additional-observability-to-authorization-via-the-datadog-integration",
            "content_html": "<p>Today, AuthZed is providing additional observability capabilities to AuthZed's cloud products with the introduction of our official Datadog Integration. All critical infrastructure should be observable and authorization is no exception. Our integration with Datadog gives engineering teams instant insight into authorization performance, latency, and anomalies—without adding custom tooling or overhead.</p>\n<p>With this new integration, customers can now centralize that observability data with the rest of their data in Datadog—giving them the ability to correlate events across their entire platform. AuthZed's cloud products continue to include a web console with out-of-the-box dashboards containing metrics across the various infrastructure components that power a permissions system. At the same time, users of the Datadog integration will also have a mirror of these dashboards available in Datadog if they do not wish to create their own.</p>\n<p><img src=\"/images/upload/post-datadog-image1.png\" alt=\"Authzed Dedicated Datadog Dashboard UI\"></p>\n<p>\"Being able to visualize how AuthZed performs alongside our other systems gives us real peace of mind,\" said Eric Zaporzan, Director of Infrastructure, at Neo Financial. \"Since we already use Datadog, it was simple to send AuthZed metrics there and gain a unified view of our entire stack.\"</p>\n<p>AuthZed metrics allow developers and SREs to monitor their deployments, including request latency, cache metrics (such as size and hit/miss rates), and datastore connection and query performance. These metrics help diagnose performance issues and fine-tune the performance of their SpiceDB clusters.</p>\n<h3 id=\"user-content-get-started-using-datadog-with-authzed-in-7-steps\">Get Started Using Datadog with AuthZed in 7 Steps</h3>\n<p>The Datadog integration is available in the AuthZed Dashboard under the “Settings” tab on a Permission System.</p>\n<ol>\n<li>Go to the dashboard homepage.</li>\n<li>Select a Permission System for which to submit metrics.</li>\n<li>Click on the Settings tab.</li>\n<li>Scroll down to the Datadog Metrics block of the settings UI.</li>\n<li>Enter your Datadog account API key.</li>\n<li>Enter your Datadog site if different from the default.</li>\n<li>Click Save.</li>\n</ol>\n<p>To ensure that the dashboard graph for latency correctly shows the p50, p95, and p99 latencies, you’ll also need to set the Percentiles setting for the authzed.grpc.server_handling metric in the Metrics Summary view to ON.</p>\n<p>TADA 🎉 You should see metrics start to flow to Datadog shortly thereafter.</p>\n<p>I want to thank all of the AuthZed engineers involved in shipping this feature, but especially Tanner Stirrat who shepherded this project from inception and I can't wait to see all the custom dashboards our customers make in the future!<br>\n<br>\nInterested in learning more? Join our Office Hours on July 3rd here on YouTube.</p>\n<p><lite-youtube videoid=\"OZBJw-OsHfg\" playlabel=\"Add Observability to Your Authorization via Datadog\" params=\"start=1&#x26;modestbranding=2&#x26;rel=0&#x26;enablejsapi=1\"></lite-youtube></p>",
            "url": "https://authzed.com/blog/authzed-brings-additional-observability-to-authorization-via-the-datadog-integration",
            "title": "AuthZed Brings Additional Observability to Authorization via the Datadog Integration",
            "summary": "Gain instant visibility into your authorization layer with AuthZed’s new Datadog integration. Stream SpiceDB metrics—latency, cache efficiency, datastore performance, and more—into the dashboards you already trust, so you can correlate events across your stack and troubleshoot faster without extra tooling.",
            "image": "https://authzed.com/images/blogs/blog-eng-datadog-integration-hero-2x.png",
            "date_modified": "2025-06-24T08:00:00.000Z",
            "date_published": "2025-06-24T08:00:00.000Z",
            "author": {
                "name": "Jimmy Zelinskie",
                "url": "https://twitter.com/jimmyzelinskie"
            }
        },
        {
            "id": "https://authzed.com/blog/authzed-announces-support-for-ai-by-providing-permissions-aware-ai",
            "content_html": "<h2 id=\"user-content-introducing-authorization-infrastructure-for-ai\">Introducing Authorization Infrastructure for AI</h2>\n<p><em>Secure your AI systems with fine-grained authorization for RAG pipelines and agents</em></p>\n<p>Today we are announcing <a href=\"https://authzed.com/ai-authorization\">Authorization Infrastructure for AI</a>, providing official support for Retrieval-Augmented Generation (RAG) pipelines and agentic AI systems. With this launch, teams building AI into their applications, developing AI products or building an AI company can enforce fine-grained permissions across every stage - from document ingestion to vector search to agent behavior - ensuring data is protected, actions are authorized, and compliance is maintained.</p>\n<p>AI is quickly becoming a first-class feature in modern applications. From retrieval-augmented search to autonomous agents, engineering teams are building smarter user experiences by integrating large language models (LLMs) into their platforms.</p>\n<p>But with that intelligence comes risk.</p>\n<p>AI systems do not just interact with public endpoints. They pull data from sensitive internal systems, reason over embeddings that bypass traditional filters, and trigger actions on behalf of users. Without strong access control, they can expose customer records, cross tenant boundaries, or operate with more agency than intended.</p>\n<p>This is the authorization problem for AI. And it is one every team building with LLMs now faces.</p>\n<h2 id=\"user-content-authorization-for-ai-is-not-optional\">Authorization for AI is not optional</h2>\n<p>When you add AI to your application, you also expand your attack surface. Consider just a few examples:</p>\n<ul>\n<li>An LLM that retrieves documents from internal systems but fails to check who is asking</li>\n<li>An agent that books travel but can also access payroll data</li>\n<li>A vector store filled with sensitive documents, exposed through approximate search</li>\n</ul>\n<p>According to the OWASP Top 10 for LLM Applications, four of the top risks require robust authorization controls as a primary mitigation. And yet, most developers are still relying on brittle, manual enforcement scattered across their codebases.</p>\n<p>We believe it’s time for a better solution.</p>\n<p><img src=\"/images/blogs/authzed-announces-support-for-ai-by-providing-permissions-aware-ai/permissions-aware-ai.png\" alt=\"AuthZed Authorization Infrastructure for AI\" title=\"Authorization Infrastructure for AI\"></p>\n<h2 id=\"user-content-meet-authzeds-authorization-infrastructure-for-ai\">Meet AuthZed's Authorization Infrastructure for AI</h2>\n<p>AuthZed’s authorization infrastructure for AI brings enterprise-grade permission systems to AI workloads. AuthZed has been better positioned to support AI from the get-go because of SpiceDB.</p>\n<p><a href=\"https://authzed.com/spicedb\">SpiceDB</a> is an open-source Google Zanzibar-inspired database for storing and computing permissions data that companies use to build global-scale fine grained authorization services. Since it is based on Google Zanzibar’s proven architecture, it can scale to massive datasets while handling complex permissions queries. In fact SpiceDB can scale to trillions of access control lists and millions of authorization checks per second.</p>\n<p><em>“AI systems are only as trustworthy as the infrastructure that governs them,\" said Janakiram MSV, industry analyst of Janakiram &#x26; Associates. \"AuthZed’s SpiceDB brings proven, cloud-native authorization principles to AI, delivering the control enterprises need to adopt AI safely and at scale.”</em></p>\n<p>Using SpiceDB to enforce access policies at every step of your AI pipeline ensures that data and actions remain properly governed. With AuthZed’s Authorization Infrastructure for AI, teams can safely scale their AI features without introducing security risks or violating data boundaries.</p>\n<h2 id=\"user-content-securing-rag-pipelines-with-fine-grained-access-control\">Securing RAG pipelines with fine-grained access control</h2>\n<p>Retrieval-Augmented Generation improves the usefulness of LLMs by injecting external knowledge. But when that knowledge includes sensitive customer or corporate data, access rules must be enforced at every stage.</p>\n<p>AuthZed enables teams to:</p>\n<ul>\n<li>Pre-filter content before generating embeddings</li>\n<li>Post-filter vector search results to remove unauthorized documents</li>\n<li>Maintain real-time permission syncs with systems like Google Workspace or SharePoint</li>\n<li>Build permission-aware retrieval layers that balance relevance with compliance</li>\n</ul>\n<p>Whether you are building with a private knowledge base, CRM data, or support logs, SpiceDB ensures your AI respects the same access controls as the rest of your systems.</p>\n<h2 id=\"user-content-governing-agent-behavior-with-clear-permission-boundaries\">Governing agent behavior with clear permission boundaries</h2>\n<p>AI agents are designed to act autonomously, but autonomy without boundaries is dangerous. With the <strong>AuthZed Agentic AI Authorization Model</strong>, teams can enforce clear limits on what agents can access and do.</p>\n<p>This model includes:</p>\n<ul>\n<li>Functionality Control: Define and restrict which tools an agent can use</li>\n<li>Permissions Management: Apply inherited user permissions to agent behavior</li>\n<li>Autonomy Oversight: Introduce approvals for high-impact actions and maintain full audit logs</li>\n</ul>\n<p>Whether your agent is summarizing data, booking a meeting, or triggering a workflow, it should only ever do what it is explicitly allowed to do.</p>\n<h2 id=\"user-content-what-this-looks-like-in-practice\">What this looks like in practice</h2>\n<p>Let’s say an employee types a natural language query into your internal AI assistant:</p>\n<p>“What was our Q3 revenue?”</p>\n<p>Without authorization, the assistant might retrieve sensitive board slides or budget drafts and present them directly to the user. No checks, no logs, no traceability.</p>\n<p>With AuthZed:</p>\n<ul>\n<li>The system checks the employee’s permissions</li>\n<li>Only authorized financial data is retrieved</li>\n<li>An audit log is created for compliance</li>\n<li>AI operates with the same access controls as the rest of the application</li>\n</ul>\n<p>This is what AuthZed’s Authorization Infrastructure for AI makes possible.</p>\n<h2 id=\"user-content-built-for-builders\">Built for builders</h2>\n<p>You should not have to choose between building smart features and maintaining secure boundaries. With AuthZed:</p>\n<ul>\n<li>Authorization integrates into your AI stack in minutes, not months</li>\n<li>SpiceDB scales with your users, tenants, and access models</li>\n<li>RAG and agent systems become extensions of your existing permission architecture</li>\n</ul>\n<p>And it is already being used in production. <a href=\"https://authzed.com/customers/workday\">Workday uses AuthZed Dedicated</a> to\nsecure its AI-driven contract lifecycle platform. Other major AI providers rely on SpiceDB to enforce permissions across\nmulti-tenant LLM infrastructure.</p>\n<h2 id=\"user-content-get-started-quickly\">Get started quickly</h2>\n<p>If you are building AI features, AuthZed’s Authorization Infrastructure for AI helps you ship faster by allowing you to focus on your product, instead of cobbling together an authorization solution. Whether you are securing vector search, gating agent behavior, or building out internal tools, AuthZed provides the authorization infrastructure you need.</p>\n<ul>\n<li>Check out the AuthZed [Authorization Infrastrructure for AI solutions page]</li>\n<li>Try the open-source <a href=\"https://github.com/authzed/workshops/tree/main/secure-rag-pipelines\">RAG security workshop on GitHub</a></li>\n<li>Try the open-srounce <a href=\"https://github.com/authzed/workshops/tree/main/ai-agent-authorization\">AI agent authorization workshop on GitHub</a></li>\n<li>Learn more from Sohan and Evan from our team about <a href=\"https://www.youtube.com/live/MuntroBcZ1s?feature=shared\">securing your RAG pipelines with fine grained authorization</a></li>\n<li><a href=\"https://authzed.com/call\">Book a demo</a> with our team</li>\n</ul>",
            "url": "https://authzed.com/blog/authzed-announces-support-for-ai-by-providing-permissions-aware-ai",
            "title": "Introducing Authorization Infrastructure for AI ",
            "summary": "AuthZed provides permissions systems that help secure and improve your RAG and agentic AI systems",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-06-13T09:00:00.000Z",
            "date_published": "2025-06-13T09:00:00.000Z",
            "author": {
                "name": "Sam Kim",
                "url": "https://github.com/samkim"
            }
        },
        {
            "id": "https://authzed.com/blog/introducing-the-authzed-cloud-api",
            "content_html": "<h2 id=\"user-content-infrastructure-for-authorization\">Infrastructure for Authorization</h2>\n<p>For the team at AuthZed, our mission is to fix access control. The first step is creating the foundational infrastructure for others to build their access control systems upon. Infrastructure for Authorization, you say? Didn't infrastructure just go through its largest transformation ever with cloud computing? From introduction to the eventual mass adoption of cloud computing, the industry has had to learn to manage all of the cloud resources they created. In response, cloud providers offered APIs for managing resource lifecycles. Our infrastructure follows this same pattern, so today we're proud to announce the <a href=\"https://www.postman.com/authzed/spicedb/collection/5fm402n/authzed-cloud-api\">AuthZed Cloud API</a> is in Tech Preview.</p>\n<h2 id=\"user-content-authzed-cloud-api\">AuthZed Cloud API</h2>\n<p>The AuthZed Cloud API is a RESTful JSON API for managing the infrastructure provisioned on AuthZed Dedicated Cloud. Today, it is able to list the available permissions systems and fully manage the configuration for restricting API-level access to SpiceDB within those permissions systems.</p>\n<p>As with all Tech Preview functionality, to get started, you must reach out to your account team and request access. Afterwards, you will be provided credentials for accessing the API. With these credentials, you're free to automate AuthZed Cloud infrastructure in any way you like! We recommend getting started by <a href=\"https://www.postman.com/authzed/spicedb/collection/5fm402n/authzed-cloud-api\">heading over to Postman</a> to explore the API. Next, why not break out a little bit of curl?</p>\n<p>Listing all of your permissions systems:</p>\n<pre><code class=\"hljs language-shell\">curl --location 'https://api.$YOUR_AUTHZED_DEDICATED_ENDPOINT/ps' \\\n     --header 'X-API-Version: 25r1' \\\n     --header 'Accept: application/json' \\\n     --header 'Authorization: Bearer $YOUR_CREDENTIALS_HERE' | jq .\u000b[​​{\n   \"id\": \"ps-8HXyWFOzGtk0Yq8dH0GBT\",\n   \"name\": \"example\",\n   \"systemType\": \"Production\",\n   \"systemState\": {\n     \"status\": \"RUNNING\"\n   },\n   \"version\": {\n     \"selectedChannel\": \"Rapid\",\n     \"currentVersion\": {\n       \"displayName\": \"SpiceDB 1.41.0\",\n       \"version\": \"v1.41.0+enterprise.v1\",\n       \"supportedFeatureNames\": [\n         \"FineGrainedAccessManagement\"\n       ]\n     }\n   }\n }]\n</code></pre>\n<p>Take note of the required headers: the API requires specifying a version as a header so that changes can be made to the API in the future releases.</p>\n<p>I'm eager to see all of the integrations our customers will build with API-level access to our cloud platform! Look out for another announcement coming <em>very</em> soon about an integration that we've built using this new API, too!</p>\n<p><em>Join us on the mission to fix access control.</em></p>\n<p><a href=\"https://authzed.com/call\">Schedule a call</a> with us to learn more about how AuthZed can help you.</p>",
            "url": "https://authzed.com/blog/introducing-the-authzed-cloud-api",
            "title": "Introducing The AuthZed Cloud API",
            "summary": "Announcing the AuthZed Cloud API in Tech Preview—an API for managing AuthZed Dedicated Cloud infrastructure. Following the cloud computing pattern of lifecycle management APIs, this new tool allows you to manage permissions systems and restrict API-level access to SpiceDB within your authorization infrastructure.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-05-28T12:00:00.000Z",
            "date_published": "2025-05-28T12:00:00.000Z",
            "author": {
                "name": "Jimmy Zelinskie",
                "url": "https://twitter.com/jimmyzelinskie"
            }
        },
        {
            "id": "https://authzed.com/blog/a-closer-look-at-authzed-dedicated",
            "content_html": "<p>At AuthZed, our mission is to fix broken access control. After years of suffering in industry from insufficient solutions for building authorization systems, we concluded that we'd have to start from the ground up by building the right infrastructure software. <a href=\"https://authzed.com/spicedb\">SpiceDB</a>, open sourced in late 2021, was our first-step to providing the solution that modern enterprises need. AuthZed Dedicated Cloud, often referred to as simply Dedicated, launched in early 2022 and productized SpiceDB by offering a dedicated cloud platform for provisioning SpiceDB deployments similar to the user experience you'd find provisioning infrastructure on a major cloud provider.</p>\n<p><img src=\"/images/upload/public-dedicated-private-cloud-timeline-2x.png\" alt=\"\"></p>\n<h2 id=\"user-content-what-are-dedicated-clouds\">What Are Dedicated Clouds?</h2>\n<p>Dedicated Clouds are a relatively new concept. When AWS hit the market, the term Public Cloud was coined; Public Clouds are cloud platforms that share their underlying hardware resources across a variety of customers. At the same time this term got coined, folks needed a term used to refer to what most folks were already doing before AWS launched: running their own dedicated infrastructure. Unfortunately, instead of calling this Dedicated Cloud, it became known as Private Cloud. So what are Dedicated Clouds? Well, they're the middle ground between Private and Public Clouds; Dedicated Clouds provide varying levels of isolation and dedicated resources than Public Clouds, but aren't placing end users fully in control quite like the traditional Private Cloud. Enterprises in regulated industries, or those that want to isolate particularly sensitive data,  increasingly reach for Dedicated Cloud because it can provide most of the niceties of the Public Cloud while also delivering better security.</p>\n<p><img src=\"/images/upload/public-dedicated-private-cloud-2x.png\" alt=\"\"></p>\n<h2 id=\"user-content-the-evolution-of-authzed-dedicated\">The Evolution of AuthZed Dedicated</h2>\n<p>When AuthZed looked to create the first commercial offering of SpiceDB, we looked at where the industry was heading and implemented a Serverless product. However, it turned out that most enterprises value peace of mind that comes from isolating their authorization data from a shared data plane with other tenants. This was a happy coincidence because at the same time we learned that the best way to operate low-latency systems is to isolate workloads by having dedicated hardware resources. With our new insights, we launched Dedicated, our \"middleground\" that provided dedicated cloud environments with reserved compute resources and private networking. Dedicated customers get a private control plane deployed into their cloud regions of choice where they can provision their own deployments using our web console, API, or Terraform/OpenTOFU. Remaining true to the Infrastructure-as-a-Service (IaaS) spirit, pricing is done on a resource consumption basis.</p>\n<h2 id=\"user-content-looking-ahead-to-authzed-cloud\">Looking Ahead to AuthZed Cloud</h2>\n<p>Since launch, Dedicated immediately became our flagship product. However, we recognized that some customers didn't require all of its isolation features.These are the same users looking for a self-service product to try things out without a long enterprise sales cycle. Our Serverless product inadvertently fits this description, but it's a limited experience compared to Dedicated. What if we could bridge the gap and bring a version of our Dedicated product where customers could share the control plane? We're calling this AuthZed Cloud (as opposed to AuthZed Dedicated Cloud) and it's under active development and expected to launch later this year. Best of all, because both Cloud and Dedicated will share the same codebase, all of the self-service features we're building will also be coming to Dedicated.</p>\n<p>If you are interested in learning more about AuthZed Cloud, you can <a href=\"https://authzed.com/z/authzed-cloud-waitlist\">sign up</a> here for the beta waitlist.</p>\n<p><a href=\"https://authzed.com/z/authzed-cloud-waitlist\"><img src=\"/images/upload/authzed-cloud-waitlist-preview.jpg\" alt=\"\" title=\"AuthZed Cloud Waitlist Preview\"></a></p>",
            "url": "https://authzed.com/blog/a-closer-look-at-authzed-dedicated",
            "title": "A Closer Look at AuthZed Dedicated",
            "summary": "AuthZed tackles broken access control through innovative authorization infrastructure. After launching open-source SpiceDB in 2021, they created AuthZed Dedicated Cloud—offering enterprises the security benefits of private clouds with public cloud convenience. This middle-ground solution provides isolated authorization data processing with dedicated resources, perfect for regulated industries requiring enhanced security.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-05-20T13:00:00.000Z",
            "date_published": "2025-05-20T13:00:00.000Z",
            "author": {
                "name": "Jimmy Zelinskie",
                "url": "https://twitter.com/jimmyzelinskie"
            }
        },
        {
            "id": "https://authzed.com/blog/building-better-authorization-infrastructure-with-arm",
            "content_html": "<p>How ARM helps AuthZed build and operate authorization infrastructure, from day-to-day productivity gains to cost-effective, performant cloud compute.</p>\n<h2 id=\"user-content-meeting-modern-development-challenges\">Meeting Modern Development Challenges</h2>\n<p>Today's cloud-native development environment requires running a growing list of simultaneous services: container orchestration, monitoring, databases, observability tools, and more. For engineering teams, this creates a critical challenge: how to balance performance, cost, and efficiency across both development environments and production deployments.</p>\n<p>At AuthZed, we provide flexible, scalable <strong>authorization infrastructure</strong>—the permissions systems that secure access for your applications’ data and functionality—enabling engineering teams to focus on building what matters—their core products. For our customers using AuthZed's dedicated cloud, the balance of performance, cost, and efficiency is also crucial—they expect a reliable, performant, and cost-effective solution.</p>\n<p>ARM architecture has become our strategic advantage in meeting these challenges across our entire workflow.</p>\n<h2 id=\"user-content-the-arm-advantage-for-development\">The ARM Advantage for Development</h2>\n<p>The availability of ARM-based laptops with customizable configurations and ample RAM has transformed our development environment. Our journey began with ARM processors in early 2022 and expanded to more powerful variants as they became available. The developer community quickly adopted these machines, and tooling and library support rapidly matured, enabling us to fully adopt ARM as our primary architecture in development.</p>\n<h3 id=\"user-content-developer-productivity-in-action\">Developer Productivity in Action</h3>\n<p>At AuthZed, we work with distributed systems and databases daily, and running the full stack locally can be resource-intensive, often requiring significant CPU and memory. ARM's efficient performance helps utilize machine capacity, while its energy efficiency keeps our laptops cool enough to truly stay on laps—even when running our resource-intensive local environment.</p>\n<p>After upgrading to higher-performance ARM-based laptops, notable improvements compared to our previous development environment included:</p>\n<ul>\n<li>27% decrease in average container image build times</li>\n<li>40% decrease in parallel build times for our application stack</li>\n<li>Ability to run our entire application stack locally, including supporting monitoring and observability services</li>\n</ul>\n<p>The qualitative benefits have been even more significant—true mobility with our laptops due to minimal battery drain and absence of overheating, smoother performance during resource-intensive tasks, and most importantly, tighter feedback loops during debugging and testing.</p>\n<h2 id=\"user-content-cicd-with-arm\">CI/CD with ARM</h2>\n<p>AuthZed has been building and publishing multi-architecture Docker images for our tools and authorization database for over three years (since March 2022), so we recognized the value of multi-architecture support in CI/CD early on.</p>\n<p>There's now robust support for third-party ARM-based action runners for GitHub Actions, our CI/CD platform. Combined with toolchain maturity across runner images for popular architectures, migration to ARM for CI/CD has never been easier.</p>\n<p>Build and test workflows are unique to each project and evolve as the project develops. Consequently, the benefits and tradeoffs for a CI/CD platform change over time. We've benefited from being able to easily migrate between architectures and runner providers to best meet our engineering needs at different stages.</p>\n<h2 id=\"user-content-powering-authzed-dedicated-with-arm\">Powering AuthZed Dedicated with ARM</h2>\n<p>Major providers like Google Cloud, AWS, and Azure have all released custom-designed ARM-based CPUs for their cloud compute platforms. The expanding ARM ecosystem bolsters our multi-cloud strategy for AuthZed Dedicated and allows our production workloads to benefit from ARM's design, which prioritizes high core count and power efficiency under load.</p>\n<p>AuthZed Dedicated is our dedicated authorization infrastructure deployed adjacent to customer applications in their preferred cloud platform. This allows for the lowest latency between user applications and our permissions systems, and for the most comprehensive region support. With the availability of ARM-based compute options across the major providers, we are able to take advantage of the economic and performance advantages of ARM-based infrastructure in production:</p>\n<ul>\n<li>20% cheaper compute costs</li>\n<li>20-25% more efficient CPU usage for our workloads</li>\n<li>20% higher throughput (based on a load tests at 1 million QPS on AWS Graviton EC2 instances)</li>\n</ul>\n<h2 id=\"user-content-end-to-end-arm-advantage\">End-to-End ARM Advantage</h2>\n<p>From developer laptops to cloud infrastructure, ARM delivers consistent advantages throughout our engineering pipeline. For AuthZed, it's now our preferred platform for building and running authorization infrastructure that helps customers secure applications with confidence and scale efficiently.</p>\n<p>The combination of developer productivity, cost efficiency, and performance gains enables our growing startup to innovate and compete effectively. As cloud providers continue expanding ARM-based offerings and development tools mature further, we expect these advantages to compound, creating even more opportunities to deliver value through our authorization infrastructure.</p>\n<p>By embracing ARM across development and production environments, we've created a seamless experience that benefits both our team and our customers—accelerating development while delivering more performant and cost-effective services.</p>\n<p>Curious about the inspiration behind AuthZed’s modern approach to authorization? Explore the <a href=\"https://authzed.com/z/google-zanzibar-annotated-paper\">Google Zanzibar research paper</a> with our annotations and foreword by Kelsey Hightower to learn how it all began.<br>\n<a href=\"https://authzed.com/z/google-zanzibar-annotated-paper\">https://authzed.com/z/google-zanzibar-annotated-paper</a></p>",
            "url": "https://authzed.com/blog/building-better-authorization-infrastructure-with-arm",
            "title": "Building Better Authorization Infrastructure with ARM: Benefits from Laptop to Cloud",
            "summary": "How ARM helps AuthZed build and operate authorization infrastructure, from day-to-day productivity gains to cost-effective, performant cloud compute.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-05-14T13:00:00.000Z",
            "date_published": "2025-05-14T13:00:00.000Z",
            "author": {
                "name": "Sam Kim",
                "url": "https://github.com/samkim"
            }
        },
        {
            "id": "https://authzed.com/blog/zed-v0-30-2-release",
            "content_html": "<p><a href=\"https://github.com/authzed/zed\">Zed</a> is the command line interface (CLI) tool that you can use to interact with your SpiceDB cluster. With it you can easily switch between clusters, write and read schemas, write and read relationships, and check for permissions. It can be launched as a standalone binary or as a Docker container. Detailed installation options documented <a href=\"https://authzed.com/docs/spicedb/getting-started/installing-zed\">here</a>.</p>\n<h2 id=\"user-content-improvements-in-v0302\">Improvements in v0.30.2</h2>\n<p>Over the last few months we’ve been making many improvements to it, such as:</p>\n<ul>\n<li>Adding support for compilation and validation of <a href=\"https://authzed.com/docs/spicedb/modeling/composable-schemas\">composable schemas</a></li>\n<li>Adding automatic retries</li>\n<li>Adding a new <code>zed backup</code> command</li>\n<li>Publishing the package to <a href=\"https://community.chocolatey.org/packages/zed\">Chocolatey</a> for all Windows users (currently in review)</li>\n</ul>\n<p>And many other small fixes that are too many to list here. We are happy to announce that last week we <a href=\"https://github.com/authzed/zed/releases/tag/v0.30.2\">released zed v0.30.2</a>, which includes all of these changes.</p>\n<p>In the near future we expect to be adding support for a new test syntax in schema files, which will allow you to validate that your schema and relationships work as you expect them to. Stay tuned!</p>\n<p>As you can see, we are continuously making improvements to zed. If you see anything not working as expected, or if you have an idea for a new feature, please don’t hesitate to open an issue in <a href=\"https://github.com/authzed/zed\">https://github.com/authzed/zed</a>. Also, while you’re at it, please give us a star!</p>",
            "url": "https://authzed.com/blog/zed-v0-30-2-release",
            "title": "Zed v0.30.2 Release",
            "summary": "Zed CLI provides seamless interaction with SpiceDB clusters, allowing you to manage schemas, relationships, and permissions checks. Our v0.30.2 release adds composable schema support, automatic retries, backup functionality, and upcoming Windows package integration via Chocolatey.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-05-01T11:12:00.000Z",
            "date_published": "2025-05-01T11:12:00.000Z",
            "author": {
                "name": "Maria Inés Parnisari",
                "url": "https://github.com/miparnisari"
            }
        },
        {
            "id": "https://authzed.com/blog/kubecon-europe-2025-highlights-navigating-authorization-challenges-in-fintech-with-authzeds-jimmy-zelinskie-and-pierre-alexandre-lacerte-from-upgrade",
            "content_html": "<h2 id=\"user-content-navigating-authorization-challenges-in-fintech-with-jimmy-zelinskie-and-pierre-alexandre-lacerte\">Navigating Authorization Challenges in FinTech with Jimmy Zelinskie and Pierre-Alexandre Lacerte</h2>\n<p>At this year's KubeCon + CloudNativeCon Europe 2025 in London, AuthZed CPO Jimmy Zelinskie sat down with Pierre-Alexandre Lacerte, Director of Software Development at Upgrade, for an insightful discussion on modern authorization challenges and solutions. The interview, hosted by Michael Vizard of Techstrong TV, covers several key topics that should be on every developer's radar.</p>\n<h2 id=\"user-content-watch-the-full-interview\">Watch the Full Interview</h2>\n<p>Before diving into the highlights, you can <a href=\"https://techstrong.tv/videos/kubecon-cloudnativecon-europe-2025/navigating-authorization-challenges-in-fintech-with-jimmy-zelinskie-and-pierre-alexandre-lacerte-kubecon-europe-2025\">watch the complete interview on Techstrong TV here</a>. It's packed with valuable insights for anyone interested in authorization, security, and cloud-native architectures.</p>\n<h2 id=\"user-content-key-highlights-from-the-conversation\">Key Highlights from the Conversation</h2>\n<h3 id=\"user-content-origins-of-spicedb-and-the-zanzibar-approach\">Origins of SpiceDB and the Zanzibar Approach</h3>\n<p>Jimmy shares the origin story of AuthZed, explaining how his experience building Quay (one of the first private Docker registries) revealed fundamental challenges with authorization:</p>\n<blockquote>\n<p>\"When you think about it, the only thing that makes a private Docker registry different from like a regular Docker registry where anyone can pull any container down is literally authorization... the core differentiator of that product was authorization.\"</p>\n</blockquote>\n<p>The turning point came when Google published the Zanzibar paper in 2019:</p>\n<blockquote>\n<p>\"We read this paper and said, this is actually how you're supposed to solve these problems. This would have solved all the problems we had building Quay.\"</p>\n</blockquote>\n<h3 id=\"user-content-what-is-relationship-based-access-control\">What is Relationship-based Access Control?</h3>\n<p>One of the most valuable segments of the interview explains the concept of relationship-based access control:</p>\n<blockquote>\n<p>\"The approach in the Zanzibar paper is basically this idea of relationship-based access control, which is not how most people are doing things today. The idea is essentially that you can save sets of relationships inside of a database and then query that later to determine who has access.\"</p>\n</blockquote>\n<p>Jimmy illustrates this with a simple example that makes the concept accessible:</p>\n<blockquote>\n<p>\"Jimmy is a part of this team. This team has access to this resource. And then if I can find that chain from Jimmy through the team to that resource, that means Jimmy has access to that resource transitively through those relationships.\"</p>\n</blockquote>\n<h3 id=\"user-content-why-upgrade-chose-not-to-build-in-house\">Why Upgrade Chose Not to Build In-House</h3>\n<p>Pierre-Alexandre explains the decision-making process that led Upgrade to adopt SpiceDB rather than building an in-house solution:</p>\n<blockquote>\n<p>\"We're a fintech, so we offer personal loans, checking accounts. But eventually we started developing more advanced products where we had to kind of change the foundation of our authorization model... we're kind of not that small, but at the same time we cannot allocate like 200 engineers on authorization.\"</p>\n</blockquote>\n<p>Their evaluation involved looking at industry leaders:</p>\n<blockquote>\n<p>\"We started looking at a few solutions actually, and then also the landscape, like what is GitHub doing? What is the Carta, Airbnb doing?... a lot of those solutions were kind of hedging into the direction of Zanzibar or Zanzibar-ish approach.\"</p>\n</blockquote>\n<h3 id=\"user-content-the-power-of-centralization\">The Power of Centralization</h3>\n<p>The interview highlights a critical advantage of centralized authorization systems:</p>\n<blockquote>\n<p>\"The real end solution to all that is centralization. If there's only one system of record, it's really easy to make sure you've just removed that person from the one single system of record.\"</p>\n</blockquote>\n<p>Pierre-Alexandre describes how Upgrade implemented this approach:</p>\n<blockquote>\n<p>\"When someone leaves the company or when someone changes teams, we do have automation that would propagate the changes across the applications you have access to down to the SpiceDB instance. So we have this kind of sync infrastructure that makes sure that this is replicated within a few seconds.\"</p>\n</blockquote>\n<h3 id=\"user-content-cloud-native-requirements\">Cloud-Native Requirements</h3>\n<p>For companies operating in regulated industries like fintech, having a cloud-native solution is essential. Pierre-Alexandre emphasizes:</p>\n<blockquote>\n<p>\"We're on Amazon EKS, so Kubernetes Foundation... For us, finding something that was cloud native, Kubernetes native was very important.\"</p>\n</blockquote>\n<h3 id=\"user-content-authorization-for-ai-the-next-frontier\">Authorization for AI: The Next Frontier</h3>\n<p>One of the most forward-looking parts of the discussion addresses the intersection of authorization and AI:</p>\n<blockquote>\n<p>\"The real kind of question is actually applying authorization to AI and not vice versa... now with AI, we don't have that same advantage of it just being like a couple folks. If you train a model or have tons of embeddings around your personal private data, now anyone querying that LLM has access to all that data at your business.\"</p>\n</blockquote>\n<p>Upgrade is already exploring solutions:</p>\n<blockquote>\n<p>\"In our lab, we're exploring different patterns, leveraging SpiceDB where we have a lot of internal documentation and the idea is to ingest those documents and tag them on SpiceDB and then leveraging some tools in the GenAI space to query some of this data.\"</p>\n</blockquote>\n<h3 id=\"user-content-the-bottom-line-dont-build-your-own-authorization\">The Bottom Line: Don't Build Your Own Authorization</h3>\n<p>Perhaps the most quotable moment from the interview is Jimmy's passionate plea to developers:</p>\n<blockquote>\n<p>\"If there's like one takeaway from kind of us building this business, it's that folks shouldn't be building their own authorization. Whether the tool is SpiceDB that they end up choosing or another one, like developers, they wouldn't dream of building their own database when they're building their applications. But authorization systems, they've been studied and researched and written about in computer science since the exact same time. Yet every developer thinks they can write custom code for each app implementing custom logic for a thing they don't have no background in, right? And I think this is kind of just like preposterous.\"</p>\n</blockquote>\n<p>Pierre-Alexandre adds a pragmatic perspective from the customer side:</p>\n<blockquote>\n<p>\"Obviously, I probably have decided to go with SpiceDB sooner. But yeah, I mean, we had to do our homework and learn.\"</p>\n</blockquote>\n<h2 id=\"user-content-beyond-the-highlights\">Beyond the Highlights</h2>\n<p>The full interview covers additional topics not summarized here, including:</p>\n<ul>\n<li>The distinction between authentication and authorization (and why the terms are confusing)</li>\n<li>Security implications of centralized authorization</li>\n<li>Enterprise features for enhanced control and monitoring</li>\n<li>How SpiceDB handles audit logging and security events</li>\n</ul>\n<h2 id=\"user-content-join-the-conversation\">Join the Conversation</h2>\n<p>Interested in learning more about modern authorization approaches after watching the interview?</p>\n<ul>\n<li><a href=\"https://github.com/authzed/spicedb\">Star our GitHub repository</a></li>\n<li><a href=\"https://authzed.com/discord\">Join our Discord community</a></li>\n<li><a href=\"https://authzed.com/zanzibar\">Read the Zanzibar paper</a></li>\n<li><a href=\"https://play.authzed.com\">Try our interactive SpiceDB playground</a></li>\n</ul>\n<p>Don't miss this insightful conversation that challenges conventional wisdom about authorization and provides a glimpse into how forward-thinking companies are approaching these challenges. <a href=\"https://techstrong.tv/videos/kubecon-cloudnativecon-europe-2025/navigating-authorization-challenges-in-fintech-with-jimmy-zelinskie-and-pierre-alexandre-lacerte-kubecon-europe-2025\">Watch the full interview now →</a></p>",
            "url": "https://authzed.com/blog/kubecon-europe-2025-highlights-navigating-authorization-challenges-in-fintech-with-authzeds-jimmy-zelinskie-and-pierre-alexandre-lacerte-from-upgrade",
            "title": "Techstrong.tv Interview with Jimmy Zelinskie and Pierre-Alexandre Lacerte from Upgrade",
            "summary": "Watch AuthZed CPO Jimmy Zelinskie and Upgrade's Pierre-Alexandre Lacerte discuss modern authorization challenges, relationship-based access control, and why companies shouldn't build their own authorization systems in this insightful KubeCon Europe 2025 interview with Techstrong.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-04-08T16:15:00.000Z",
            "date_published": "2025-04-08T16:15:00.000Z",
            "author": {
                "name": "Sam Kim",
                "url": "https://github.com/samkim"
            }
        },
        {
            "id": "https://authzed.com/blog/meet-dibs-the-mascot-bringing-spicedb-to-life",
            "content_html": "<p>We're pleased to introduce you to the official SpiceDB mascot – the Muad'dib, or Dibs for short. As we prepare for KubeCon + CloudNativeCon EU in London, we're unveiling this distinctive character who will represent our project in meaningful ways.</p>\n<p><img src=\"/images/upload/blog-meet_dibs-2x.png\" alt=\"\"></p>\n<h2 id=\"user-content-why-a-muaddib\">Why a Muad'dib?</h2>\n<p>The name \"Muad'dib\" continues our <a href=\"https://authzed.com/blog/writing-relationships-to-spicedb#why-is-spicedb-a-db\">tradition</a> of referencing Frank Herbert's Dune series. For those unfamiliar with Dune, the Muad'dib is a small desert mouse known for its resilience and adaptability—qualities we strive to incorporate into SpiceDB.</p>\n<p>With its distinctive oversized ears and agile movements, the Muad'dib is far more than just a charming emblem. In the unforgiving desert, every step matters, and this remarkable creature's fast, efficient navigation mirrors how SpiceDB processes complex data in real time. Those attentive ears serve as a reminder to remain vigilant and responsive, embodying survival instincts honed in the harshest environments.</p>\n<p>Much like SpiceDB's approach to authorization challenges, the Muad'dib transforms obstacles into opportunities. This desert-dwelling creature represents our commitment to resilience, speed, and a collaborative spirit – all values that drive SpiceDB forward in the cloud-native ecosystem.</p>\n<h2 id=\"user-content-dibs-at-kubecon--cloudnativecon-eu\">Dibs at KubeCon + CloudNativeCon EU</h2>\n<p>We will be at KubeCon + CloudNativeCon in London so stop by our booth <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/venue-travel/#venue-information\">#: N632</a> to pick up your very own Dibs swag.</p>\n<p>And join us for our scheduled activities:</p>\n<h3 id=\"user-content-april-2-2025---live-ama\">April 2, 2025 - Live AMA</h3>\n<p>Kelsey Hightower AMA at our booth <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/venue-travel/#venue-information\">#: N632</a></p>\n<p><img src=\"/images/upload/KubeConEU-Kelsey-Social@2x.png\" alt=\"\"></p>\n<h3 id=\"user-content-april-3-2025---party-hosted-by-authzed\">April 3, 2025 - Party hosted by AuthZed</h3>\n<p>Come party with AuthZed, Spotify, Rootly and Infiscal at the Munich Cricket Club Canary Wharf.</p>\n<p><a href=\"https://lu.ma/t5zxycim\"><button class=\"text-light bg-suns-800 hover:bg-suns-200 rounded-lg px-8 py-4 no-underline\">RSVP</button></a></p>\n<p><img src=\"/images/upload/KubeCon-EU-party.jpg\" alt=\"\"></p>\n<p>We would love to talk with you about how we can help fix your access control and provide the infrastructure necessary to support your applications.</p>\n<h2 id=\"user-content-get-involved-with-dibs-and-spicedb\">Get Involved with Dibs and SpiceDB</h2>\n<p>We look forward to seeing how our community connects with Dibs the Muad'dib. Here's how you can get involved:</p>\n<ol>\n<li><strong>Follow Us Online</strong>: We post project updates and industry news regularly on <a href=\"https://www.linkedin.com/company/authzed\">LinkedIn</a>, <a href=\"https://bsky.app/profile/authzed.com\">BlueSky</a>, and <a href=\"https://twitter.com/authzed\">X</a>.</li>\n<li><strong>Visit Us at KubeCon</strong>: Stop by our booth <a href=\"https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/venue-travel/#venue-information\">#: N632</a> at KubeCon + CloudNativeCon EU in London to see Dibs in person and collect exclusive Muad'dib merchandise.</li>\n<li><strong>Contribute to SpiceDB</strong>: Consider <a href=\"https://github.com/authzed/spicedb\">contributing to SpiceDB</a> and continue the development of our open source project.</li>\n<li><strong>Join Our Community</strong>: Connect with other SpiceDB users and developers in our <a href=\"https://authzed.com/discord\">Discord server</a>.</li>\n</ol>\n<p>This creature represents not just our project, but the spirit of our community – adaptable, resilient, and ready to navigate complex challenges.</p>\n<p>Welcome, Dibs.</p>",
            "url": "https://authzed.com/blog/meet-dibs-the-mascot-bringing-spicedb-to-life",
            "title": "Meet Dibs: The Mascot Bringing SpiceDB to Life",
            "summary": "Meet Dibs the Muad'dib, SpiceDB's new mascot that embodies our commitment to resilience, adaptability, and precision in solving complex authorization challenges. Drawing inspiration from Frank Herbert's Dune universe, this vigilant desert creature symbolizes how SpiceDB navigates the harsh terrain of modern access control with efficiency and intelligence.",
            "image": "https://authzed.com/images/upload/blog-meet_dibs-2x.png",
            "date_modified": "2025-03-25T12:17:00.000Z",
            "date_published": "2025-03-25T12:17:00.000Z",
            "author": {
                "name": "Corey Thomas",
                "url": "https://www.linkedin.com/in/cor3ythomas/"
            }
        },
        {
            "id": "https://authzed.com/blog/the-evolution-of-expiration",
            "content_html": "<h1 id=\"user-content-feature-highlight-relationship-expiration\">Feature Highlight: Relationship Expiration</h1>\n<p>We are excited to <a href=\"/blog/build-time-bound-permissions-with-relationship-expiration-in-spicedb\">announce</a> that as of the <a href=\"https://github.com/authzed/spicedb/releases/tag/v1.40.0\">SpiceDB v1.40</a> release, users now have access to a new experimental feature: Relationship Expiration. When writing relationships, requests can now include an optional expiration time, after which a relationship will be treated as removed, and eventually automatically cleaned up.</p>\n<h2 id=\"user-content-the-evolution-of-expiration\">The evolution of expiration</h2>\n<p>Even when first setting out to create SpiceDB, there was never any doubt whether or not users would want time-bound access control to their resources. However, the inspiration for SpiceDB, Google's Zanzibar system, has no public documentation for how this functionality is built. As our initial goals for the SpiceDB project were to be as faithful to Google's design as possible, we initially left expiration as an exercise to the user.</p>\n<p>Without explicit support within SpiceDB, users could still use external systems like workflow engines (e.g. <a href=\"http://temporal.io\">Temporal</a>) to schedule calls to the SpiceDB DeleteRelationships or WriteRelationships APIs in order to solve this problem. This is a perfectly valid way to solve this problem, but it has a major tradeoff: users must adopt yet another system to coordinate their usage of the SpiceDB API.</p>\n<p>After we had successfully reached our goal of being the premier implementation of the concepts expressed in the Google Zanzibar paper, we turned our focus to improving developer experience and more real-world requirements outside of the walls of Google. This led us to <a href=\"https://authzed.com/customers/netflix\">collaborating with Netflix</a> on a system for supporting lightweight policies to more effectively model ABAC-style use cases. This design came to be known as Relationship Caveats. Caveats allow SpiceDB users to write conditional relationships that exist depending on whether a CEL expression evaluates to true while their request is being processed. With the introduction of Caveats, SpiceDB had its first way to create time-bounding without relying on any external system. The use case was so obvious, even our first examples of Caveats demonstrated how to implement time-bounded relationship expiration.</p>\n<p>As more SpiceDB users adopted Caveats, we began to acknowledge some trends in its usage. Many folks didn't actually need or want the full expressiveness of policy; instead they cared solely about modelling expiration itself. Eventually it became obvious that expiration was its own fully-fledged use case. If we could craft an experience specifically for expiration, we could steer many folks away from some of the tradeoffs associated with caveats. If you still need caveats for reasons other than expiration and you're wondering if relationships support both caveats and expiration simultaneously, they do!</p>\n<h2 id=\"user-content-whats-going-on-under-the-hood-for-relationship-expiration\">What's going on under-the-hood for Relationship Expiration?</h2>\n<p>If you've spent time reading some of the deeper discussions on SpiceDB internals or studying other systems, you might be familiar with the fact that time is incredibly nebulous in distributed systems. Distributed systems typically eschew \"wall clocks\" altogether. Instead, for correctness they need to model time based on the <a href=\"https://lamport.azurewebsites.net/pubs/time-clocks.pdf\">ordering of events that occur in the system</a>. This observation, among others, ultimately led Leslie Lamport to win a Turing Award. SpiceDB is no exception to this research: the opaque values encoded into SpiceDB's ZedTokens act as logical clocks used to provide consistency guarantees throughout the system.</p>\n<p>If the problem here isn’t already clear: fundamentally, relationship expiration is tied to wall clock time, but distributed systems research proves this is a Bad Idea™. In order to avoid any inconsistencies caused by the skew in synchronization of clocks across machines, SpiceDB implements expiration by pushing as much logic into the underlying datastore as possible. For a datastore like PostgreSQL, there is no longer a synchronization problem because there's only one clock that matters: the one on the leader's machine. Some datastores even have their own first-class expiration primitives that SpiceDB can leverage in order to offload this logic entirely while ensuring that the removal of relationships are done as efficiently as possible. This strategy is only possible because of SpiceDB's unique architecture of reusing other existing databases for its storage layer rather than the typical disk-backed key-value store.</p>\n<h2 id=\"user-content-trying-out-relationship-expiration\">Trying out Relationship Expiration</h2>\n<p>There are only a few steps required to try out expiration once you've upgraded to SpiceDB v1.40:</p>\n<ol>\n<li>Enable the experimental flag when running SpiceDB</li>\n</ol>\n<pre><code class=\"hljs language-command\">spicedb serve --enable-experimental-relationship-expiration [...]\n</code></pre>\n<ol start=\"2\">\n<li>Annotate that you want to use first-class expiration in your schema.</li>\n</ol>\n<pre><code class=\"hljs language-zed\">use expiration\u000b\u000b\n\ndefinition folder {}\u000b\ndefinition resource {\n  relation parent: folder\n}\n</code></pre>\n<ol start=\"3\">\n<li>Annotate the relations in your schema where you want to support expiration.</li>\n</ol>\n<pre><code class=\"hljs language-zed\">use expiration\u000b\u000b\n\ndefinition folder {}\u000b\n  definition resource {\n  relation parent: folder with expiration\n}\n</code></pre>\n<ol start=\"4\">\n<li>Provide a timestamp for the `OptionalExpiresAt` field when writing relationships.</li>\n</ol>\n<pre><code class=\"hljs language-proto\">WriteRelationshipsRequest { Updates: [\n  RelationshipUpdate {\n    Operation: CREATE\n    Relationship: {\n      Resource: { ObjectType: \"resource\", ObjectId: \"123\", },\n      Relation: \"parent\",\n      Subject: { ObjectType: \"folder\", ObjectId: \"456\", },\n      OptionalExpiresAt: \"2025-12-31T23:59:59Z\"\n      }\n    }]\n}\n</code></pre>\n<h2 id=\"user-content-the-journey-for-peak-performance-continues\">The journey for peak performance continues</h2>\n<p>Relationship Expiration is a great example of our never-ending journey to achieve the best possible performance for SpiceDB users. As SpiceDB is put to the test in an ever-increasing number of diverse enterprise use-cases, we learn new things about where optimizations should be made in order to deliver the best product for scaling authorization. Sometimes it requires going back to the drawing board on a problem we thought we had previously solved and totally reconsidering its design. With that, I encourage you to go out and experiment with Relationship Expiration so that we learn even more about the problemspace and continue refining our approach.</p>",
            "url": "https://authzed.com/blog/the-evolution-of-expiration",
            "title": "The Evolution of Expiration",
            "summary": "We are excited to announce that as of the SpiceDB 1.40 release, users now have access to a new experimental feature: Relationship Expiration. When writing relationships, requests can now include an optional expiration time, after which a relationship will be treated as removed, and eventually automatically cleaned up.",
            "image": "https://authzed.com/images/blogs/blog-eng-relationship-expiration-hero-2x.png",
            "date_modified": "2025-02-13T10:16:00.000Z",
            "date_published": "2025-02-13T10:16:00.000Z",
            "author": {
                "name": "Jimmy Zelinskie",
                "url": "https://twitter.com/jimmyzelinskie"
            }
        },
        {
            "id": "https://authzed.com/blog/build-time-bound-permissions-with-relationship-expiration-in-spicedb",
            "content_html": "<p>Today we are announcing the experimental release of Relationship Expiration, which is a straightforward, secure, and dynamic way to manage time-bound permissions directly within SpiceDB.</p>\n<p>Building secure applications is hard, especially when it comes to implementing temporary access management for sensitive data. You need to grant the right level of access to the right people for the right duration, without creating long-term vulnerabilities or drowning in administrative overhead.</p>\n<p>Consider the last time you needed to give a contractor access to your brand guidelines, a vendor access to a staging environment, or a new employee access to onboarding materials. The usual workarounds – emailing files, uploading to external systems, or (<em>please, please don’t</em>) sharing logins – quickly become a tangled mess of version control nightmares, security risks, and administrative headaches. And what happened when you completed the project? How did you guarantee that access gets promptly revoked? Leaving lingering access privileges hanging around is an AppSec war room waiting to happen.</p>\n<p>We’re helping application development teams solve this problem with this powerful new feature in SpiceDB v1.40.</p>\n<p><em>\"Authorization is essential for building secure applications with advanced sharing capabilities,\"</em> said Larry Carvalho, Principal Consultant and Founder at RobustCloud. <em>\"SpiceDB, inspired by Google's approach to authorization, provides developers with a much-needed feature for managing fine-grained access control. By leveraging AuthZed’s expertise, developers can build the next generation of applications with greater efficiency, security, and flexibility.\"</em></p>\n<h2 id=\"user-content-beyond-workarounds-a-first-class-solution\">Beyond workarounds: a first class solution</h2>\n<p>While workarounds exist – scheduling API calls with external tools like Temporal or crafting complex policies – they add complexity and can be difficult to manage and deploy at scale (think 10,000 relationships generated and refreshed every 10 minutes). SpiceDB's Relationship Expiration provides first-class support for building time-bound permissions, leveraging SpiceDB’s powerful relationship-based approach.</p>\n<p>As the name suggests, expirations are attached as a trait to relationships between subjects and resources in SpiceDB’s graph-based permissions evaluation engine. Once the relationship expires, SpiceDB automatically removes it. Without this built-in support, conditional time-bound relationships in a Zanzibar-style schema clutter the permissions graph, bloating the system and impacting performance.</p>\n<h2 id=\"user-content-why-you-should-be-building-time-bound-permissions-with-spicedb\">Why you should be building time-bound permissions (with SpiceDB)</h2>\n<h3 id=\"user-content-collaborate-productively-and-securely\">Collaborate productively and securely</h3>\n<p>Time-bound access helps teams to collaborate securely and efficiently. By eliminating the friction of manual access management, it frees up valuable time and resources while minimizing the risk of human error. Knowing that access will automatically expire fosters a culture of confident sharing, removing the hesitation that can lead to information silos and slower project cycles. Additionally, just-in-time access with session-based privileges streamlines workflows and minimizes the risk of unauthorized access.</p>\n<h3 id=\"user-content-dynamic-permissions\">Dynamic permissions</h3>\n<p>Put access control in the hands of your users: they can define expiration limits for the resources they manage, unlocking powerful workflows like time-limited review cycles or project-based access. A designer, for example, could grant edit access to a file for a specific review period, with access automatically revoked afterward. This granular control enhances security by minimizing the window of opportunity for unauthorized access and fosters a culture of security awareness. Leave a positive impression with custom permissions options that welcome a broad range of use cases.</p>\n<h3 id=\"user-content-optimize-permissions-systems\">Optimize permissions systems</h3>\n<p>With millions of users and billions of resources, authorization can become a major performance bottleneck, especially since permissions checks sit in the critical path between user input and service response. By automatically removing expired relationships, SpiceDB reduces the size of its database and load on its system, leading to more performant authorization checks and lower costs.</p>\n<h2 id=\"user-content-learn-more-today\">Learn more today</h2>\n<p>Want to learn more TODAY? Join Sohan, AuthZed technical evangelist, and Joey Schorr, one of the founders of AuthZed, during our biweekly Office Hours livestream at 9 am PT / 12 pm ET on February 13th! We hope to see you there.</p>\n<iframe width=\"100%\" height=\"360\" src=\"https://www.youtube.com/embed/u39_Yj_RYc0\" title=\"Feature Launch! Expiring Relationships on SpiceDB\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\"></iframe>\n<p>Or, hop over to Jimmy Zelinskie’s <a href=\"https://authzed.com/blog/the-evolution-of-expiration\">blog post</a> to learn more about how to implement expiring relationships and try them out in SpiceDB today.</p>\n<h2 id=\"user-content-dont-let-relationships-linger-past-their-expiration-date\">Don’t let relationships linger past their expiration date!</h2>\n<p>You may have noticed that we've lined up this launch just in time for Valentine’s Day. Most relationships between humans do, sadly, have an expiration date… To recognize the (somewhat) unfortunate timing of this release, we’ve compiled a Spotify list of songs sourced from the AuthZed team just for those nursing broken hearts this season. And if you’re one of the lucky ones celebrating, hey, it’s fun music to jam to while you learn SpiceDB.</p>\n<iframe style=\"border-radius:12px\" src=\"https://open.spotify.com/embed/playlist/7JVjboSdmt8QJU5mxAAP1z?utm_source=generator\" width=\"100%\" height=\"352\" allow=\"autoplay; clipboard-write; encrypted-media; fullscreen; picture-in-picture\"></iframe>\n<p>If you haven’t already, give SpiceDB a <a href=\"https://github.com/authzed/spicedb\">star</a> on GitHub, or follow us on <a href=\"https://www.linkedin.com/company/73024037/\">LinkedIn</a>, <a href=\"https://x.com/authzed\">X</a>, or <a href=\"https://bsky.app/profile/authzed.com\">BlueSky</a> to stay up to date on all things AuthZed. Or ready to get started? <a href=\"https://authzed.com/call\">Schedule a call</a> with us to talk about how we can help with your authorization needs.</p>",
            "url": "https://authzed.com/blog/build-time-bound-permissions-with-relationship-expiration-in-spicedb",
            "title": "Build Time-Bound Permissions with Relationship Expiration in SpiceDB",
            "summary": "Today we are announcing the experimental release of Relationship Expiration, which is a straightforward, secure, and dynamic way to manage time-bound permissions directly within SpiceDB. \n",
            "image": "https://authzed.com/images/blogs/blog-relationship-expiration-hero-2x.png",
            "date_modified": "2025-02-13T10:16:00.000Z",
            "date_published": "2025-02-13T10:16:00.000Z",
            "author": {
                "name": "Jess Hustace",
                "url": "https://twitter.com/_jessdesu"
            }
        },
        {
            "id": "https://authzed.com/blog/deepseek-balancing-potential-and-precaution-with-spicedb",
            "content_html": "<p>DeepSeek has emerged as a phenomenon since its announcement in late December 2024 by hedge fund company High-Flyer. The AI industry and general public have been captivated by both its capabilities and potential implications.</p>\n<p>Security has been at the forefront of recent conversation due to reports from Wiz that the <a href=\"https://www.wiz.io/blog/wiz-research-uncovers-exposed-deepseek-database-leak\">DeepSeek database is leaking sensitive information, including chat history</a> as well as geopolitical concerns. Even RedMonk analyst Stephen O’Grady discussed <a href=\"https://redmonk.com/sogrady/2025/01/27/deepseek-and-the-enterprise/\">DeepSeek and the Enterprise</a> focusing on considerations for business adoption.</p>\n<p>At AuthZed, we recognize that trust and security fundamentally shape how organizations evaluate AI models, which is why we're sharing our perspective on this crucial discussion.</p>\n<h2 id=\"user-content-the-deepseek-phenomenon\">The DeepSeek Phenomenon</h2>\n<p>What makes DeepSeek particularly noteworthy is its unique combination of features. As an open-source model, it demonstrates performance comparable to frontier models from industry leaders like OpenAI and Anthropic, yet achieves this with (reportedly) significantly lower training costs. The R1 version exhibits impressive reasoning capabilities, further challenging conventional assumptions about the infrastructure investments required for advancing LLM performance.</p>\n<h2 id=\"user-content-balancing-potential-and-precaution\">Balancing Potential and Precaution</h2>\n<p>While these factors drive DeepSeek’s popularity, they’ve also drawn skepticism alongside geopolitical considerations based on DeepSeek’s origin. The uncertainty surrounding the source of training data and potential biases in responses warrants careful consideration. A recent data breach of the hosted service has heightened privacy concerns, particularly given the official hosted service’s terms of service permit user data retention for future model training.</p>\n<p>Despite the concerns, users and companies increasingly express interest in exploring its capabilities. Organizations seeking to leverage DeepSeek's capabilities while maintaining data security can adopt permissions systems to define data access controls. This strategy is especially relevant for applications built on DeepSeek's large language models, where protecting sensitive information is paramount.</p>\n<h2 id=\"user-content-spicedb-a-solution-for-secure-ai-integration\">SpiceDB: A Solution for Secure AI Integration</h2>\n<p>SpiceDB offers a robust framework for organizations integrating AI capabilities. Its fine-grained permissions help avoid oversharing by letting you precisely define which data the model can and cannot access. This granular control extends beyond data access - you can prevent excessive agency by explicitly defining the scope of actions a DeepSeek-based agent is permitted to take. This dual approach to security - controlling both data exposure and action boundaries - makes SpiceDB particularly valuable for organizations that want to leverage DeepSeek’s capabilities but in a controlled environment.</p>\n<h2 id=\"user-content-practical-implementation\">Practical Implementation</h2>\n<p>To help organizations get started, we've created a demo notebook showcasing SpiceDB integration with a DeepSeek-based RAG system: <a href=\"https://github.com/authzed/workshops/tree/deepseek/secure-rag-pipelines\">https://github.com/authzed/workshops/tree/deepseek/secure-rag-pipelines</a></p>\n<p>For further exploration and community support, join our SpiceDB Discord community to connect with other developers implementing secure AI applications.</p>",
            "url": "https://authzed.com/blog/deepseek-balancing-potential-and-precaution-with-spicedb",
            "title": "DeepSeek: Balancing Potential and Precaution with SpiceDB",
            "summary": "DeepSeek has emerged as a phenomenon since its announcement in late December 2024 and security has been at the forefront of recent conversation. At AuthZed, we recognize that trust and security fundamentally shape how organizations evaluate AI models, which is why we're sharing our perspective on this crucial discussion.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-01-31T07:56:00.000Z",
            "date_published": "2025-01-31T07:56:00.000Z",
            "author": {
                "name": "Sam Kim",
                "url": "https://github.com/samkim"
            }
        },
        {
            "id": "https://authzed.com/blog/2024-soc2-reflection",
            "content_html": "<p>I'm happy to announce that AuthZed recently renewed our SOC2 compliance and our SOC2 Type 2 and SOC3 reports are now available on <a href=\"https://security.authzed.com\">security.authzed.com</a>.</p>\n<p>Having just endured the audit process again, I figured it would be a good time to reflect on my personal feelings toward compliance and how my opinion has evolved.</p>\n<h2 id=\"user-content-an-unbiased-description-of-soc2\">An unbiased description of SOC2</h2>\n<p>If you're reading this now and aren't familiar with SOC2 and SOC3, I'll give you an overview by someone that isn't trying to sell you a compliance tool (feel free to skip this section):</p>\n<p>SOC (System and Organization Controls) is a suite of annual reports that result from conducting an audit of the internal controls that you use to guarantee security practices at your company. An example of an \"internal control\" is a company-wide policy that enforces that \"all employees have an anti-virus installed on their devices\". Controls vary greatly and can be automated by using software like password managers and MDM solutions, but some will always require human intervention, such as performing quarterly security reviews and annual employee performance reviews.</p>\n<p>In the tech industry, SOC2 is the standard customers expect (or ISO27001 if you're in the EU, but they are similar enough that you often only need either one). As I wrote this, it came to my attention that I have no idea what SOC1 is, so I looked it up to discover that it is apparently a financial report which I've never heard of customers requesting in the tech industry. SOC3 is a summary of a SOC2 report that contains less detail and is designed to be more publicly sharable so that you don't necessarily need to sign an NDA to get some details. SOC2 comes in two variants \"Type 1\" and \"Type 2\". It's fairly confusing, but this is just shorthand for how long the audit period was. Type 1 means that the audit looked at the company at one point in time, while Type 2 means that the auditor actually monitored the company over a period of time usually 6 or 12 months.</p>\n<h2 id=\"user-content-what-engineers-think-about-soc2-compliance\">What engineers think about SOC2 Compliance</h2>\n<p>To engineering organizations, compliance is often seen as a nuisance or a distraction from shipping code that moves the needle for actual security issues. Software engineers are those deepest in the weeds, so they have the code that they're familiar with at the top of mind when you ask where security concerns lie. Because I knew where the bodies were buried when I first transitioned my career to product management from engineering, I always tried to push back and shield my team from having to deliver compliance features. The team celebrated this as a win for focus, but we never got to fully understand the externalities of this approach.</p>\n<p>Fast forward a few years, I've now gotten <em>much</em> wider exposure to the rest of the business functions at a technology company. From the overarching view of an executive, the perspective of the software engineer seems quite amiss. If you asked an engineer what they're concerned about, it might be that they quickly used the defaults for bcrypt and didn't spend the time evaluating the ideal number of bcrypt rounds or alternative algorithms. This perspective is valuable, but can also be missing the forest for the trees; it's far easier to perform phishing attacks on a new hire than it is to reverse engineer the cryptography in their codebase. That simple fact makes it clear that if you haven't already addressed the foundational security processes at your business, it doesn't matter how secure the software you're building is.</p>\n<h2 id=\"user-content-compliance-is-ultimately-about-one-thing-trust\">Compliance is ultimately about one thing: trust</h2>\n<p>All of that said, AuthZed's engineering-heavy team is not innocent from this line of thinking, especially since our core product is engineering security infrastructure. However, if we put our egos aside, there is one thing that reigns supreme regardless of the product you're building: the trust you build with your customers.</p>\n<p>The compliance industry was never trying to hide that its end goal is purely trust in processes. SOC2 is defined by the American Institute of Certified Public <em>Accountants</em> and not a cybersecurity standards body; this is because compliance is about ensuring processes at your business and not finding remote code execution in your codebase. That doesn't mean that compliance cannot uncover deep code issues because SOC2 audits actually require you to perform an annual penetration test from an actual cybersecurity vendor. Coding vulnerabilities are only one aspect of the comprehensive approach that compliance is focused on.</p>\n<p>Without compliance, our industry would be stuck having to blindly trust that vendors are following acceptable security practices. By conforming to the processes required for certifications like SOC2, we can build trust with our partners and customers as well as prove the maturity of our products and business. While it may feel like toil at times, it's a necessary evil to ensure consistency across our supply chains.</p>\n<p>The final thought I'd like to leave you with is the idea that compliance isn't a checkbox to do business. It's a continuous process where you offer transparency to your customers to prove that they should trust you. I'm looking forward to seeing if my opinions change next renewal.</p>\n<p>I'd like to thank the teams at SecureFrame and Modern Assurance who we've collaborated with during this last audit as well as all of the vendors and data subprocessors we rely on to operate our business everyday.</p>",
            "url": "https://authzed.com/blog/2024-soc2-reflection",
            "title": "Our SOC2 Renewal and Reflections on Compliance",
            "summary": "I'm happy to announce that AuthZed recently renewed our SOC2 compliance and our SOC2 Type 2 and SOC3 reports are now available on security.authzed.com.\nHaving just endured the audit process again, I figured it would be a good time to reflect on my personal feelings toward compliance and how my opinion has evolved.\n",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-01-07T20:20:00.000Z",
            "date_published": "2025-01-07T20:20:00.000Z",
            "author": {
                "name": "Jimmy Zelinskie",
                "url": "https://twitter.com/jimmyzelinskie"
            }
        },
        {
            "id": "https://authzed.com/blog/the-dual-write-problem",
            "content_html": "<h2 id=\"user-content-overview\">Overview</h2>\n<p>The dual-write problem presents itself in all distributed systems. A system that uses SpiceDB for authorization and also has an application database (read: most of them) is a distributed system. Working around the dual-write problem typically requires a non-trivial amount of work.</p>\n<h2 id=\"user-content-what-is-the-dual-write-problem\">What is the Dual-Write Problem?</h2>\n<p>If you've heard this one before, feel free to skip down where we talk about solutions and approaches to the dual-write problem. If it's your first time, welcome!</p>\n<p>Let's consider a typical monolithic web application. Perhaps it's for managing and sharing files and folders, which makes it a natural candidate for a relation-based access control system like SpiceDB. The application has an upload endpoint that looks something like the following:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">upload</span>(<span class=\"hljs-params\">req</span>):\n  validate_request(req)\n  <span class=\"hljs-keyword\">with</span> new_transaction() <span class=\"hljs-keyword\">as</span> db:\n    db.write_file(req.file)\n  <span class=\"hljs-keyword\">return</span> Response(status=<span class=\"hljs-number\">200</span>)\n</code></pre>\n<p>All of the access control logic is neatly contained within the application database, so no other work needed to happen up to this point. However, we want to start using SpiceDB in anticipation of the application growing more complex and services splitting off of our main monolith.</p>\n<p>We start with a simple schema:</p>\n<pre><code class=\"hljs language-zed\">definition user {}\n\ndefinition folder {\n  relation viewer: user\n  permission view = viewer\n}\n\ndefinition file {\n  relation viewer: user\n  relation folder: folder\n  permission view = viewer + folder->viewer\n}\n</code></pre>\n<p>Note that if a user is a viewer of the folder, they are able to view any file within the folder. That means that we'll need to keep SpiceDB updated with the relationships between files and folders, which is held in the <code>folder</code> relation on the file.</p>\n<p><img src=\"/images/blogs/the-dual-write-problem/permission-model.png\" alt=\"Picture of permission model\"></p>\n<p>That doesn't sound so bad. Let's go and implement it:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">upload</span>(<span class=\"hljs-params\">req</span>):\n  validate_request(req)\n  <span class=\"hljs-keyword\">with</span> new_transaction() <span class=\"hljs-keyword\">as</span> db:\n    file_id = db.write_file(req.file)\n    write_folder_relationship(\n      file_id=file_id\n      folder_id=req.folder_id\n    )\n    \n  <span class=\"hljs-keyword\">return</span> Response(status=<span class=\"hljs-number\">200</span>)\n</code></pre>\n<p>We've got a problem, though. What happens if the server crashes? We're going to use a server crash as an example problem because it's relatively conceptually simple and is also something that's hard to recover from. Let's mark up the function and then consider what happens if the server crashes at each point:</p>\n<pre><code class=\"hljs language-python\"><span class=\"hljs-keyword\">def</span> <span class=\"hljs-title function_\">upload</span>(<span class=\"hljs-params\">req</span>):\n  validate_request(req)\n  <span class=\"hljs-comment\"># point 1</span>\n  <span class=\"hljs-keyword\">with</span> new_transaction() <span class=\"hljs-keyword\">as</span> db:\n    file_id = db.write_file(req.file)\n    <span class=\"hljs-comment\"># point 2</span>\n    write_folder_relationship(\n      file_id=file_id\n      folder_id=req.folder_id\n    )\n    <span class=\"hljs-comment\"># point 3</span>\n  <span class=\"hljs-comment\"># point 4 (outside of the transaction)</span>\n  <span class=\"hljs-keyword\">return</span> Response(status=<span class=\"hljs-number\">200</span>)\n</code></pre>\n<p>Note that the <code>point</code>s refer to the boundaries between lines of code, rather than pointing at the line of code above or below them.\nHere's an alternative view of things in a sequence diagram:</p>\n<p><img src=\"/images/blogs/the-dual-write-problem/handler-sequence-diagram.png\" alt=\"Application handler sequence diagram\"></p>\n<p>If the server crashes at points #1 or #4, we're fine - the request will fail, but we're still in a consistent state. The application server and SpiceDB agree about what the system should look like. If the server crashes at point #2, we're still okay - we've opened a database transaction but we haven't committed it, so the database will roll back the transaction and everything will be fine. If we crash at point #3, however, we're in a state where we've written to SpiceDB but we haven't committed the transaction to our database, and now SpiceDB and our database disagree about the state of the world.</p>\n<p>There isn't a neat way around this problem within the context of the process, either. <a href=\"https://www.confluent.io/blog/dual-write-problem/\">This blog post</a> goes further into potential approaches and their issues if you're curious. Things like adding a transactional semantic to SpiceDB or reordering the operations move the problem around but don't solve it, because there's still going to be some boundary in the code where the process could crash and leave you in an inconsistent state.</p>\n<p>Note as well that there's nothing particularly unique about the dual-write problem in systems using SpiceDB and an application database, either. If we were writing to two different application databases, or to an application database and to a cache, or to two different RPC-invoked services, we still have the same issue.</p>\n<h2 id=\"user-content-so-what-can-we-do\">So what can we do?</h2>\n<p>We can solve the dual-write problem in SpiceDB using a few different approaches, each with varying levels of complexity, prerequisites, and tradeoffs to be made</p>\n<h3 id=\"user-content-do-nothing\">Do Nothing</h3>\n<p>Doing nothing <em>is</em> an option that may be viable in the right context.\nThe sort of data inconsistency where SpiceDB and your application database disagree can be hard to diagnose.\nHowever, if there are mechanisms by which a user could recognize that something is wrong and remediate it in a timely manner, or if the authorized content in question isn't particularly sensitive, you may be able to run a naive implementation and avoid the complexity associated with other approaches.\nThe more stable your platform is, the more likely this is to cause fewer issues.</p>\n<h3 id=\"user-content-out-of-band-consistency-checking\">Out-of-band consistency checking</h3>\n<p>Out-of-band consistency checking would be one step beyond \"doing nothing.\"\nIf you have a source of truth that SpiceDB's state is meant to reflect in a given context, you can check that the two systems agree on a periodic basis.\nIf there's disagreement, the issues can be automatically remediated or flagged for manual intervention.</p>\n<p>This is a conceptually simple approach, but it's limited by both the size of your data and the velocity of changes to your data.\nThe more data you have, the more expensive and time-consuming the reconciliation process becomes.\nIf the data change rapidly, you could have false positives or false negatives when a change has been applied\nto one system but not the other.\nThis could theoretically be handled through locking or otherwise pinning SpiceDB and your application's database so that their data\nreflect the same version of the world while you're checking their associated states,\nbut that will greatly reduce your ability to make writes in your system.\nThe sync process itself can become a source of drift or inconsistency.</p>\n<h3 id=\"user-content-make-spicedb-the-source-of-truth\">Make SpiceDB the source of truth</h3>\n<p>For certain kinds of relationships and data, it may be sufficient to make SpiceDB the source of truth for that particular information.\nThis works best for data that matches SpiceDB's storage and access model: binary presence or absence of a relationship between two objects, and no requirement to sort those relationships or filter by anything other than which subject or object they're associated with.</p>\n<p>If your data meet those conditions, you can remove the application database from the question and make a single write to SpiceDB and avoid the dual-write problem entirely.</p>\n<p>For example, if we wanted to add a notion of a file \"owner\" to our example application, we probably wouldn't need an <code>owner</code> column with a foreign key to a user ID in our application database.\nInstead, we could represent the relationship entirely with an <code>owner</code> relation in SpiceDB, such that an API handler for adding or updating an owner of a file or folder would only talk to SpiceDB and not to the application database.\nBecause only one system is being written to in the handler, we avoid the dual-write problem.</p>\n<p><img src=\"/images/blogs/the-dual-write-problem/spicedb-sot.png\" alt=\"SpiceDB as source of truth\"></p>\n<p>The limitation here is that if you wanted to build a user interface where a user can see a table of all of the files they own, you wouldn't be able to filter, sort, or paginate\nthat table as easily, because SpiceDB isn't a general-purpose database and doesn't support that functionality in the same way.</p>\n<h3 id=\"user-content-event-sourcingcommand-query-responsibility-segregation-cqrs\">Event Sourcing/Command-Query Responsibility Segregation (CQRS)</h3>\n<p>Event sourcing and CQRS are related ideas that involve treating your system as eventually consistent.\nRather than an API call being a procedure that runs to completion, an API call becomes an event that kicks off a chain of actions.\nThat event goes into an event stream, where consumers (to use Kafka's language) can pick them up and process them, which may involve producing new events.\nMultiple consumers can listen to the same topic.\nThe events flow through the system until they've all been processed, and the surrounding runtime ensures that nothing is dropped.</p>\n<p>There's a cute high-level illustration of how an event sourcing system works here: <a href=\"https://www.gentlydownthe.stream/\">https://www.gentlydownthe.stream/</a></p>\n<p>In our example application, it might look like the following:</p>\n<ol>\n<li>A client makes a request to create a file in a folder</li>\n<li>The API handler receives the request and puts a message into the event stream that includes the information about the file</li>\n<li>One consumer picks up the creation message and writes a relation to SpiceDB between the file and its folder</li>\n<li>Another consumer picks up the creation message and writes the information about the file to the application database</li>\n</ol>\n<p>The upside is that you're never particularly worried about the dual-write problem, because any individual failure of a subscriber can be recovered and re-run.\nEverything just percolates through until the system arrives at a new consistent state.</p>\n<p>The downside is that you can't treat API calls as RPCs.\nThe API call doesn't represent a change to the state of your system, but rather a command or request that will\neventually result in your desired changes happening.\nYou can work around this by having the client or UI listen to an event stream from the backend,\nsuch that all you're doing is passing messages back and forth, but this often requires\nsignificant rearchitecture, and not every runtime is amenable to this architecture.</p>\n<p>Here are some examples of event queues that you might see in an event sourcing system:</p>\n<ul>\n<li><a href=\"https://kafka.apache.org/\">Kafka</a></li>\n<li><a href=\"https://www.redpanda.com/\">Redpanda</a></li>\n<li><a href=\"https://nats.io/\">NATS</a></li>\n</ul>\n<h3 id=\"user-content-durable-execution-environments\">Durable Execution Environments</h3>\n<p>A durable execution environment is a set of software tools that let you pretend that you're writing relatively simple transactional logic within your application while abstracting over the concerns involved in writing to multiple services. They promise to take care of errors, rollbacks, and coordination, provided you've written the according logic into the framework.</p>\n<p>An upside is that you don't have to rearchitect your system if you aren't already using the paradigms necessary for event sourcing.\nThe code that you write with these systems tends to be familiar, procedural, and imperative, which lowers the barrier to entry\nfor a dev trying to solve a dual-write problem.</p>\n<p>A downside is that it can be difficult to know when your write has landed, because you're effectively dispatching it off to a job runner.\nThe business logic is moved off of the immediate request path. This means that the result of the business logic is also off of the request\npath, which raises a question of what you would return to an API client.</p>\n<p>Some durable execution environments are explicitly for running jobs and don't give you introspection into the results;\nothers can be inserted into your code in such a way that you can wait for the result and pretend that everything happened synchronously.\nNote that this means that the associated runtime that handles those jobs becomes a part of the request path, which can carry operational overhead.</p>\n<p><a href=\"https://temporal.io/\">Temporal</a>, <a href=\"https://restate.dev/\">Restate</a>, <a href=\"https://www.windmill.dev/\">Windmill</a>, <a href=\"https://trigger.dev/\">Trigger.dev</a>, and <a href=\"https://www.inngest.com/\">Inngest</a> are a few examples of durable execution environments. You'll have to evaluate which one best fits your architecture and infrastructure.</p>\n<h3 id=\"user-content-transactional-outbox-patterns\">Transactional Outbox Patterns</h3>\n<p>A transactional outbox pattern is related to both Event Sourcing and Durable Execution, in that it works around the dual-write problem\nthrough eventual consistency.\nThe idea is that within your application database, when there's a change that needs to be written to SpiceDB, you write to an outbox table, which is an append-only log of modifications that should happen to SpiceDB.\nThat write can happen within the same database transaction, which means you don't have the dual write problem.\nYou then read that log (or subscribe to a changestream) with a separate process which marks the entries as it reads them and then submits them to SpiceDB through some other mechanism.</p>\n<p>As long as this process is effectively single-threaded and retries operations until they succeed (which is helped by SpiceDB allowing for idempotent writes with its TOUCH operation), you have worked around the dual-write problem.</p>\n<p>One of the most commonly-used tools in a system based on the transactional outbox pattern is <a href=\"https://debezium.io/\">Debezium</a>.\nIt watches changes in an outbox table and submits them as events to Kafka, which can then be consumed downstream to write to another system.</p>\n<p>Some other resources are available here:</p>\n<ul>\n<li><a href=\"https://www.decodable.co/blog/revisiting-the-outbox-pattern\">https://www.decodable.co/blog/revisiting-the-outbox-pattern</a></li>\n<li><a href=\"https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/transactional-outbox.html\">https://docs.aws.amazon.com/prescriptive-guidance/latest/cloud-design-patterns/transactional-outbox.html</a></li>\n</ul>\n<h2 id=\"user-content-so-which-one-should-i-choose\">So which one should I choose?</h2>\n<p>Unfortunately, when making writes to multiple systems, there are no easy answers. SpiceDB isn't unique in this regard, and most systems of sufficient complexity will eventually run into some variant of this problem. Which solution you choose will depend on the shape of your existing system, the requirements of your domain, and the appetite of your organization to make the associated changes. We still think it's worth it - when you centralize the data required for authorization decisions, you get big wins in consistency, performance, and safety. It just takes a little work.</p>",
            "url": "https://authzed.com/blog/the-dual-write-problem",
            "title": "The Dual-Write Problem",
            "summary": "The dual-write problem is present in any distributed system and is difficult to solve. We discuss where the problem arises and several approaches.",
            "image": "https://authzed.com/images/blogs/blog-featured-image.png",
            "date_modified": "2025-01-02T12:48:00.000Z",
            "date_published": "2025-01-02T12:48:00.000Z",
            "author": {
                "name": "Tanner Stirrat",
                "url": "https://www.linkedin.com/in/tannerstirrat/"
            }
        },
        {
            "id": "https://authzed.com/blog/spicedb-amazon-ecs",
            "content_html": "<h1 id=\"user-content-deploy-spicedb-to-amazon-ecs\">Deploy SpiceDB to Amazon ECS</h1>\n<p><a href=\"https://aws.amazon.com/ecs/\">Amazon Elastic Container Service (ECS)</a> is a fully managed container orchestration service that simplifies your deployment, management, and scaling of containerized applications. This blog will illustrate how you can install SpiceDB on Amazon ECS and is divided into 3 parts:</p>\n<ol>\n<li>A quickstart guide - Intended as a learning exercise</li>\n<li>A prod-friendly CloudFormation template</li>\n<li>Limitations of using Amazon ECS as a deployment target for SpiceDB</li>\n</ol>\n<p>It's important to note that this guide is meant for:</p>\n<ul>\n<li>You want to learn how to deploy SpiceDB with Amazon ECS</li>\n<li>Your current infrastructure is on Amazon ECS and you want to create a Proof of Concept with SpiceDB.</li>\n</ul>\n<p>It is not recommended to use SpiceDB on ECS as a production deployment target. See the final section of this post for more details.</p>\n<p>Here are the prerequisites to follow this guide:</p>\n<ol>\n<li>An Amazon Web Services (AWS) account with relevant permissions</li>\n<li><a href=\"https://aws.amazon.com/cli/\">AWS CLI</a></li>\n<li><a href=\"https://authzed.com/docs/spicedb/getting-started/installing-zed\">The zed CLI</a> (this is optional if you’re writing permissions via code</li>\n<li><a href=\"https://docs.docker.com/engine/install/\">Docker</a> installed on your system</li>\n</ol>\n<h2 id=\"user-content-quickstart\">Quickstart</h2>\n<p>Let’s start by pushing the SpiceDB Docker image to Amazon Elastic Container Registry (ECR)</p>\n<h3 id=\"user-content-push-a-spicedb-image-to-amazon-ecr\">Push a SpiceDB Image to Amazon ECR</h3>\n<h4 id=\"user-content-create-an-ecr-repository-using-the-aws-console\">Create an ECR Repository Using the AWS Console</h4>\n<ol>\n<li>Go to the ECR Console in the AWS Management Console.</li>\n<li>Click on Create repository.</li>\n<li>Enter a name for the repository, like spicedb, and configure any settings (like image scanning or encryption).</li>\n<li>Click Create repository to finish.</li>\n</ol>\n<p><img src=\"/images/upload/private-repo.png\" alt=\"Create private repository\"></p>\n<p>Alternately, you can create this using the AWS CLI with the following command:</p>\n<pre><code class=\"hljs language-shell\">aws ecr create-repository --repository-name spicedb --region &#x3C;your-region>\n</code></pre>\n<h4 id=\"user-content-authenticate-docker-to-amazon-ecr\">Authenticate Docker to Amazon ECR</h4>\n<p>Amazon ECR requires Docker to authenticate before pushing images.\nRetrieve an authentication token and authenticate your Docker client to your registry using the following command (you’ll need to replace <code>region</code> with your specific AWS region, like us-east-1)</p>\n<pre><code class=\"hljs language-shell\">aws ecr get-login-password --region &#x3C;region> | docker login --username AWS --password-stdin &#x3C;account-id>.dkr.ecr.&#x3C;region>.amazonaws.com\n</code></pre>\n<h4 id=\"user-content-tag-the-docker-image\">Tag the Docker Image</h4>\n<ul>\n<li>Pull and build the SpiceDB image from Docker Hub using this command</li>\n</ul>\n<pre><code class=\"hljs language-shell\">docker pull authzed/spicedb:latest\ndocker build -t spicedb .\n</code></pre>\n<ul>\n<li>After the build completes, tag your image so that you can push it to the ECR repository.</li>\n</ul>\n<pre><code class=\"hljs language-powershell\">docker tag spicedb:latest &#x3C;account-id>.dkr.ecr.&#x3C;region>.amazonaws.com/spicedb:latest\n</code></pre>\n<p><strong>Note</strong>: If you are using an Apple ARM-based machine (Ex: Mac with Apple Silicon) and you eventually want to deploy it to a x86-based instance you need to build this image for multi-architecture using the <code>buildx</code> command.</p>\n<p>You cannot use <code>docker buildx build</code> with an image reference directly.\nInstead, create a lightweight Dockerfile to reference the existing image by adding this one line:</p>\n<p><code>FROM authzed/spicedb:latest</code></p>\n<p>and save it in the directory. While in that directory, build and push a Multi-Architecture Image using the <code>buildx</code> command:</p>\n<pre><code class=\"hljs language-shell\">docker buildx build --platform linux/amd64,linux/arm64 -t &#x3C;account-id>.dkr.ecr.&#x3C;region>.amazonaws.com/spicedb:latest --push .\n</code></pre>\n<h4 id=\"user-content-push-the-image-to-ecr\">Push the Image to ECR</h4>\n<ul>\n<li>Once the image is tagged, push it to your newly-created ECR repository:</li>\n</ul>\n<pre><code class=\"hljs language-powershell\">docker push &#x3C;account-id>.dkr.ecr.&#x3C;region>.amazonaws.com/spicedb:latest\n</code></pre>\n<p>Replace <code>account-id</code> and <code>region</code> with your AWS account ID and region.</p>\n<ul>\n<li>Go to the Amazon ECR Console and navigate to the <code>spicedb</code> repository. Verify that the <code>spicedb:latest</code> image is available.</li>\n</ul>\n<p><strong>Note</strong>: All the above commands are pre-filled with your account details and can be seen by opening your repository on ECR and clicking the <strong>View push commands</strong> button</p>\n<p><img src=\"/images/upload/view-cmds.png\" alt=\"View push commands\"></p>\n<h3 id=\"user-content-run-a-spicedb-task-in-an-ecs-cluster\">Run a SpiceDB task in an ECS Cluster</h3>\n<h4 id=\"user-content-create-an-amazon-ecs-cluster\">Create an Amazon ECS Cluster</h4>\n<p>Using AWS Console:</p>\n<ol>\n<li>Go to the ECS console and click ‘Create cluster’</li>\n<li>Give it a name and namespace (optional)</li>\n<li>For this guide, we will use ‘AWS Fargate (serverless)’ as the infrastructure for our cluster</li>\n</ol>\n<p>Alternately, you can create this using the AWS CLI with this command:</p>\n<pre><code class=\"hljs language-shell\">aws ecs create-cluster --cluster-name spicedb-cluster\n</code></pre>\n<p><img src=\"/images/upload/create-cluster.png\" alt=\"Create cluster\"></p>\n<h4 id=\"user-content-create-iam-roles-if-they-dont-exist\">Create IAM Roles (if they don’t exist)</h4>\n<p>If you don’t see these roles, you can create them as follows:</p>\n<p><strong>Creating <code>ecsTaskExecutionRole</code>:</strong></p>\n<p>The ECS Task Execution Role is needed for ECS to pull container images from ECR, write logs to CloudWatch, and access other AWS resources.</p>\n<ol>\n<li>\n<p>Go to the IAM Console.</p>\n</li>\n<li>\n<p>Click Create Role.</p>\n</li>\n<li>\n<p>For Trusted Entity Type, choose AWS Service.</p>\n</li>\n<li>\n<p>Select Elastic Container Service and then Elastic Container Service Task.</p>\n</li>\n<li>\n<p>Click Next and attach the following policies:</p>\n<ul>\n<li>AmazonECSTaskExecutionRolePolicy</li>\n</ul>\n</li>\n</ol>\n<p>Or use these commands using AWS CLI:</p>\n<pre><code class=\"hljs language-shell\">aws iam create-role --role-name ecsTaskExecutionRole \n\n--assume-role-policy-document '{\"Version\": \"2012-10-17\", \"Statement\": [{\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"ecs-tasks.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]}'\n</code></pre>\n<p>Attach the AmazonECSTaskExecutionRolePolicy to the role:</p>\n<pre><code class=\"hljs language-shell\">aws iam attach-role-policy --role-name ecsTaskExecutionRole \n\n--policy-arn arn:aws:iam::aws:policy/service-role/AmazonECSTaskExecutionRolePolicy\n</code></pre>\n<p><strong>Creating <code>ecsTaskRole</code> (Optional):</strong></p>\n<p>The ECS Task Role is optional and should be created if your containers need access to other AWS services such as Amazon RDS or Secrets Manager.</p>\n<ol>\n<li>Go to IAM Console and click Create Role.</li>\n<li>Choose Elastic Container Service and then select Elastic Container Service Task as the trusted entity.</li>\n<li>Attach any necessary policies (such as SecretsManagerReadWrite or other policies based on your application’s needs).</li>\n<li>Name the role ecsTaskRole and click Create role.</li>\n</ol>\n<p>Or use these commands using AWS CLI:</p>\n<p>Create the role using:</p>\n<pre><code class=\"hljs language-shell\">aws iam create-role --role-name ecsTaskRole \n\n--assume-role-policy-document '{\"Version\": \"2012-10-17\", \"Statement\": [{\"Effect\": \"Allow\", \"Principal\": {\"Service\": \"ecs-tasks.amazonaws.com\"}, \"Action\": \"sts:AssumeRole\"}]}'\n</code></pre>\n<p>Attach any policies based on the specific AWS services your application needs access to:</p>\n<pre><code class=\"hljs language-shell\">aws iam attach-role-policy --role-name ecsTaskRole \n\n--policy-arn arn:aws:iam::&#x3C;policy-arn-for-service-access>\n</code></pre>\n<h4 id=\"user-content-define-the-ecs-task-definition\">Define the ECS Task Definition</h4>\n<p>The task definition defines how SpiceDB containers will be configured and run. Below is the JSON configuration for the task definition. To create a task definition:</p>\n<ul>\n<li>\n<p>AWS Console</p>\n<ul>\n<li>Look for Amazon ECS and then click on Task Definitions on the left</li>\n<li>Click Create new task definition -> Create new task definition with JSON</li>\n</ul>\n</li>\n</ul>\n<p>Copy the JSON below:</p>\n<pre><code class=\"hljs language-jsonc\"><span class=\"hljs-punctuation\">{</span>\n  <span class=\"hljs-attr\">\"family\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"spicedb-task\"</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"networkMode\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"awsvpc\"</span><span class=\"hljs-punctuation\">,</span>\n  <span class=\"hljs-attr\">\"requiresCompatibilities\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span><span class=\"hljs-string\">\"FARGATE\"</span><span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>  \n  <span class=\"hljs-attr\">\"cpu\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"512\"</span><span class=\"hljs-punctuation\">,</span>  \n  <span class=\"hljs-attr\">\"memory\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"1024\"</span><span class=\"hljs-punctuation\">,</span>  \n  <span class=\"hljs-attr\">\"executionRoleArn\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"arn:aws:iam::&#x3C;account-id>:role/ecsTaskExecutionRole\"</span><span class=\"hljs-punctuation\">,</span> <span class=\"hljs-comment\">//Copy the ARN from the ecsTaskExecutionRole created above</span>\n  <span class=\"hljs-attr\">\"taskRoleArn\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"arn:aws:iam::&#x3C;account-id>:role/ecsTaskRole\"</span><span class=\"hljs-punctuation\">,</span> <span class=\"hljs-comment\">//Copy the ARN from the ecsTaskRole created above</span>\n  <span class=\"hljs-attr\">\"containerDefinitions\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n    <span class=\"hljs-punctuation\">{</span>\n      <span class=\"hljs-attr\">\"name\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"spicedb\"</span><span class=\"hljs-punctuation\">,</span>\n      <span class=\"hljs-attr\">\"image\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"&#x3C;account-id>.dkr.ecr.&#x3C;region>.amazonaws.com/spicedb\"</span><span class=\"hljs-punctuation\">,</span>  <span class=\"hljs-comment\">//ECR Repository URI</span>\n      <span class=\"hljs-attr\">\"essential\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-literal\"><span class=\"hljs-keyword\">true</span></span><span class=\"hljs-punctuation\">,</span>\n      <span class=\"hljs-attr\">\"command\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n                <span class=\"hljs-string\">\"serve\"</span><span class=\"hljs-punctuation\">,</span>\n                <span class=\"hljs-string\">\"--grpc-preshared-key\"</span><span class=\"hljs-punctuation\">,</span>\n                <span class=\"hljs-string\">\"somekey\"</span>  \n            <span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n      <span class=\"hljs-attr\">\"portMappings\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n        <span class=\"hljs-punctuation\">{</span>\n          <span class=\"hljs-attr\">\"containerPort\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">50051</span><span class=\"hljs-punctuation\">,</span>\n          <span class=\"hljs-attr\">\"hostPort\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-number\">50051</span><span class=\"hljs-punctuation\">,</span>\n          <span class=\"hljs-attr\">\"protocol\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"tcp\"</span>\n        <span class=\"hljs-punctuation\">}</span>\n      <span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n      <span class=\"hljs-attr\">\"environment\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span><span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n      <span class=\"hljs-attr\">\"logConfiguration\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">{</span>\n        <span class=\"hljs-attr\">\"logDriver\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"awslogs\"</span><span class=\"hljs-punctuation\">,</span>\n       <span class=\"hljs-attr\">\"options\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">{</span>\n                    <span class=\"hljs-attr\">\"awslogs-group\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"/ecs/spicedb-ecs\"</span><span class=\"hljs-punctuation\">,</span>\n                    <span class=\"hljs-attr\">\"mode\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"non-blocking\"</span><span class=\"hljs-punctuation\">,</span>\n                    <span class=\"hljs-attr\">\"awslogs-create-group\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"true\"</span><span class=\"hljs-punctuation\">,</span>\n                    <span class=\"hljs-attr\">\"max-buffer-size\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"25m\"</span><span class=\"hljs-punctuation\">,</span>\n                    <span class=\"hljs-attr\">\"awslogs-region\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"us-east-1\"</span><span class=\"hljs-punctuation\">,</span>\n                    <span class=\"hljs-attr\">\"awslogs-stream-prefix\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-string\">\"ecs\"</span>\n                <span class=\"hljs-punctuation\">}</span>\n      <span class=\"hljs-punctuation\">}</span>\n    <span class=\"hljs-punctuation\">}</span>\n  <span class=\"hljs-punctuation\">]</span>\n<span class=\"hljs-punctuation\">}</span>\n</code></pre>\n<p>The <code>command</code> section specifies <code>serve</code> which is the <a href=\"https://authzed.com/docs/spicedb/concepts/commands#serve\">primary command for running SpiceDB</a>.\nThis command serves the gRPC and HTTP APIs by default along with a pre-shared key for authenticated requests.</p>\n<p><strong>Note</strong>: This is purely for learning purposes so any permissions and relationships written to this instance of SpiceDB will be stored in-memory and not in a persistent database.\nTo write relationships to a persistent database, create a Amazon RDS instance for Postgres and note down the DB name, Master Password and Endpoint.</p>\n<p>You can add those into the task definition JSON in the <code>command</code> array like this:</p>\n<pre><code class=\"hljs language-jsonc\"><span class=\"hljs-attr\">\"command\"</span><span class=\"hljs-punctuation\">:</span> <span class=\"hljs-punctuation\">[</span>\n                <span class=\"hljs-string\">\"serve\"</span><span class=\"hljs-punctuation\">,</span>\n                <span class=\"hljs-string\">\"--grpc-preshared-key\"</span><span class=\"hljs-punctuation\">,</span>\n                <span class=\"hljs-string\">\"somekey\"</span><span class=\"hljs-punctuation\">,</span>\n                <span class=\"hljs-string\">\"--datastore-engine\"</span><span class=\"hljs-punctuation\">,</span>\n                <span class=\"hljs-string\">\"postgres\"</span><span class=\"hljs-punctuation\">,</span>\n                <span class=\"hljs-string\">\"--datastore-conn-uri\"</span><span class=\"hljs-punctuation\">,</span>\n                <span class=\"hljs-string\">\"postgres://&#x3C;username>:&#x3C;password>@&#x3C;RDS endpoint>:5432/&#x3C;dbname>?sslmode=require\"</span>\n            <span class=\"hljs-punctuation\">]</span><span class=\"hljs-punctuation\">,</span>\n</code></pre>\n<p>The defaults for <code>username</code> and <code>dbname</code> are usually <code>postgres</code></p>\n<p>You can also use the AWS CLI by storing the above JSON in a file an then running this command</p>\n<pre><code class=\"hljs language-shell\">aws ecs register-task-definition --cli-input-json file://spicedb-task-definition.json\n</code></pre>\n<h4 id=\"user-content-run-the-task-in-a-ecs-cluster\">Run the task in a ECS Cluster</h4>\n<p>Now that we’ve defined a task, we can create a task that would run within your ECS cluster.\nClick on your ECS Cluster created earlier</p>\n<ol>\n<li>Click on the Tasks tab, and then Run new task</li>\n<li>Under Compute Configuration, click on Launch Type (since this is just a demo)</li>\n<li>Choose FARGATE as Launch Type and LATEST as Platform Version</li>\n<li>Under Deployment Configurat